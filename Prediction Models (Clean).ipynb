{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f04abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f2434b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5364e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# Sklearn models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Topic Modelling\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a34febcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df = pd.read_csv(r'C:\\Users\\Patrick\\Documents\\concordia-bootcamps\\Final Project\\data\\game_reviews_combined.csv', usecols=['review', 'voted_up', 'genre'])\n",
    "validation_df = pd.read_csv('steamcharts_with_genre.csv', usecols=['review', 'voted_up', 'genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe6d38f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big = pd.concat([df, validation_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "522b8a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 86070 entries, 0 to 52786\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   review    85862 non-null  object\n",
      " 1   voted_up  86070 non-null  bool  \n",
      " 2   genre     85050 non-null  object\n",
      "dtypes: bool(1), object(2)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_big.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74ad8ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEJCAYAAAB4yveGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAc8UlEQVR4nO3de1TUdf7H8dcwAwwXIbmJ4AVErbylG15TMWXTyrXyuEZGmpfOtmiKJgtluqaWmJJoWdKNhMrNVtnWWuuEd3MtFM0VDVTQMCfkImIhDsy8f394+P4cAW8hg/N5Pc7pHOc7n+/3+/mw43O+fAdcnYgIiIhICU72ngARETUdRp+ISCGMPhGRQhh9IiKFMPpERAph9ImIFMLo0zWdOHECOp0Ou3btuuq4Dz/8EAaDoYlmdfsICQnBokWL7D2NRqXT6fDRRx/Zexp0Exh9B/D0009Dp9NBp9PBYDCgffv2ePbZZ1FaWtoox2/bti1MJhP69u0LADh16hR0Oh22bdtmM+7xxx/Hzz//3CjnrE9ERARWrlwJ4FJ09Ho99u/fbzPGnm88U6ZMwZAhQ+psz8rKwsyZM2/5+YcMGaK9DpydnRESEoLnnnsO5eXljX4uk8mEMWPGNPpx6dZj9B3EoEGDYDKZcOLECaxcuRLr16/H+PHjG+XYer0egYGBcHZ2vuo4Nzc3tGrVqlHOeaXi4mJ8++23ePTRR7Vtrq6umDVr1i05X2Py9/eHh4dHk5xr3LhxMJlMKCgowOrVq7FhwwbExMQ0+nkCAwNhNBob/bh06zH6DsLFxQWBgYFo06YNHnnkEcTGxuKrr77ChQsXICJYtmwZOnToABcXF4SFhSE5Odlm/88//xy9evWCu7s77rjjDvTp00e7ir7y9k7btm0BAPfffz90Oh1CQkIA2F5lV1RUwN3dHZ988onNeUwmE/R6Pb766isAQE1NDebPn4/Q0FAYjUZ07doVKSkpddb3+eefo2fPnmjXrp22bcaMGdi5cycyMjKu+rXZt28fHnjgAXh6esLf3x+jR4/GyZMnbcYkJyejTZs2cHd3x/Dhw5Geng6dTodTp04BAM6ePYvo6Gi0a9cObm5uuPPOO5GUlITaX2ifP38+3n//fWzfvl272v7www8B2N7emTNnDu688846c/zrX/+Kfv363dCc6+Pm5qa9DkaMGIGoqCh8/fXXNmO++eYb3HfffXBzc0NwcDAmTpyofVf4zTffQK/Xo7Cw0GafTz/9FEajUfuu4crbO7/++itmzJiB4OBguLu7o1evXtiwYYP2fHR0NKKjo7XHqamp0Ol0eO+997RtEyZMwNixYwFcev1MnDgRgYGBcHV1Rdu2bW+LN/jbgtBtb8KECTJs2DCbbUlJSQJAKioq5M033xSj0SgpKSmSl5cnb7/9tri6usp7770nIiImk0mcnZ1lyZIlkp+fL4cPH5aPP/5YDh48KCIiBQUFAkB27twpIiLZ2dkCQNavXy8mk0nOnDkjIiKpqami1+u1OURFRckDDzxgM6+lS5dK69atpaamRpt79+7d5euvv5b8/Hz5xz/+Id7e3trcaj300EOyaNEi7TEASU9Pl7/85S/SsWNHuXjxYr1zyMnJEQ8PD5k3b54cOXJEDh48KGPGjJFOnTrJhQsXRERk/fr1otfrJTk5WfLy8iQ1NVVat24tAKSwsFD7GiUmJsq+ffskPz9f0tPTxcPDQz744AMRETl//ryMGzdO+vfvLyaTSUwmk1RWVoqISPv27WXhwoUiIpKbmysAZPfu3docL168KD4+PvLWW29d95zrExERIZMnT9YeHz16VO666y4JDAzUtm3evFnc3Nxk5cqVkpeXJ99//70MGTJEBg0aJFarVSwWiwQHB8urr75qc+yHH35Yxo4dW+frLyJitVplyJAhEhERITt37pTjx49LSkqKODs7S2ZmpoiIvP/++9K6dWtt/+joaPH395eoqChtW9u2beXtt98WEZHnnntOevToIXv27JGTJ0/Kt99+K++8806Da6frx+g7gCujn5OTIx06dJC+ffuKiEibNm0kLi7OZp/Y2FgJDQ0Vkf+PeEFBQb3HvzL6hYWFAkC2bt1qM+7K4G7atEn0er38/PPP2rYePXrI7NmzRUQkPz9fdDqdHDlyxOY4L7/8stxzzz3a44qKCnF1dZWcnBxtW210ioqKxMvLS5KSkuqdw4QJE+Txxx+3OX5VVZW4ublJRkaGiIgMGDBAoqOjbcbEx8fbRL8+06dPl8jISO3x5MmTJSIios64y6MvItK3b1959tlntcfr168XFxcXKS0tve451yciIkIMBoN4eHiIq6urABAA8sYbb9iMiY+Pt9nv5MmTAkD279+vrf3uu+/Wni8qKhKDwSBffPGFtu3y6G/dulVcXV2lvLzc5rgTJ06URx55RERETpw4IQC0/w2Dg4Nl2bJlEhAQICIieXl5AkByc3NFRGTUqFEyYcKEBtdKN4+3dxzEtm3b4OnpCTc3N3Tr1g0dOnTAJ598goqKCpw6dQqDBw+2GR8REYETJ06gsrISPXr0wPDhw9GtWzc89thjWLFiRZ1v72/GH//4RwQEBODjjz8GAPzwww84ePCg9lnD3r17ISIIDw+Hp6en9t+rr76Ko0ePasf58ssv0b59e3Tp0qXOOQICAvDiiy9i4cKF9X5wnZWVhYyMDJvj+/r6oqqqSjvH4cOHbW6tAED//v1tHlutViQmJqJnz57w8/ODp6cnVq9efV23XK40fvx4fPrppzCbzQCA9PR0/OlPf4KPj891z7khjz32GA4cOIDvvvsOzzzzDEaPHm1zTz8rKwvJyck2x679utYee8KECThy5AiysrIAAGvXroWvry+GDx9e7zmzsrJgNpsRHBxsc9yPPvpIO2b79u0RGhqKLVu2IDc3F+Xl5YiJiUFVVRUOHTqELVu2IDg4GJ07dwYAxMTE4J///Ce6deuGGTNmYNOmTbBarTf8taa6+PN1DqJv375Ys2YNDAYDWrduDVdXVwCX7o0Cl+7BXk4u+8dV9Xo9Nm3ahKysLGRmZmL9+vVISEjAZ599hpEjR970nPR6PZ588kmkpaUhLi4OaWlp6NWrF7p37w4A2l/i3bt3w93d3Wbfy+ebkZGBxx57rMHzxMbGIiUlBfPnz8e9995r85zVasVTTz2FhISEOvv5+vrWe776JCUlYfHixXj99dfxhz/8AS1atMDy5cvx5ZdfXnW/+kRFRWHmzJnYuHEj7r//fvznP//BZ599dsNzro+Xlxc6duwIAEhJScGAAQOwaNEizJs3Tzt2fHw8nnrqqTr7BgYGAgDuvvtuhIeHIy0tDb1790ZaWhrGjRvX4E9FWa1WeHt7a28Sl3NxcdH+PHToUGzevBl6vR4DBw6Em5sbBg8ejM2bN2P37t0YOnSoNnb48OH46aef8PXXX2Pbtm2Ijo5G9+7dtf3p5jH6DsLNzU37y345Ly8vtGnTBtu3b8fDDz+sbd+xYwdCQ0O12Op0OvTp0wd9+vTBiy++iBEjRiA1NbXe6Nf+RbZYLNec14QJE7Bs2TLs3bsXa9euRXx8vPZcbaB/+umnBt9cLl68iE2bNuGbb75p8Byurq5ITEzEk08+ieeff97mufDwcBw8eBBhYWENhr1Lly7473//a3NFvGfPHpsxO3bswIgRIzB58mRt25VX3S4uLtf1NfHx8cHIkSORlpaGoqIieHt748EHH7yhOV8PnU6Hl19+GY888ggmTZqENm3aIDw8HDk5OfW+Vi43fvx4LFiwAFOmTEF2djY++OCDBseGh4ejvLwcVVVV6NatW4Pjhg4dimnTpsHJyQnDhg3Ttm3evBnfffcdlixZYjPex8cHTzzxBJ544glMnDgR/fv3x+HDh7WLBrpJ9r6/RL9ffR/kXm7VqlViNBrlnXfekby8PFm9erXNB7nffvutLFiwQPvQLDMzU1q3bi0vvfSSiNS9p2+xWMTT01P+9re/iclkkrKyMhGpez+9Vq9evaRnz55iMBikqKjI5rlJkyZJYGCgpKWlydGjR+XAgQPy/vvvS2JiooiIbNy4UYKCgsRqtdrsh8vuKde67777xM3NzWYOhw8fFk9PTxk3bpx89913kp+fL1u2bJHp06fL8ePHReTSPXWDwSArV66Uo0ePypo1ayQoKEgAyKlTp0RE5Pnnn5eAgADZsmWL5Obmypw5c8TLy0vat2+vneu1114TPz8/OXTokBQXF0tVVZWI1L2nLyLy+eefi7Ozs3Tt2lWmT59u89z1zLk+V36QW+uee+6RKVOmiIjIli1bxGAwSGxsrOzfv1+OHTsmmzZtkkmTJmkfPIuIFBcXi7Ozs/Ts2VN69OhR55i44oPcyMhI6dSpk2zYsEGOHz8ue/fulZUrV9p8+GoymQSAGAwG2bt3r4iIHDhwQAwGgwCQkydPamNffPFFWb9+vfz444+Sl5cn06ZNE09PzzqfG9CNY/QdwLWib7Va5bXXXpOQkBAxGAwSGhoqy5cv154/dOiQPPjgg9KqVStxcXGRdu3ayezZs7WfiLky+iIia9as0Y5XG76Gop+cnCwAZOTIkXWeq6mpkSVLlsidd94pzs7O4uvrK4MHD5Z169aJyKU3hZiYmDr71Rf977//XnQ6XZ05HDx4UEaNGiV33HGHGI1GCQsLk2eeeUb74FRE5PXXX5egoCAxGo3ywAMPSEpKigCQkpISEREpLy+XP//5z9KiRQvx8fGRmJgYeemll2yiX1paKg8++KB4eXkJAElNTRWR+qNvNpvF399fAGgBvNE5X6mh6H/00Uei1+vlxx9/FBGRHTt2yLBhw8TT01Pc3d3lrrvukhkzZkh1dbXNfo8++qgAkGXLltU55pVf/8rKSomPj5eQkBBxdnaWVq1ayfDhw2Xz5s02+3Xp0kVatmwpFotFRC69Nv38/CQsLMxm3IIFC6Rr167i4eEhXl5eMnjwYJvXH908nQj/n7OoebJYLAgMDMTatWsRGRnZpOdesGABVqxY0Wi/1UzUXPCePjVbpaWlmDZtWr3/tEFjqq6uRlJSEh566CF4eHhg69atWLp0KaZOnXpLz0tkD7zSJ+XV1NRg5MiR2LdvH86fP4/Q0FCMHz8ecXFx/AfkyOEw+kRECuEvZxERKYTRJyJSSLO/YXn69Gl7T6FJ+Pn5oaSkxN7TaBJcq2PiWpuPoKCgBp/jlT4RkUIYfSIihTD6REQKYfSJiBTC6BMRKYTRJyJSCKNPRKQQRp+ISCGMPhGRQhh9IiKFMPpERAph9ImIFMLoExEphNEnIlIIo09EpBBGn4hIIYw+EZFCGH0iIoUw+kRECmH0iYgUwugTESmE0SciUgijT0SkEEafiEghjD4RkUIYfSIihehEROw9iaspfDjc3lMgImpS+nf//bv2DwoKavA5XukTESmE0SciUgijT0SkEEafiEghjD4RkUIYfSIihTD6REQKYfSJiBTC6BMRKYTRJyJSCKNPRKQQRp+ISCGMPhGRQhh9IiKFMPpERAph9ImIFMLoExEphNEnIlIIo09EpBBGn4hIIYw+EZFCGH0iIoUw+kRECjHcyODz589jwYIFAIDy8nI4OTnBy8sLALB48WIYDDd0OCIiamI3VOkWLVpg6dKlAIB169bBaDRi1KhR2vMWiwV6vb5xZ0hERI3md1+ar1q1Cp6enjhx4gRCQ0NhNBpt3gyef/55xMfHIyAgADt27MCmTZtQU1ODTp06YcqUKXBy4h0mIqKm0ij3Y0wmE+bOnQsnJyesW7eu3jGnTp3C7t27sXDhQhgMBrz33nvYuXMnIiIibMZlZmYiMzMTAJCYmNgY0yMiuq34+fndsmM3SvT79et3zSv2Q4cOoaCgAC+88AIAwGw2a58HXC4yMhKRkZGNMS0iottSSUnJ79o/KCiowecaJfpGo1H7s16vh4hoj81mMwBARBAREYFx48Y1ximJiOgmNPoNdX9/fxQUFAAA8vPzcebMGQBA9+7dsWfPHpw7dw4A8Ouvv6K4uLixT09ERFfR6D9j2a9fP+zYsQNxcXEICwvTvs1o06YNoqKisGjRIogI9Ho9Jk+eDH9//8aeAhERNUAnl9+LaYYKHw639xSIiJqU/t1//679r3ZPnz8vSUSkEEafiEghjD4RkUIYfSIihTD6REQKYfSJiBTC6BMRKYTRJyJSCKNPRKQQRp+ISCGMPhGRQhh9IiKFMPpERAph9ImIFMLoExEphNEnIlIIo09EpBBGn4hIIYw+EZFCGH0iIoUw+kRECtGJiNh7Eldz+vRpe0+hSfj5+aGkpMTe02gSXKtj4lqbj6CgoAaf45U+EZFCGH0iIoUw+kRECmH0iYgUwugTESmE0SciUgijT0SkEEafiEghjD4RkUIYfSIihTD6REQKYfSJiBTC6BMRKYTRJyJSCKNPRKQQRp+ISCGMPhGRQhh9IiKFMPpERAph9ImIFMLoExEphNEnIlIIo09EpBBGn4hIIYw+EZFCGH0iIoUw+kRECmH0iYgUwugTESmE0SciUgijT0SkEEafiEghjD4RkUIYfSIihTD6REQKYfSJiBTC6BMRKYTRJyJSCKNPRKQQRp+ISCGMPhGRQhh9IiKFMPpERAph9ImIFMLoExEphNEnIlIIo09EpBBGn4hIIYw+EZFCGH0iIoUw+kRECmH0iYgUwugTESnEYO8JXIvlmVH2nkKTKLL3BJoQ19ow/bv/viXzIKrFK30iIoUw+kRECmH0iYgUwugTESmE0SciUgijT0SkEEafiEghjD4RkUIYfSIihTD6REQKYfSJiBTC6BMRKYTRJyJSCKNPRKQQRp+ISCGMPhGRQhh9IiKFMPpERAph9ImIFMLoExEphNEnIlIIo09EpBBGn4hIIYZrDXj88cfRrl077XFcXBwCAgLqHfvUU08hPT298WZHRESN6prRd3FxwdKlS5tiLkREdItdM/pXqqqqwmuvvYbffvsNNTU1iIqKQu/evW3GnD17FsnJyaisrITVasWUKVNw991344cffsC6detQU1ODVq1aISYmBkajsdEWQ0REV3fN6JvNZsTFxQEAAgICMGvWLMyePRvu7u6oqKjAnDlzEB4eDp1Op+2za9cu3HPPPRg9ejSsVisuXryIiooKbNiwAXPnzoXRaMS//vUvfPHFFxgzZozN+TIzM5GZmQkASExMbMy1EjV7fn5+9p7CTTMYDLf1/G/E7bzWG769U1NTg7Vr1+LIkSPQ6XQoKyvDuXPncMcdd2hjwsLC8Pbbb6OmpgZ9+vRBSEgIDh8+jFOnTmHu3LnacTp37lznfJGRkYiMjGyEpRHdfkpKSuw9hZvm5+d3W8//RjT3tQYFBTX43A3f3tm1axcqKiqQmJgIg8GAqVOnwmw224zp0qULXn75ZWRnZ+ONN97AqFGj4OHhge7duyM2NvaGF0BERI3jhn9ks7KyEt7e3jAYDDh06BCKi4vrjCkuLoa3tzciIyMxdOhQFBQUoHPnzsjNzcUvv/wCALh48SJOnz79+1dARETX7Yav9AcOHIglS5YgISEBISEhCA4OrjMmJycHGzduhF6vh9FoxLRp0+Dl5YWpU6dixYoVqK6uBgBERUVd9dsQIiJqXDoREXtP4moKHw639xSImoz+3X/bewo3rbnf525MzX2tV7uY5m/kEhEphNEnIlIIo09EpBBGn4hIIYw+EZFCGH0iIoUw+kRECmH0iYgUwugTESmE0SciUgijT0SkEEafiEghjD4RkUIYfSIihTD6REQKYfSJiBTC6BMRKYTRJyJSCKNPRKQQRp+ISCGMPhGRQhh9IiKFGOw9gWvRv/tve0+hSfj5+aGkpMTe02gSXCuR/fBKn4hIIYw+EZFCGH0iIoUw+kRECmH0iYgUwugTESmE0SciUgijT0SkEEafiEghjD4RkUIYfSIihTD6REQKYfSJiBTC6BMRKYTRJyJSCKNPRKQQRp+ISCGMPhGRQhh9IiKFMPpERAph9ImIFMLoExEphNEnIlIIo09EpBBGn4hIIYw+EZFCdCIi9p4EERE1jWZ9pZ+QkGDvKTQZrtUxca2O6XZea7OOPhERNS5Gn4hIIc06+pGRkfaeQpPhWh0T1+qYbue18oNcIiKFNOsrfSIialyMPhGRQgz2nkBDDhw4gNTUVFitVgwbNgyPPvqovad0Xd566y1kZ2fD29sbSUlJAIBff/0Vy5cvR3FxMfz9/TFz5kx4enoCADIyMrBlyxY4OTlh4sSJ6NmzJwAgPz8fq1atgtlsRq9evTBx4kTodDpUV1fjzTffRH5+Plq0aIHY2FgEBAQ0+TpLSkqwatUqlJeXQ6fTITIyEg899JBDrtVsNuPvf/87ampqYLFY0K9fP4wdO9Yh11rLarUiISEBPj4+SEhIcNi1Tp06FUajEU5OTtDr9UhMTHTYtWqkGbJYLDJt2jT55ZdfpLq6WmbPni2FhYX2ntZ1ycnJkePHj8usWbO0benp6ZKRkSEiIhkZGZKeni4iIoWFhTJ79mwxm81SVFQk06ZNE4vFIiIiCQkJkpubK1arVV555RXJzs4WEZGvvvpKUlJSRERk165d8vrrrzfh6v5fWVmZHD9+XEREKisrZfr06VJYWOiQa7VarXLhwgUREamurpYXXnhBcnNzHXKttTZu3CjJycmyePFiEXHM17CISExMjJw7d85mm6OutVazvL1z7NgxBAYGolWrVjAYDBgwYACysrLsPa3r0qVLF+2qoFZWVhYiIiIAABEREdpasrKyMGDAADg7OyMgIACBgYE4duwYzp49iwsXLqBz587Q6XQYPHiwts/evXsxZMgQAEC/fv1w6NAhiB0+i2/ZsiU6dOgAAHBzc0NwcDDKysoccq06nQ5GoxEAYLFYYLFYoNPpHHKtAFBaWors7GwMGzZM2+aoa62Po6+1WUa/rKwMvr6+2mNfX1+UlZXZcUa/z7lz59CyZUsAl2JZUVEBoO46fXx8UFZWdtX1X/6cXq+Hu7s7zp8/31RLqdeZM2dQUFCAjh07OuxarVYr4uLiMGXKFHTv3h2dOnVy2LV++OGHiI6Ohk6n07Y56loB4JVXXkF8fDwyMzMBOPZagWZ6T7++d8LLX4COoqF3/KtdCTS3r01VVRWSkpLw9NNPw93dvcFxt/tanZycsHTpUvz2229YtmwZfvrppwbH3s5r3bdvH7y9vdGhQwfk5ORcc/ztvFYAWLhwIXx8fHDu3DksWrQIQUFBDY693ddaq1lG39fXF6Wlpdrj0tJS7Z33duTt7Y2zZ8+iZcuWOHv2LLy8vADUXWdZWRl8fHzqXb+Pj4/NPr6+vrBYLKisrKxzO6mp1NTUICkpCYMGDULfvn0BOO5aa3l4eKBLly44cOCAQ641NzcXe/fuxf79+2E2m3HhwgWsXLnSIdcKQJuTt7c3evfujWPHjjnsWms1y9s7YWFhMJlMOHPmDGpqarB7926Eh4fbe1o3LTw8HNu3bwcAbN++Hb1799a27969G9XV1Thz5gxMJhM6duyIli1bws3NDXl5eRAR7NixQ1v/vffei23btgEA9uzZg65du9rlykFEsHr1agQHB2PkyJHadkdca0VFBX777TcAl36S53//+x+Cg4Mdcq3jxo3D6tWrsWrVKsTGxqJbt26YPn26Q661qqoKFy5c0P588OBBtGvXziHXerlm+xu52dnZWLNmDaxWK+6//36MHj3a3lO6LsnJyTh8+DDOnz8Pb29vjB07Fr1798by5ctRUlICPz8/zJo1S3u337BhA7Zu3QonJyc8/fTT6NWrFwDg+PHjeOutt2A2m9GzZ09MmjQJOp0OZrMZb775JgoKCuDp6YnY2Fi0atWqydf5448/Yt68eWjXrp32In7iiSfQqVMnh1vryZMnsWrVKlitVogI+vfvjzFjxuD8+fMOt9bL5eTkYOPGjUhISHDItRYVFWHZsmUALn1AP3DgQIwePdoh13q5Zht9IiJqfM3y9g4REd0ajD4RkUIYfSIihTD6REQKYfSJiBTC6BMRKYTRJyJSyP8BmzAmSoulVmwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_big['voted_up'].value_counts().sort_values().plot(kind='barh', title='Positive/Negative Reviews');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c375407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_big.groupby('genre')['voted_up'].value_counts().plot(kind='barh')\n",
    "plt.title('Reviews by Genre')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7143d549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Reviews by Genre'}, ylabel='genre'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_big.groupby('genre').agg(['count', 'sum'])['voted_up'].plot(kind='barh', title='Reviews by Genre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f8960f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_big.groupby('genre')['voted_up'].count().sort_values().plot(kind='barh', title='# Reviews by Genre');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b4ededc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 84845 entries, 0 to 52573\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   review    84845 non-null  object\n",
      " 1   voted_up  84845 non-null  bool  \n",
      " 2   genre     84845 non-null  object\n",
      "dtypes: bool(1), object(2)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Cleaning Data - drops 1500 NAN rows\n",
    "df_big.dropna(inplace=True)\n",
    "df_big.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9569e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Immersive story, lovely characters and voice a...\n",
       "1    This game is a slow burn, it builds over time....\n",
       "2    The character interactions in the cutscenes in...\n",
       "3                       Daryl Dixon Simulator.....YES!\n",
       "4    This game is incredibly underrated. I find mys...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_text = df_big['review']\n",
    "review_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a39ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "replaceDict = dict({\n",
    "'{':\" \", '}':\" \", ',':\"\", '.':\" \", '!':\" \", '\\\\':\" \", '/':\" \", '$':\" \", '%':\" \",\n",
    "'^':\" \", '?':\" \", '\\'':\" \", '\"':\" \", '(':\" \", ')':\" \", '*':\" \", '+':\" \", '-':\" \",\n",
    "'=':\" \", ':':\" \", ';':\" \", ']':\" \", '[':\" \", '`':\" \", '~':\" \", '☑': ' ', '☐':' ',\n",
    "})\n",
    "\n",
    "rep = dict((re.escape(k),v) for k, v in replaceDict.items())\n",
    "pattern = re.compile('|'.join(rep.keys()))\n",
    "def replacer(text):\n",
    "    \"\"\"\n",
    "    Removes punctuation and symbols\n",
    "    \"\"\"\n",
    "    return rep[re.escape(text.group(0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad9e53fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "def remove_stopwords(lst):\n",
    "    \"\"\"\n",
    "    Removes stopwords from list of word tokens\n",
    "    Returns a string\n",
    "    \"\"\"\n",
    "    return ' '.join(word for word in lst if word not in stopwords)\n",
    "\n",
    "def get_stems(lst):\n",
    "    \"\"\"\n",
    "    Takes a list of words and stems them using PorterStemmer\n",
    "    Returns a string with all words stemmed\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    return ' '.join(ps.stem(w) for w in lst.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47bb3220",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = df_big['review'].str.replace(pattern, replacer).str.lower().str.split()\n",
    "review_clean = words.apply(remove_stopwords)\n",
    "review_stemmed = review_clean.apply(get_stems)\n",
    "df_big['review_stemmed'] = review_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61b91830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        {'immers': 1, 'stori': 3, 'love': 2, 'charact'...\n",
       "1        {'game': 5, 'slow': 1, 'burn': 1, 'build': 1, ...\n",
       "2        {'charact': 3, 'interact': 1, 'cutscen': 2, 'g...\n",
       "3            {'daryl': 1, 'dixon': 1, 'simul': 1, 'ye': 1}\n",
       "4        {'game': 5, 'incred': 1, 'underr': 1, 'find': ...\n",
       "                               ...                        \n",
       "52569    {'bother': 1, 'even': 2, 'sale': 1, 'go': 1, '...\n",
       "52570    {'liter': 1, 'play': 1, 'game': 1, '2k': 1, 'a...\n",
       "52571    {'play': 1, 'xcom': 1, 'style': 1, 'game': 5, ...\n",
       "52572    {'miss': 1, '4': 1, 'shot': 1, '90': 1, 'chanc...\n",
       "52573    {'like': 2, 'type': 1, 'game': 3, 'gener': 1, ...\n",
       "Name: review, Length: 84845, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def FrequencyDistribution(series):\n",
    "    \"\"\"\n",
    "    Takes in a string. Tokenizes and then reports the frequency distribution of those words\n",
    "    Returns frequency distribution\n",
    "    \"\"\"\n",
    "    fd_words = series.apply(nltk.word_tokenize)\n",
    "    fd = fd_words.apply(nltk.FreqDist)\n",
    "    return fd\n",
    "\n",
    "fd = FrequencyDistribution(review_stemmed)\n",
    "fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2e0e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigrams and bigrams, ignore words that appear in more than 75% of docs\n",
    "tf = TfidfVectorizer(ngram_range=(1, 2), max_df=0.75)\n",
    "\n",
    "X = df_big['review_stemmed']\n",
    "y = df_big['voted_up']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "X_train = tf.fit_transform(X_train)         #fit_transform TfidfTransformer on training data\n",
    "X_test = tf.transform(X_test)               #transform TfidfTransformer on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88dcb32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(clf, X_train=X_train, X_test=X_test):\n",
    "    \"\"\"\n",
    "    Fit and predict using a classifier\n",
    "    Returns the classifier description, the accuracy score, training time, and testing time\n",
    "    \"\"\"\n",
    "    print(\"_\" * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split(\"(\")[0]\n",
    "    return clf_descr, score, train_time, test_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9721b89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: \n",
      "VotingClassifier(estimators=[('Ridge Classifier',\n",
      "                              RidgeClassifier(solver='sparse_cg', tol=0.01)),\n",
      "                             ('Perceptron', Perceptron()),\n",
      "                             ('Passive Aggressive Classifier',\n",
      "                              PassiveAggressiveClassifier()),\n",
      "                             ('Complement Naive Bayes',\n",
      "                              ComplementNB(alpha=0.01)),\n",
      "                             ('Multinomial Naive Bayes',\n",
      "                              MultinomialNB(alpha=0.01)),\n",
      "                             ('Bernoulli Naive Bayes', BernoulliNB(alpha=0.01)),\n",
      "                             ('LinearSVC', LinearSVC(dual=False, tol=0.001))])\n",
      "train time: 21.725s\n",
      "test time:  0.333s\n",
      "accuracy:   0.892\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('VotingClassifier',\n",
       " 0.8920973539984678,\n",
       " 21.725444078445435,\n",
       " 0.3329508304595947)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensemble Voting classifier\n",
    "# Uses all 7 classifiers to predict a class. Majority vote wins.\n",
    "rc = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n",
    "pc = Perceptron(max_iter=1000)\n",
    "pac = PassiveAggressiveClassifier()\n",
    "cnb = ComplementNB(alpha=0.01)\n",
    "mnb = MultinomialNB(alpha=0.01)\n",
    "bnb = BernoulliNB(alpha=0.01)\n",
    "lsvc = LinearSVC(penalty='l2', dual=False, tol=1e-3)\n",
    "\n",
    "eclf_sentiment = VotingClassifier(\n",
    "     estimators=[('Ridge Classifier', rc), ('Perceptron', pc), ('Passive Aggressive Classifier', pac),\n",
    "                 ('Complement Naive Bayes', cnb), ('Multinomial Naive Bayes', mnb), ('Bernoulli Naive Bayes', bnb),\n",
    "                 ('LinearSVC', lsvc)\n",
    "                ],\n",
    "     voting='hard')\n",
    "\n",
    "benchmark(eclf_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89f1d86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.89      0.81      0.85      6279\n",
      "        True       0.90      0.94      0.92     10690\n",
      "\n",
      "    accuracy                           0.89     16969\n",
      "   macro avg       0.89      0.88      0.88     16969\n",
      "weighted avg       0.89      0.89      0.89     16969\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\seaborn\\matrix.py:302: MatplotlibDeprecationWarning: Auto-removal of grids by pcolor() and pcolormesh() is deprecated since 3.5 and will be removed two minor releases later; please call grid(False) first.\n",
      "  mesh = ax.pcolormesh(self.plot_data, cmap=self.cmap, **kws)\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\seaborn\\matrix.py:312: MatplotlibDeprecationWarning: Auto-removal of grids by pcolor() and pcolormesh() is deprecated since 3.5 and will be removed two minor releases later; please call grid(False) first.\n",
      "  cb = ax.figure.colorbar(mesh, cax, ax, **self.cbar_kws)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = eclf_sentiment.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')\n",
    "plt.title('Review Sentiment Classification');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc1efc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier(estimators=[('Ridge Classifier',\n",
      "                              RidgeClassifier(alpha=2.330395678259211,\n",
      "                                              normalize=False,\n",
      "                                              solver='sparse_cg', tol=0.01)),\n",
      "                             ('Perceptron', Perceptron(penalty='l1')),\n",
      "                             ('Passive Aggressive Classifier',\n",
      "                              PassiveAggressiveClassifier(C=953.6843579744861)),\n",
      "                             ('Complement Naive Bayes',\n",
      "                              ComplementNB(alpha=517.3986277024461)),\n",
      "                             ('Multinomial Naive Bayes',\n",
      "                              MultinomialNB(alpha=0.01)),\n",
      "                             ('Bernoulli Naive Bayes', BernoulliNB(alpha=0.01)),\n",
      "                             ('LinearSVC', LinearSVC(dual=False, tol=0.001))])\n"
     ]
    }
   ],
   "source": [
    "# RandomSearch\n",
    "params = {'Ridge Classifier__alpha':uniform(loc=1.0, scale=1000), 'Ridge Classifier__normalize':[True, False],\n",
    "         'Perceptron__penalty':['l2','l1','elasticnet'],\n",
    "         'Passive Aggressive Classifier__C':uniform(loc=1.0, scale=1000),\n",
    "         'Complement Naive Bayes__alpha':uniform(loc=1.0, scale=1000), 'Complement Naive Bayes__norm':[True, False],\n",
    "         }\n",
    "eclf_sentiment_RS = RandomizedSearchCV(eclf_sentiment, params, random_state=101, n_iter=50)\n",
    "search_sentiment = eclf_sentiment_RS.fit(X_train, y_train)\n",
    "print(search_sentiment.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c77c0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.892215215982085"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After 2.5 hours, the RandomSearchCV improved the model by 0.02%\n",
    "eclf_sentiment_best = search_sentiment.best_estimator_\n",
    "eclf_sentiment_best.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d082c080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Vader Sentiment Analysis to improve the model\n",
    "# I didn't end up using this in the final model as it didn't add any predictive power\n",
    "def sentiment_scores(sentence):\n",
    "    \"\"\"\n",
    "    Creates a sentiment score for text\n",
    "    Returns 1 for positive, 0 for neutral, -1 for negative sentiment polarity reviews\n",
    "    \"\"\"\n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    # polarity_scores method of SentimentIntensityAnalyzer\n",
    "    # oject gives a sentiment dictionary.\n",
    "    # which contains pos, neg, neu, and compound scores.\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    # +/- 0.05 is the cutoff the authors suggest for neutrality\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        return 1\n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        return -1\n",
    "    else :\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "af284705",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentiment = df_big['review'].apply(sentiment_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4fa15799",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big['predicted_sentiment'] = predicted_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8785bcc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: \n",
      "VotingClassifier(estimators=[('Ridge Classifier',\n",
      "                              RidgeClassifier(solver='sparse_cg', tol=0.01)),\n",
      "                             ('Perceptron', Perceptron()),\n",
      "                             ('Passive Aggressive Classifier',\n",
      "                              PassiveAggressiveClassifier()),\n",
      "                             ('Complement Naive Bayes',\n",
      "                              ComplementNB(alpha=0.01)),\n",
      "                             ('Multinomial Naive Bayes',\n",
      "                              MultinomialNB(alpha=0.01)),\n",
      "                             ('Bernoulli Naive Bayes', BernoulliNB(alpha=0.01)),\n",
      "                             ('LinearSVC', LinearSVC(dual=False, tol=0.001))])\n",
      "train time: 12.697s\n",
      "test time:  0.176s\n",
      "accuracy:   0.892\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('VotingClassifier',\n",
       " 0.8921562849902764,\n",
       " 12.696874380111694,\n",
       " 0.1760399341583252)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unigrams and bigrams, ignore words that appear in more than 75% of docs\n",
    "tf = TfidfVectorizer(ngram_range=(1, 2), max_df=0.75)\n",
    "\n",
    "X = df_big['review_stemmed']\n",
    "y = df_big['voted_up']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "X_train = tf.fit_transform(X_train)         #fit_transform TfidfTransformer on training data\n",
    "X_test = tf.transform(X_test)               #transform TfidfTransformer on testing data\n",
    "\n",
    "# Ensemble Voting classifier\n",
    "# Uses all 7 classifiers to predict a class. Majority vote wins.\n",
    "rc = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n",
    "pc = Perceptron(max_iter=1000)\n",
    "pac = PassiveAggressiveClassifier()\n",
    "cnb = ComplementNB(alpha=0.01)\n",
    "mnb = MultinomialNB(alpha=0.01)\n",
    "bnb = BernoulliNB(alpha=0.01)\n",
    "lsvc = LinearSVC(penalty='l2', dual=False, tol=1e-3)\n",
    "\n",
    "eclf_sentiment = VotingClassifier(\n",
    "     estimators=[('Ridge Classifier', rc), ('Perceptron', pc), ('Passive Aggressive Classifier', pac),\n",
    "                 ('Complement Naive Bayes', cnb), ('Multinomial Naive Bayes', mnb), ('Bernoulli Naive Bayes', bnb),\n",
    "                 ('LinearSVC', lsvc)\n",
    "                ],\n",
    "     voting='hard')\n",
    "\n",
    "benchmark(eclf_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c3587ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some example test reviews\n",
    "a = [\"\"\"Doomed to die.\n",
    "\n",
    "MGH is a fairly well polished, well supported, flashy prop hunt game which I can imagine no future for apart from it being ultimately consumed by an increasingly competitive, ever dwindling community peppered with toxic dirtbags. For those of you who have played your fair share of online competitive multiplayer games, this is a pattern I am sure you are familiar with.\n",
    "\n",
    "1. Notice the game in the \"New and Trending\" list\n",
    "2. Decide to fork over $20 to try it out\n",
    "3. Have a blast for 2-3 hours while you slowly start to understand the nuances of the mechanics\n",
    "4. Eventually get matched into a game with people who have been playing the game nonstop since beta\n",
    "5. Realize that apart from a few key abilities and weapons that are easily exploitable, most of the hunter upgrades you spent hours unlocking are pretty useless compared to the harpoon gun + ecto sensor\n",
    "6. Realize that hiding as a ghost is less effective than simply finding the smallest object in a level and spending the whole game darting around, dodging the hunter's attacks until midnight\n",
    "\n",
    "The game advertises itself as a fun \"hide and seek\" game. This is false. MGH game is basically Call of Duty dressed up to look like a fun prop hunt game to play casually.\n",
    "\n",
    "Unfortunately, I don't see these as issues that can be fixed through anything short of a total overhaul of the mechanics. The developer's claim that the \"win rates hover around 50 / 50\" simply reinforces my opinion that any nerfs made to gameplay elements like hunter ammo, harpoon limitations, etc. will actually throw the game balance out of whack unless nerfs are made to the ghosts as well.\n",
    "\n",
    "Overwatch had a similar issue when the pro scene discovered GOATS comp, which ultimately led to the enforcement of team comp in QP and Ranked play.\n",
    "\n",
    "Bottom line: there's just no good solution to mechanics that are this fundamentally opposed to what the game is advertised as.\n",
    "\n",
    "Before I wrap up, I do want to recognize that the developers seem to be very responsive, passionate, and invested in working with their community to make this game great. Sadly, I don't expect this to pan out in the long run, but I wish them the best!\"\"\",\n",
    "    \"\"\"Terrible game\"\"\"]\n",
    "a = tf.transform(a)\n",
    "eclf_sentiment.predict(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "72565816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove_stopwords(rev[0])\n",
    "b = get_stems(remove_stopwords(rev[0].split()))\n",
    "# b = list(b)\n",
    "b\n",
    "b = tf.transform([b])\n",
    "eclf_sentiment.predict(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244b76ce",
   "metadata": {},
   "source": [
    "### Saving Sentiment Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "bc659869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "file_name = 'sentiment_classifier_best_final.pkl'\n",
    "\n",
    "with open(file_name, 'wb') as file:\n",
    "    pickle.dump(eclf_sentiment,file)\n",
    "    \n",
    "with open('tfidf_sentiment_best_final.pkl', 'wb') as output:\n",
    "    pickle.dump(tf, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cc38a0",
   "metadata": {},
   "source": [
    "## Genre Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d28ad9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigrams and bigrams, ignore words that appear in more than 75% of docs\n",
    "tf = TfidfVectorizer(ngram_range=(1, 2), max_df=0.75)\n",
    "\n",
    "X = df_big['review_stemmed']\n",
    "y = df_big['genre']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "X_train = tf.fit_transform(X_train)         #fit_transform TfidfTransformer on training data\n",
    "X_test = tf.transform(X_test)               #transform TfidfTransformer on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e8195fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5934350875125228"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "edd06c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RidgeClassifier(solver='sag', tol=0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:729: UserWarning: \"sag\" solver requires many iterations to fit an intercept with sparse inputs. Either set the solver to \"auto\" or \"sparse_cg\", or set a low \"tol\" and a high \"max_iter\" (especially if inputs are not standardized).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 18.985s\n",
      "test time:  0.045s\n",
      "accuracy:   0.598\n",
      "\n",
      "Ridge Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RidgeClassifier(solver='sparse_cg', tol=0.01)\n",
      "train time: 13.044s\n",
      "test time:  0.075s\n",
      "accuracy:   0.598\n",
      "\n",
      "Perceptron Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "Perceptron()\n",
      "train time: 2.351s\n",
      "test time:  0.073s\n",
      "accuracy:   0.563\n",
      "\n",
      "Passive Aggressive Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "PassiveAggressiveClassifier()\n",
      "train time: 4.881s\n",
      "test time:  0.070s\n",
      "accuracy:   0.577\n",
      "\n",
      "l1\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(max_iter=50, penalty='l1')\n",
      "train time: 5.121s\n",
      "test time:  0.075s\n",
      "accuracy:   0.373\n",
      "\n",
      "l2\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(max_iter=50)\n",
      "train time: 2.517s\n",
      "test time:  0.083s\n",
      "accuracy:   0.581\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(max_iter=50, penalty='elasticnet')\n",
      "train time: 4.325s\n",
      "test time:  0.070s\n",
      "accuracy:   0.497\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB()\n",
      "train time: 0.570s\n",
      "test time:  0.045s\n",
      "accuracy:   0.465\n",
      "\n",
      "Bernoulli Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "BernoulliNB()\n",
      "train time: 0.607s\n",
      "test time:  0.232s\n",
      "accuracy:   0.368\n",
      "\n",
      "Complement Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "ComplementNB()\n",
      "train time: 0.648s\n",
      "test time:  0.051s\n",
      "accuracy:   0.586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing out many classifiers to see what to include in the ensemble classifier\n",
    "\n",
    "results = []\n",
    "for clf, name in (\n",
    "    (RidgeClassifier(tol=1e-2, solver=\"sag\"), 'Ridge Classifier'),\n",
    "    (RidgeClassifier(tol=1e-2, solver=\"sparse_cg\"), 'Ridge Classifier'),\n",
    "    (Perceptron(max_iter=1000), 'Perceptron Classifier'),\n",
    "    (PassiveAggressiveClassifier(), 'Passive Aggressive Classifier'),\n",
    "):\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))\n",
    "\n",
    "for penalty in ['l1', 'l2']:\n",
    "    print(f'{penalty}')\n",
    "    results.append(LinearSVC(penalty=penalty, dual=False, class_weight='balanced'))\n",
    "    results.append(benchmark(SGDClassifier(alpha=0.0001, max_iter=50, penalty=penalty)))\n",
    "\n",
    "results.append(benchmark(SGDClassifier(alpha=0.0001, max_iter=50, penalty=\"elasticnet\")))\n",
    "\n",
    "for clf, name in (\n",
    "    (MultinomialNB(), 'Multinomial Naive Bayes'),\n",
    "    (BernoulliNB(), 'Bernoulli Naive Bayes'),\n",
    "    (ComplementNB(), 'Complement Naive Bayes'),\n",
    "):\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c6587fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: \n",
      "VotingClassifier(estimators=[('Ridge Classifier',\n",
      "                              RidgeClassifier(class_weight='balanced',\n",
      "                                              solver='sparse_cg', tol=0.01)),\n",
      "                             ('Perceptron',\n",
      "                              Perceptron(class_weight='balanced')),\n",
      "                             ('Passive Aggressive Classifier',\n",
      "                              PassiveAggressiveClassifier()),\n",
      "                             ('Complement Naive Bayes', ComplementNB()),\n",
      "                             ('Logistic Regression',\n",
      "                              LogisticRegression(class_weight='balanced',\n",
      "                                                 multi_class='multinomial',\n",
      "                                                 solver='saga'))])\n",
      "train time: 17.623s\n",
      "test time:  0.209s\n",
      "accuracy:   0.594\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('VotingClassifier',\n",
       " 0.5940243974306088,\n",
       " 17.622549533843994,\n",
       " 0.20922017097473145)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using an Ensemble voting classifier to get the best predictions from multiple models\n",
    "rc = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\", class_weight='balanced')\n",
    "pc = Perceptron(max_iter=1000, class_weight='balanced')\n",
    "pac = PassiveAggressiveClassifier()\n",
    "cnb = ComplementNB()\n",
    "lr = LogisticRegression(solver='saga', multi_class='multinomial', class_weight='balanced')\n",
    "\n",
    "eclf_genre = VotingClassifier(\n",
    "     estimators=[('Ridge Classifier', rc), ('Perceptron', pc), ('Passive Aggressive Classifier', pac),\n",
    "                 ('Complement Naive Bayes', cnb), ('Logistic Regression', lr)],\n",
    "     voting='hard')\n",
    "\n",
    "benchmark(eclf_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8e22a233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      action       0.51      0.73      0.60      4244\n",
      "   adventure       0.59      0.53      0.56      2952\n",
      "         rpg       0.67      0.55      0.60      2449\n",
      "  simulation       0.58      0.54      0.56      2931\n",
      "      sports       0.75      0.60      0.66      1668\n",
      "    strategy       0.72      0.57      0.64      2725\n",
      "\n",
      "    accuracy                           0.60     16969\n",
      "   macro avg       0.64      0.59      0.60     16969\n",
      "weighted avg       0.62      0.60      0.60     16969\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\seaborn\\matrix.py:302: MatplotlibDeprecationWarning: Auto-removal of grids by pcolor() and pcolormesh() is deprecated since 3.5 and will be removed two minor releases later; please call grid(False) first.\n",
      "  mesh = ax.pcolormesh(self.plot_data, cmap=self.cmap, **kws)\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\seaborn\\matrix.py:312: MatplotlibDeprecationWarning: Auto-removal of grids by pcolor() and pcolormesh() is deprecated since 3.5 and will be removed two minor releases later; please call grid(False) first.\n",
      "  cb = ax.figure.colorbar(mesh, cax, ax, **self.cbar_kws)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAEJCAYAAABIRuanAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABdwElEQVR4nO3dd1RURx/G8S+79F4EUayIDRsqFuy9azTFxCS2aDSJRiPG2FvsBQuKGktsMUVji4kVUTFiwW5siGCnV+mwO+8fvG7EAojCrmQ+nj2HvXvLsyv8GObOnasnhBBIkiRJOkWh7QCSJEnS82RxliRJ0kGyOEuSJOkgWZwlSZJ0kCzOkiRJOkgWZ0mSJB0ki3MhqVChAjNnztR2jGJp2rRpuLi4FNnxNmzYgL6+fo5lR48epWbNmhgYGNCqVSvu3LmDnp4ef//9d6HnadWqFYMHDy7040ja9Z8qzgMGDEBPTw89PT2USiVlypShX79+PHz48I0fKzAwkFGjRr3x/eYlNTWV2bNnU69ePczNzbG0tKRWrVqMGDGCGzduFHmeV5WSksLMmTOpXbs2pqam2Nra0qhRI5YtW0ZKSopWMn344YfPfY98+eWX1KtXj5CQEHbs2EHZsmUJCwujUaNGb+y4M2fOpEKFCs8t37FjB4sWLXpjx5F0k37eqxQvzZs3Z+vWrahUKm7fvs2wYcP44IMPCAgIeKPHsbe3f6P7y4/ExERatmxJWFgYU6dOpWHDhlhbWxMSEoKvry9Tpkxh69athZohMzMTfX199PT0XnnbJ/kfPXrE999/T6NGjbCysuLs2bN4e3tTtmxZevbs+eZD58HExAQTE5Mcy27dusWECRMoW7asZpmjo2OR5LG1tS2S40haJv5D+vfvL9q2bZtjmbe3twBEQkKCZtnZs2dF+/bthZmZmShRooTo1auXuHPnjhBCiKCgIAGIEydO5NjPqVOnBCCuX78uhBCifPnyYsaMGZrXMzMzxdSpU0WFChWEkZGRcHV1FatWrdK8PnHiRNG0aVPNcz8/PwGIiRMnapZNmTJFNGjQ4KXvb/jw4cLExEST9VlqtTrH819++UXUqVNHGBkZifLly4tRo0aJpKQkzestW7YUgwYNEt9//70oWbKksLGxEf3798+xzpPP1NvbW5QvX17o6emJx48fi/DwcNG/f39RokQJYW5uLpo0aSKOHTv20uxP8hsbG4uQkJAXZo+LixNCCDF16lRRqVIlzWshISGiV69eolSpUsLExETUrFlTbNq0Kcf2x48fF02aNBHm5ubC3Nxc1K5dW+zfv1/z+qxZs0TFihWFoaGhKFGihOjQoYNISUkRQgixfv16oVQqhRBCHDlyRAA5HuvXrxehoaECEMePH9fsMyIiQgwYMEA4ODgIIyMjUaVKFbFu3TrN+xk8eLBwdnYWxsbGomLFimL8+PEiLS1Nc8xnjzN16tQc/y9PZGRkiLFjx4rSpUsLAwMDUb16dbFly5Yc7x8QPj4+4tNPPxXm5uaiTJkyYt68ebn+f0ja9Z8uzg8fPhQtWrQQSqVSU3CuXr0qzMzMxJQpU8T169fF5cuXxfvvvy8qV64sUlNThRBCNG7cWAwZMiTHvocNGyYaNmyoef5sce7fv7+oVauWOHDggAgJCRG//vqrsLKyEmvXrhVCCHH48GGhr68vHj9+LIQQYtKkScLe3l40btxYs49mzZqJsWPHvvC9qVQqYWtrKz7//PN8fRbr168X1tbWYtOmTeL27dvi2LFjolatWuLTTz/VrNOyZUthZWUlvvnmG3H9+nWxb98+YWVlJaZMmZLjfVlYWIiePXuKCxcuiMuXL4vExERRvXp18e6774rAwEBx69YtMXPmTGFoaCiuXbuWa/6ni87LPFucL1++LJYvXy4uXbokgoODhbe3t1AqlcLPz08IIURWVpawsbERo0aNEkFBQSIoKEjs2LFD+Pv7CyGE2L59u7CwsBB//PGHuHv3rrhw4YJYvHjxC4tzenq6CAsLE4BYvny5CAsLEykpKc8V55SUFFGtWjVRt25dcejQIXH79m1x4MAB8csvv2je78SJE8WpU6dEaGio2L17t3B0dNR8tikpKWLs2LGiTJkyIiwsTISFhWm+N54tzt9++62wtbUVW7duFTdv3hSzZs0Senp6wtfXV7MOIBwcHMTq1atFcHCwWLp0qQA0n5Gke/5zxVmpVAozMzNhYmKiaZGMHj06xzoffvhhju3S0tKEiYmJ2LlzpxBCiJUrVwpra2tNKycjI0OUKFFCLF++XLPN08U5JCRE6OnpaVrVT0yfPl3UqVNHCCFEamqqMDY2Fn/99ZcQQogmTZqIhQsXCn19fZGQkCCSk5OFoaGhOHDgwAvfW0REhADEokWLciz/6KOPhJmZmebxdL6VK1fmWPfYsWMCELGxsUKI7CJQq1atHOsMHTo0xy+M/v37CysrK03hECK7mDk5OYnMzMwc27Zu3VqMHDky1/xeXl4vfP1pzxbnF+nRo4cYPHiwEEKI2NhYAYgjR468cN1FixaJypUri4yMjBe+/nRxfgIQmzdv1jx/tjivXbtWGBkZifv37+f5fp7O4eLionk+Y8YMUb58+efWe7o4P/m+8PHxybFOz549RevWrXPk/frrr3OsU7VqVTFu3Lh855OK1n/qhCBAo0aNuHjxImfOnGHy5Mk0btyYGTNmaF4PDAxk586dmJubax52dnakpaVx69YtIPsEUWpqKn/88QcAe/fuJTExkY8++uiFxzx79ixCCNzd3XPsd/bs2Zp9Ghsb4+HhgZ+fH0lJSQQGBvLRRx9RpUoV/P39OX78OADNmjV74THES+avWrx4MRcvXmT69OkkJycDEBUVxd27d/H09MyRp3PnzgAEBwdrtndzc8uxPycnJyIiInIsq169Oubm5jk+w/DwcKytrXPs//jx45r3+7L8BemrTklJYdy4cdSoUQNbW1vMzc3Zu3cvd+/eBcDGxobBgwfTsWNHOnfuzNy5c7l586Zm+969e5OZmUn58uUZMGAAmzdv5vHjx6+c42nnzp3D1dWVMmXKvHSdNWvW0KhRI0qWLIm5uTnjx4/XZM6v4OBgMjIyaNGiRY7lLVu25OrVqzmW5ef/UtId/7kTgiYmJpphWDVr1iQoKIhhw4bx448/AqBWq+nbty/jxo17bls7Ozsg+4e9e/fubNq0iQ8++IBNmzbRtWtXzevPUqvVAAQEBGBqaprjtaeLUZs2bdi+fTtt27bF2dkZJycn2rRpw+HDhzE0NKRRo0bPbf+Evb09NjY2XLt2LcfyJyepSpYs+VyepUuX0rp16+f29XRBMTQ0fC7vk+2fMDMze+79Vq9enZ07dz6377zyP1tQ8mPMmDHs3r0bLy8vqlWrhpmZGaNHjyYhIUGzzpo1axg5ciQHDx7k0KFDTJ48meXLlzN06FCcnJy4ceMGR44cwc/PjxkzZjB27FhOnz6d44Tfq8rtF822bdsYNmwYc+fOpWXLllhaWrJt2zYmTpz4Ro4lhHhuWX7+LyXd8Z9rOT9r2rRpbNy4kbNnzwLg7u7O5cuXqVSpEi4uLjkeNjY2mu369evH/v37uXnzJn/99Rf9+/d/6THq168PwL17957bZ6VKlTTrtWnThkuXLrFt2zbatm2rWebn54efnx9t2rR56TEUCgUff/wxW7ZsITQ0NNf3XLJkScqWLcvNmzefy+Pi4oKxsXHeH1wu3N3dCQkJwdLS8rl9ly5dukD5hRA5iu3T/P39+eSTT/jwww+pU6cOzs7OBAUFPbdezZo18fT0ZN++fQwaNIjVq1drXjMyMqJTp07Mnz+fK1eukJKSwq5duwr2AZD9f3716lUePHjw0sx169bF09OT+vXrU7lyZe7cuZNjHUNDQ1QqVa7HcXFxwcjIiGPHjj23/xo1ahQ4v6R9//niXK1aNbp168b48eMBmDBhAtevX+fTTz/lzJkzhIaGcuTIEUaOHElISIhmu86dO2Nra8tHH32EhYUFXbp0eekxXFxc+Oyzz/j888/ZvHkzwcHBXLp0iR9//JF58+Zp1mvYsCFmZmZs3rxZU4hbtWrF1atXOX/+fK7FGWDWrFlUrlyZxo0bs2LFCs6dO8edO3c4cuQIP//8MwqFIse63t7ezJw5k3/++YebN2+ya9cuhg4dWqDP8WmffPIJFStWpGvXrhw8eJA7d+5w+vRp5syZk2vBezr/6tWruXTpEqGhoezcuZOWLVty5MiRF25XtWpVdu/ezZkzZ7h27RpDhgzh0aNHmteDg4MZO3Ysf//9N3fv3uXkyZMcP34cV1dXANatW8eaNWu4dOkSd+/eZcuWLTx+/FjzekH06dOH8uXL06NHD3x9fQkNDeXw4cP89ttvmsxXrlxh9+7d3L59m6VLl7Jjx44c+6hYsSLh4eGcPHmS6OjoF47zNjU1ZcSIEUyePJlt27Zx69YtZs+eze7du5kwYUKB80s6QKs93kXsRUPphBDi77//FoDm7Pbly5dFjx49hLW1tTA2NhaVKlUSn3/+uYiJicmx3TfffCMAMXz48Of2+exojaysLDFv3jxRtWpVYWBgIOzs7ESLFi3E1q1bc2zXpUsXoaenJ6KjozXL6tWrJ0xMTER6enqe7zE5OVnMmDFD1KlTR5iYmAhDQ0NN/suXL+dYd+fOnaJx48bCxMREWFhYiDp16ojp06drXn92VIAQz5+ketlnGh0dLb744gvN8K7SpUuLnj17ivPnz+eaPykpSUybNk3UqFFDGBsbC2tra9GwYUOxfPlyzeiJZ08I3rt3T3To0EGYmppqRjx89tlnomXLlkIIIR49eiR69eolnJychKGhoShVqpQYPHiwiI+PF0Jkj9bw8PAQ1tbWwsTERNSoUUMzikaIgp0QFEKIsLAw0bdvX2FnZyeMjIxE1apVxfr164UQ2SeRhwwZImxsbISFhYXo06ePWLZsmXj6RzIjI0P06dNH2NjYvJGhdE/nFUKItm3biv79++f6/yFpj54Q8k4okiRJuuY/360hSZKki2RxliRJ0kGyOEuSJOkgWZwlSZJ0UKFfhJIZHZL3SjrGrUYfbUd4JbHpr3c1mzaYG5jkvZKOufc4UtsRXomF4dv3GUcnPj8+/VW9Ss0xKOH82scrLP+5KwQlSSrm1LlfuPO2kMVZkqTiRRSPS9JlcZYkqXgpJvOFyOIsSVKxIt5QyzkjI4OpU6eSlZWFSqWicePG9O7dm6SkJBYvXkxUVBT29vaMGjVKMyvjzp078fPzQ6FQMHDgQM1MgCEhIfj4+JCRkUHdunUZOHBgnjMwytEakiQVL6qs/D9yYWBgwNSpU1mwYAHz58/n4sWLBAUFsWvXLmrVqoW3tze1atXSzBfz4MEDAgICWLRoERMnTmTdunWaWf/WrFnD0KFD8fb2Jjw8nIsXL+b5NmRxliSpeFGr8v/IhZ6enmaGRpVKhUqlQk9Pj8DAQFq2bAlkz5sdGBgIZM9j3qRJEwwMDHBwcMDR0ZHg4GDi4uJITU2lSpUq6Onp0aJFC802uZHdGpIkFS9v8ISgWq1m7NixhIeH07FjRypXrkxCQoJm+mAbGxsSExMBiI2NpXLlypptbW1tiY2NRalU5pjr3c7OjtjY2DyPLYuzJEnFyyucEPT19cXX11fzvF27drRr107zXKFQsGDBApKTk1m4cCH37t176b5eNodcQeeWk8VZkqRi5VVOCD5bjF/GzMwMV1dXLl68iJWVFXFxcdjY2BAXF4elpSWQ3SKOiYnRbBMbG4utre1zy2NiYrC1tc3zmLLPWZKk4kWtzv8jF4mJiZr7bmZkZHDlyhWcnJxwd3fX3Hnm2LFjNGjQAMi+A1BAQACZmZlERkYSFhamuYOSiYkJQUFBCCHw9/fH3d09z7chW86SJBUvqsw3spu4uDh8fHxQq9UIIfDw8KB+/fpUqVKFxYsX4+fnR4kSJfD09ASgbNmyeHh44OnpiUKhYNCgQZq7Dw0ePJgVK1aQkZGBm5sbdevWzfP4hT7Zvpxbo/DJuTWKhpxbo/C9ibk10q+/+HZmL2JU/fkbHOsK2XKWJKl4kVcIvnnp6Rn0HzaGjMxMVFkq2rduxvDBfTngd5wV634i5O59flmzhJrVq2i2WbPpN3b8eQClQsH4UV/StFF9UtPS8Jw0mwcPw1AoFLRq1ohRX35WqNkNjQzZtHsVhoaGKJVKDv7ph8+CNQB8POgDPv7sA1RZKvx9T+A1YzkeLRoyatIwDAz1yczIwut7b07/fa5QMz6rtJMj3qvm4OBQArVa8NPGraxd9ROTv/+WDp1akZGZyd3Q+3wzbCKJCY8pU640/qf/5HbwHQDOB15irOf0Is1saGTIz3+swdDQEH19Jfv3HMZ7/g906tGOEWOGUKlKRd7r0I9/Ll0HwNrGimU/zqdWXVd2/LqH78fNL9K8z7KysmTVyvnUqFEVIQRDhn5Lamoay5fNwdjYiKwsFSNGTuTs2Ytay7jUZzYdOrUmOiqG5o27AbB2/RIqVa74//dgQULCY1o3ewcA1xpV8Vr6PRYW5qjVatq3eo/09Ayt5ZdzaxQCQ0MDfvSei6mpCZlZWfT78luaN3bHxbk8S2ZPZvoC7xzr3w69y77Dx9j90yoio2MZPHI8f/26FoCBfd6jYf06ZGZmMmjEeI6fDKS5R4NCy56RnsFn7w4jJSUVfX0lm/es5rjfSYyNjWjTqQW9Wn9CZkYmtiWyx0fGxcYzrO9ooiKicanmzOpfl9LGrXuh5XuRrKwspk+az5VL1zEzN+XA0d/xP3IS/yMBzJ6+GJVKxcRpnnw96nNmTVsEwN3Q+7Rv/m6R5nxaRnoG/d79gpTkVPT19fn1z3X4Hz7BrevBDBswhhleOe84nZ6ezpK5K6lSrRKVq1fSUup/eXlN4+Cho/T5+AsMDAwwNTXh5y0rmTVrMQcOHqVTx9bMnj2BDh16ay3jr1t2sG71T/j88O8vssEDv9F8/f2scSQmZnelKZVKVq5ZwFdDvuPqPzewsbUmMzP3K+8KnWw5v3l6enqYmmb3k2VlZZGVlYWenh6VKpR74fp+x0/RuW1LDA0NKVPakXJlSnPlehBuNavTsH4dIPsSzOpVXYiIii70/CkpqQDoG+ijr6+PEIIP+7/L2mWbyMzIPkkRGx0HwI1//u1bC74RgpGREQaGBpr1ikJkRDSREdmfS3JSCreCQnAs5cCxIwGadc6fvUS3Hh2LLFN+pCQ/9Tkb6CME3L5154Xrpqakce70RcpXLFuECV/MwsKc5s0aMXhw9gmkzMxMEhIyEUJgYWkBgKWVJWFhEdqMycmAs5Qt5/TS19/p1Zle3fsB0LptM65dvcnVf24A2Y0ObRPqovsZKkw6N5ROpVLxXv9htOjWB48Gdaldo9pL142MisGxpL3meUmHEkQ+U4QTHydx7MRpGtV3K6zIGgqFgu2HN3P86n5OHjvDlfNXqVCpHPUbufHLvnVs2LmSmm7Vn9uuQ7c2XP/nZpEW5meVKVeaWrWqc/7c5RzLP/r0Xfx8j2uelyvvxEH/7ez4ayONPOoXdUwg+3P+48jPnLp+iBNHT3Hp/D9ayfGqKlYsR1RULGvWLOL0qX2sXDkfU1MTvv12GnPmTCQ4+DRz50xi8uS52o76Uh5N3ImKjCbk9l0AKrlUQAjYunMdfv47+XrkYC0n5I0NpdO2PFvODx8+JDAwkNjYWPT09LCxscHd3Z0yZcq8dJunr7qZ8e2QVwqkVCrZvtGHxMdJjBw/g1shd6jsXOGF6wqeH2iix78zPWVlqfhu2jw+eb8HZZ1KvVKOglCr1bzXti8WluZ4b5iPSzVnlPpKLK0t6NN5ELXquuK1ZjYdG/TSbFOpakVGTR7GkN4jCj3fy5iambJu01KmTJhD0uNkzfKRo4eiylKxfeseACLDo3Cv2Za4uARq13Hlxy3LaOXRI8c2RUGtVtOj9cdYWJqzYqMXlatV4taN20WaoSD09fWpW7cmozwnExh4Ea+F0xgzZhhWlhaMGTOdXbv28d573fhh1QI6d/lY23Ff6N33u7Hj9780z/WVSho1rkf7Vu+TmprKjj0buXjxKsePndReyGLS55xry3nXrl0sWbIEABcXFypVyu6zW7p0qWYmphdp164dc+fOZe7cgrcALC3MaVCvNn+fOvvSdUralyA8IkrzPCIyGnv7f69hnzZ/KeXKlKbvh71etHmheZyYxJkT52jW2oOIR5H4/nUUgCsXrqFWq7GxswagZCkHvNfPZ8Lw6dy/+7BIMz6hr6/Puk1L2LHtT/bu+fcy1g/6vEO7ji0Z9vl3mmUZGZnExSUAcPnSNe7euU+lShWKOrLG48QkTp84S4s2TbSW4VU8fBjGg4dhBAZeBGDHzr3UdavJp5++z65d+wDYvv1P3N3dtBcyF0qlkq49OrBzx7/F+dGjCAJOBBIbG0dqahq+B49Rp46rFlPyxiY+0rZci/ORI0eYM2cOPXv2pEWLFrRo0YKePXsyZ84c/Pz83niY2Lh4Eh8nAZCWns6pwAtULP/yvsLWzRqz7/AxMjIyePAonHsPHlHr/yM5vFdvJCkphXEjh77xnC9iY2eNhWX2nK5GxkZ4tGhIaPAdDu87RqNm2VcDlXcui4GBAXEx8VhYmrNyyyKWzFrBhcDLue26UC1aPoNbQSH84LNRs6x122YMHzmYAX2GkZqaplluZ2ejGVRfrnwZKjqX5+6dB0Wa1/aZz7lJy0aEvKS/WddERETx4EEYVSpn37eudeumXL9+i7CwCFq0aKxZFhwcqs2YL9WydROCg0IIe/Rvn7jf4ePUqFEVExNjlEolTZo25OZNLf8VI9T5f+iwXLs19PT0iIuLw97ePsfyuLi4PCeKLoiomDgmzlyISq1GqAUd2zSnVdNG+B47wZzFK4mNT+CrMVOpVtmZ1Ytn4eJcno5tmtPjk6HoK5VM9PwKpVJJeGQUqzf+SsXyZflg4NcA9HmvO+/36PTGMz9hX7IEs72noFAqUCgUHNh9mGOHTmBgoM+MJZPYdexnMjMymTgie+jZx4M+oGzFMnzh+RlfeGYP8/v8wxGaE4ZFoWHjenzw0Ttcu3qTQ8d3ADDn+yXMnDcBQ0MDft21Dvh3yFzjpu6MGf81Waos1Co1Yz2nEx+fUGR5Iftznr98OgqFEoVCj327fTly6Djtu7Rmypwx2NrZsObnpVy/GsRnvYcDcOTcHswtzDAwNKB951YM/GAYwUHaKYCjRk1mw4ZlGBoaEBp6j8+HjGbPnwfxWjgNfX190tLS+WrYOK1ke2L1j4to2qwhtnY2XL7uz7zZ3mzZ/Du93uvKjt//zLFuQnwiK33Wc+jodoQQ+B48xqEDR7UT/Akd70vOr1yvELx48SLr1q2jVKlSminvoqOjCQ8PZ9CgQZpZ/nMjrxAsfPIKwaIhrxAsfG/iCsG0E1vyva5x009e+3iFJdeWs5ubG0uXLiU4OFgz/6itrS0uLi6aP28lSZJ0SjFpOec5WkOhUFClSpW8VpMkSdIJQuj2ib780qmLUCRJkl7bf6XlLEmS9FbR8VEY+SWLsyRJxYtsOUuSJOkglZYnXnpDZHGWJKl4kd0akiRJOkh2a0iSJOkgWZwlSZJ0kOzWkCRJ0kHyhKAkSZIOkt0akiRJOkh2a0iSJOkg2XLOn6/cxxb2Id64jQaFf0urN6l9coy2I7wyA4VS2xGKveTMdG1H0A5ZnCVJknTQy6eof6vI4ixJUvGSJUdrSJIk6Z43dEIwOjoaHx8f4uPj0dPTo127dnTp0oWtW7dy+PBhLC0tAejTpw/16tUDYOfOnfj5+aFQKBg4cKDmblEhISH4+PiQkZFB3bp1GThwYJ63+pPFWZKk4uUN9TkrlUr69u2Ls7MzqampjBs3jtq1awPQtWtXevTokWP9Bw8eEBAQwKJFi4iLi2PGjBksXboUhULBmjVrGDp0KJUrV2bOnDlcvHiRunXr5np8ea8pSZKKFyHy/8iFjY0Nzs7Zd0o3MTHByclJc7u+FwkMDKRJkyYYGBjg4OCAo6MjwcHBxMXFkZqaSpUqVdDT06NFixYEBgbm+TZky1mSpOLlFVrOvr6++Pr6ap63a9eOdu3aPbdeZGQkoaGhuLi4cOPGDQ4cOIC/vz/Ozs7069cPc3NzYmNjqVy5smYbW1tbYmNjUSqVmhtkA9jZ2eVa5J+QxVmSpOLlFYrzy4rx09LS0vDy8mLAgAGYmprSoUMH3n//fQB+++03Nm3axFdffYV4SUv8ZcvzIrs1JEkqVoRKle9HXrKysvDy8qJ58+Y0atQIAGtraxQKBQqFgrZt23L79m0gu0UcE/PvNQexsbHY2to+tzwmJgZbW9s8jy2LsyRJxYtanf9HLoQQrFq1CicnJ7p166ZZHhcXp/n6zJkzlC1bFgB3d3cCAgLIzMwkMjKSsLAwXFxcsLGxwcTEhKCgIIQQ+Pv74+7unufbkN0akiQVL29oKN3Nmzfx9/enXLlyjBkzBsgeNnfixAnu3LmDnp4e9vb2DBkyBICyZcvi4eGBp6cnCoWCQYMGoVBkt38HDx7MihUryMjIwM3NLc+RGgB6oqAdIvn0eYUPCnP3hWJoVqa2I7yS9vH/aDvCK7MwNNF2hFcWkRKv7QivRKH39v1hnJp697X3keIzPN/rmg5b/trHKyyy5SxJUvEi59aQJEnSQfk40fc20Om/e9oN6sr0g4uYdsCLz71Hom9kQP0ujZl+cBE/hPxG+VrOmnWVBvoMWPAVU/d7MWXfAqo0di2SjBUWDqfOxQ3U8F2qWVba8yNqn12H64HFuB5YjFWb+gDY9mqhWeZ6YDH17+3AxLVijv25/Dghx74K07IVcwgKPU3Amb2aZWMnjOBq0N/4B/yBf8AftO/QMsc2ZcqU4n74JYaPGFQkGZ9Vyqkkv+5ex+FTu/EN2MlnQz8BYNTYLznzjy/7jm1j37FttG7XPDtv2dIEPQzULJ/tNVkruZ+wsrLkl59XcfnSES5d9KNRo3pMmjSKkNuBnDm9nzOn99OpY2utZnzW118P4ty5Q5w9e5CNG70xMjJi4sRvuH37NKdO7eXUqb101KXMb+iEoLbpbMvZuqQtbQd0YUq7UWSmZzB0+Sgadm9KyIVbrPhiIX1nD8mxfvOP2gIwvdNoLOwsGblhIrN6jCvwGMP8it7mR+SGvVRcMjLH8og1fxDxw+4cy2J3+hO70x8Ak2rlcVk3ntRroZrXrTs3RpWSVqh5n/bLlh2s+eEnVq1ZkGP5yuXrWe697oXbzJo3Ed9D/kUR74VUWSpmTl7IP5evY2Zuyl9+v3H86EkA1q7azOrlG5/b5u6d+3RuqRvnPry8pnHw0FH6fPwFBgYGmJqa0L59S5YtW8viJT9oO95zSpcuyVdfDaRu3bakpaXz008+fPBBdwCWLVvHkiWrtZzwBdTFY1Y6nW45K5QKDIwNUSgVGJoYER8RS/jth0SEPHpu3dKVy3D9xBUAHsckkpKYTPnalQo9Y9Lpa2TFJ73ydrbvNCd293HNc4WpMSU/70HY0q1vMl6uAk4EEhcXn+/1u3Rrx93Q+9y4fqvwQuUhMiKafy5fByA5KYXgoFAcS5XUWp5XYWFhTvNmjVi//lcAMjMzSUhI1HKqvOnrKzExMUapVGJiYkJYWIS2I+VOqPP/0GE6W5zjI2I5uGYP8wJWsvDMGlIfp3Dt+OWXrn//+l3c2jdAoVRQoowD5Ws5Y1vK7qXrFzaHAV1xPbSECguHo7Qye+51m+7NiHmqODuN+ZiI1btRp2YUZcwX+nxoX/4+9SfLVszByjp75i1TUxNGjhrKvDnLtJzuX2XKlqZG7WpcOJf9fdF/cB8OHN/OgmXfY2VlqVmvbDkn9h7dytY962nYuJ624lKxYjmiomJZs2YRp0/tY+XK+ZiaZo9a+eLL/pwNPMgPPyzE2tpKaxmf9ehRBEuWrCYo6CShoYEkJj7m8OHs79svvujHmTP7WbVqAdbWlnnsqQipRf4fOqzAxfnIkSMvfc3X15dx48Yxbty4gu4eU0sz3No3YHzzYYxpNARDUyMa9Wz+0vVPbPUjLjyGSXvm8eHUAdw+dxOVlk4MRG7ax5WmX3CtwygyI+MoO3lgjtfN6lZGnZZO2s17AJi4VsSoQini95/WRtwcfly7hbq12tDcozsREVHMnD0egHETR7LSZz3JySlaTpjN1MyEHzYuZvqEeSQ9Tmbzj1tpXq8LnVq8T2R4FJNmfgtAZEQUjWt3oEur3syYtADvNfMwt3j+l2VR0NfXp27dmqxevYlGjTuTkpzCmDHDWL16M9WrN6NBw46Eh0cyb552+8WfZm1tSbduHahevRnOzg0xMzPho496sWbNT7i6tqBRo86Eh0cyd67uZBZqdb4fuqzAxXnr1pf/+d2uXTvmzp3L3LlzC7p7qjerRfT9SJJiE1Flqbiw/zSV6ld96fpqlZqtMzbyfZcx+Hw+HxNLMyJDwwt8/NeRFZ2QfbJBCKJ+PoSZW+Ucr9v2aE7srn9bzeb1q2JaqxK1Tq6m2s7ZGDmXpuq2mUUdG4CoyBjUajVCCDau/4367nUAcG9Qh+kzvuPS1aN8+dUAPL/9ks+H9tVKRn19fX7YuJidv//F/j8PAxAd9W/uXzZtx61eTQAyMjKJj0sA4Mqla9wNvY9zpfJayf3wYRgPHoYRGHgRgB0791LXrSaRkdGa7D/++DMN3N20ku9F2rRpxp0794mOjiUrK4tdu/bTuHH9ZzL/gvv/v090gkqV/4cOy/WE4LfffvvC5UIIEhISCiXQE7GPonGuWxlDY0My0jKo1rQWdy/ffun6hsaGoKdHRmo61ZvVRp2lIiz4QaFmfBkDBxsyI7Mv8bTp1IjU/7eQAdDTw6ZbE268N1GzKGrzfqI27wfAsIwDlTdM5OYHk4o08xMlS9oTEREFQLfuHbh+LQiALh36aNYZO2EEyUnJrPlhs1YyLvCeTnBQCGtXbNIscyhZgsiIaAA6dmvLzevBANja2RAfl4BaraZc+TJUdC7H3Tva+b6IiIjiwYMwqlR2JuhWCK1bN+X69Vs4OjoQHh4JwDs9OnH16k2t5HuR+/cf0bBhXUxMjElNTaN166acP38lZ+Z3OnLtmu5k1vXuivzKtTgnJCQwceJEzMxy/hkohGDy5ML9Myb0YjDn9p1i0l/zUWepuHf1Dv6/+FK3Y0P6TPsMc1tLRvw4nvvX77Ck3ywsSljxzcZJCKEmLjyWdZ5F0zdacbknFh410be1pHbgWh55/YqFR01MalQEIci4H8ndcSs161s0rkFGWAwZ97R/UmXt+sU0bd4IOzsb/rn5N3NnLaVZ80bUql0dIQT37j5k1Ajt/JJ4mQaN6vLeRz24fjWIfce2ATB/hjfvvNcZ11rVEELw4N5Dxnt+D0CjJvUZPX4YWVkqVCoVE0bPICFeeyfhRo2azIYNyzA0NCA09B6fDxnNokXTqVO7BkII7t59wLDhBe8OfNMCAy+yc+deTp78i6wsFZcuXWXdup9ZuXIetWu7ajJ//fUEbUf9l453V+RXrpdvr1y5ktatW1OtWrXnXlu6dCkjR458wVY5ycu3C5+8fLtoyMu3C9+buHw7ecpH+V7X7PtfX/t4hSXXlvOXX3750tfyU5glSZKKnI4Pkcsvnb0IRZIkqUD+C33OkiRJbxuRpdujMPJLFmdJkooX2XKWJEnSQbLPWZIkSQfJlrMkSZLuEbI4S5Ik6SB5QlCSJEkHyZazJEmSDpLFWZIkSfcU9t2PiooszpIkFS+y5SxJkqSDZHHOn5Cswp33uTB0fXwv75V0yHrzhtqO8Mq+SLug7QivTPWWTUVpoP/fbHuJrLfr/+ll/pv/e5IkFV/FozbL4ixJUvHypi5CiY6OxsfHh/j4ePT09GjXrh1dunQhKSmJxYsXExUVhb29PaNGjcLc3ByAnTt34ufnh0KhYODAgbi5uQEQEhKCj48PGRkZ1K1bl4EDB6Knp5fr8d++2bglSZJy84buvq1UKunbty+LFy9m1qxZHDhwgAcPHrBr1y5q1aqFt7c3tWrVYteuXQA8ePCAgIAAFi1axMSJE1m3bh3q/3eFrVmzhqFDh+Lt7U14eDgXL17M823I4ixJUvGifoVHLmxsbHB2dgbAxMQEJycnYmNjCQwMpGXLlgC0bNmSwMBAAAIDA2nSpAkGBgY4ODjg6OhIcHAwcXFxpKamUqVKFfT09GjRooVmm9zIbg1JkoqVV+nW8PX1xdfXV/O8Xbt2tGvX7rn1IiMjCQ0NxcXFhYSEBGxsbIDsAp6YmH1PytjYWCpXrqzZxtbWltjYWJRKJXZ2dprldnZ2xMbG5plNFmdJkooVkZX/4vyyYvy0tLQ0vLy8GDBgAKampi8/7ksufinoRTGyW0OSpOLlDXVrAGRlZeHl5UXz5s1p1KgRAFZWVsTFxQEQFxeHpaUlkN0ijomJ0WwbGxuLra3tc8tjYmKwtbXN89iyOEuSVKwIdf4fue5HCFatWoWTkxPdunXTLHd3d+fYsWMAHDt2jAYNGmiWBwQEkJmZSWRkJGFhYbi4uGBjY4OJiQlBQUEIIfD398fd3T3P9yG7NSRJKl7e0Djnmzdv4u/vT7ly5RgzZgwAffr0oWfPnixevBg/Pz9KlCiBp6cnAGXLlsXDwwNPT08UCgWDBg1Cochu/w4ePJgVK1aQkZGBm5sbdevWzfP4eqKQZwlpW6ZDYe6+UPzzll0h+INJPW1HeGVv4xWC0SmJ2o7wSoz1DbUd4ZUlpYS+9j6iO7fM97ol9h177eMVFtlyliSpWBFZ2k7wZsjiLElSsVJM7u8qi7MkScWLLM5FYMvJTaQkp6JWqVFlqfiq63AAeg58h54DeqDKUnHa7wyrZ62lqltVPOd9A4CeHmxc9BMn9p8o0rxLls+ifadWREfF0NKjBwCuNauyYPF0zMxMuX/vIV9+/i1Jj5N574NufDVikGZb15pVadfiXa5euVGoGesuHoJj+7qkRyfi12osANW/+wDHTvVBrSY9OpHzI1eRFhGPadkStPVfSNLtRwDEngvm0tgfUZoY0mDNSMzKl0So1YQfPM+1Wb8Wau4nSjs5smzVXOwdSiDUgs0bt7J21Wasra34Yf0iypZz4v69hwwZMIqEhETq1qvFgqXTAdDT02PhXB/2/embx1EKR5kypdnw41JKOtqjVqtZu3YLy5avY96cSXTt1p6MjAxCQu4yaLAnCQna699esWoenTu1ISoqhoYNOgHQq1cXJkwcSdVqLrRs0ZML568A0LpNM76f8R2GBgZkZGYyacIcjh07qbXsAIjc56x4W+j0CcEtJzfxZZfhJMb9+43q1qQOH3/dh4n9J5OZkYm1nTXxMfEYGRuRmZmJWqXG1sGW1QdX0bv+R6hVr/5rtKAnBBs3cSc5OYXlq+ZqivP+I9uYPmk+J08E0ufTdylXvgzzZnnn2K66axU2/uJDwzrtC3TcVzkhaNe4GlnJadRf9qWmOOubm5CVlAqA86COWFRx4tLYHzEtW4LGm8do1ntCaWKITT0Xok9cQ89ASdNtEwny3k2k36V85yjoCUGHkvaUdLTnyqVrmJmbcvDodgZ+MpwPP+5FXFw8y5esZfg3g7G2tmLmNC9MTIzJyMhEpVLhUNIev793UqdaS1SqV78J6OueEHR0dKCUowMXLv6DubkZZ07v5733P6OMUyn8jpxApVIxZ/YEAMZPmP1ax4KCnxBs2rQhScnJrFnjpSnOVatWQq0WeC+bxYQJszXFuXYdVyIjowkPi8TVtQq7/thIFRePAmd+EycEw1u0yve6jv5HX/t4heWtG+fcvW83fvX5jcyMTADiY+IBSE9L1xRiQyND0MKtak4FnCU+Luf81S4uFTl5Ivs6+mNHAuja4/lfVr3e78rO3/8qkowxp26QGZ+UY9mTwgygNDXKcx+q1AyiT1wDQGSqSLhyB5NSeQ+qfxMiI6K4cin72MlJKdwKuo1jqZJ07NKGrb/sBmDrL7vp1LUtAKmpaZpCbGxsqNVbGIWHR3Lh4j8AJCUlc+PGLZxKO3LI11+T8dTp8zg5ldJaRoATJ84QFxufY9nNm7e5dSvkuXUvX7pGeFgkANeuBWFkZIShoXZHiQi1Xr4fuizP4vzw4UOuXLlCWlpajuX5mVXpdQkB83+ew8q9PnT9pAsAZZzLUKtRTZbv8WbR7wupWqeKZv1qdaux7vBq1vr+wOLx3gVqNb9pN67folOXNgB079nphT9477zbuciK88tUH9ebDueWUfa9plyfv02z3LScPa0OzabZzsnYNar63HYGlqY4dqhH1PGrRRkXgLLlSlOzVnXOn7uEvYMdkRFRQHYBL2H/7y+LuvVrc+zkHo6c2M13ntML1Gp+08qXL4NbnZqcPpPzL4iBAz5i/4EjWkr1enr27MzlS1fJyMjQag61Si/fD12Wa5/z3r17OXDgAE5OTqxatYoBAwZorob55ZdfNHOVPuvZyUQKamSvb4iJiMXazpr5v8zhXvB9lEol5lYWDO8+gqpuVZm8chKfNukHwI0LNxjUdgjlXMoydskYzhw5Q2Z65mvneB3fDJvArPmT8Bw7jAN7/cjIzJmnXv3apKakceP6LS0lzHZ97lauz91K5a974PxZB24s2E5aRDwH6o8gMy4Jq9oVabTeE7+W32la2npKBe6rhhOydj8p9yKLNK+pmSlrN3kzZcJckh4n57ruhXOXaenRncpVnPFeOQe/Q/6kp2uvgJiZmbL1tzV4fjuVx4///Stm/LgRZGVl8fPPO7SWraCqV6/M9zPH8k73ftqO8t84IXj48GHmzZuHsbExkZGRLFq0iKioKLp06ZLrn4dPTybS9qeC9znHRGTP3BQfE8/f+wOo5laVqPAo/t73NwA3L95EqNVY2VqREPtvd8K94PukpaRRsWoFgi5rt+gF3wrlw17ZJ/6cK1WgfcecA+R7vteFndu122p+2oOdAXj8NIYbC7ajzshCnZFdPBIuh5JyNwLzSo7EX8ruF3RbOJikkHBur9lfpBn19fVZt2kpO7btYe+eQwBERcbgUNKeyIgoHEraEx31/Kxft4JCSElJpVr1yly6WPQtfcjOvu23Nfzyy0527dqnWd637wd07dKO9h17ayXX6yjt5MjPv/7AkMGjCQ3V/gVcut5dkV+5dmuo1WqMjY0BcHBwYNq0aVy4cIGNGzcWet+dsYkxJmYmmq/dW9Tjzs07nNgfQN2mbgCUqeiEvqEBCbEJOJZ1RKHMfjsOTg6UcS5L+P2IQs2YHyVKZP95raenx6gxX7Dxx39HNejp6dG9Zyd2abk4m1V01HxdqmM9Hgdnj84wtLMARfY3umk5B8wqOpJ8N7uFXH3sBxhYmHJl8uYiz7t4+UxuBYXwg89GzbKD+/zo3ecdAHr3eYcDe/0AKFfeCaVSCUCZsqWp5FKR+/ceFnnmJ9as9uL6jWCWLF2tWdaxQyvGfPsVPd8dQGpqWi5b6x4rKwu2b/+RaVPmc+rUOW3HAbK7Q/P70GW5tpytra25c+cOFSpUAMDY2Jhx48axcuVK7t0r3N+QNvbWTF87Fci+I8HhXUcIPHoWfQN9xniNZq3varIyM5n3zQIAajasQZ+vvicrS4VQq/GeuCzHKI+isGqdF02aNcDWzoYL146yYM4yzMxMGfj5JwDs3XOQX376909Wj6YNCHsUzt07D4oso/vK4ZRoUh1DWws6nl/GjQXbKdnWDXOXUgi1IPVBNBe/WwdAicbVqPbdB4gsFUKl5tJ3P5IZn4xxKVuqjurF46CHtD40C4CQHw9y9+ejhZ6/YeN6fPDRO1y7ehPf49mf5Zzvl7Bs8VpWb1jEx33f5+GDR3zef9T/16/P1998TmZWJmq1YNy33xP7zMmuotK0SQP6fvo+l69c42zgQQAmT57L4kXfY2RkxP592b+4T58+z7Dh47SSEWD9hqU0b9EYOzsbbt4KYNbMJcTFxbPQaxolStiyffuPXL58jZ7v9GfoF/1xrlSeseO/Zuz4rwF4p3s/oqJi8jhK4SkuLedch9LFxMSgVCqxtrZ+7rUbN25QrVq1PA8g59YofHJujaIh59YofG9iKF3oKwxJrXjp0Gsfr7Dk2nJ+evb+Z+WnMEuSJBW14tJy1ukrBCVJkl6VKCZXCMriLElSsfKfGEonSZL0tlHLlrMkSZLukd0akiRJOkjXL8vOL1mcJUkqVuRoDUmSJB0k+5wlSZJ0kOxzliRJ0kG6PmdGfsniLElSsSK7NSRJknSQWp4QlCRJ0j2y5ZxPloq870mna/QVSm1HeCX9k05rO8IrO2JTU9sRXlnbLO1M0F9Q+sq36/v4TZEnBCVJknTQm2w5r1ixgvPnz2NlZYWXlxcAW7du5fDhw1haWgLQp08f6tXLnrZ3586d+Pn5oVAoGDhwoOZWfiEhIfj4+JCRkUHdunUZOHAgenq555TFWZKkYuVNDtZo1aoVnTp1wsfHJ8fyrl270qNHjxzLHjx4QEBAAIsWLSIuLo4ZM2awdOlSFAoFa9asYejQoVSuXJk5c+Zw8eJF6tatm+ux87z7tiRJ0ttEpVbk+5EXV1dXzM3N83XcwMBAmjRpgoGBAQ4ODjg6OhIcHExcXBypqalUqVIFPT09WrRoQWBgYJ77ky1nSZKKlVeZMdTX1xdfX1/N86dvTp2bAwcO4O/vj7OzM/369cPc3JzY2FgqV66sWcfW1pbY2FiUSmWOG5fY2dkRG/v8DYifJYuzJEnFiiD/fc75LcZP69ChA++//z4Av/32G5s2beKrr7566U2vC3ozbNmtIUlSsaIW+X8UhLW1NQqFAoVCQdu2bbl9+zaQ3SKOifn3xraxsbHY2to+tzwmJgZbW9s8jyOLsyRJxYoavXw/CiIuLk7z9ZkzZyhbtiwA7u7uBAQEkJmZSWRkJGFhYbi4uGBjY4OJiQlBQUEIIfD398fd3T3P48huDUmSipVX6dbIy5IlS7h27RqPHz/miy++oHfv3ly9epU7d+6gp6eHvb09Q4YMAaBs2bJ4eHjg6emJQqFg0KBBKBTZ7d/BgwezYsUKMjIycHNzy3OkBoCeKGiHSD71Kte9MHdfKM48DtF2hFeSlJmm7Qiv7K28CCVOXoRS2KISbr72Pg6W/Cjf63aI+PW1j1dYZMtZkqRipZjc31UWZ0mSihdZnCVJknTQm+xz1iZZnCVJKlaKyYyhuj2UztTSjDGrxrHMbyXLDq+gar2qAHQZ0I3lR1ay1NeHfhMGAFCnuRsL/1rMkoPLWPjXYmo1qV2kWUs7ObLtj/UcPfUHfgG7GTT0UwDGTPiaQ3/v4KD/dn7evpqSjvY5tytTiqD7gQwdPqBI8wIsXzGX4NAznDyzT7Ns4uRRnDj1F8cD9rBz9wYcHR0AsLG1Zs/eLTwMv8wCr6lFmrPcwq+pdWEj1X29NctKjfqImoE/Um3/YqrtX4xl6/rZOXu21Cyrtn8xde/uxMS1YvZr3ZtR/eBSqvsuw2lC/yLLv2zFHIJCTxNwZu9zrw0fMYi4pGBs7Ww0y2rUqMqBw9sICNzHidN/YWRkWGRZAZYun8214AD8T+7RLFuzfjFHju/iyPFdnLt8mCPHdwFgYGCAt89sjgX8wZG/d9OkWcMizfoihT2Urqjo9GiNEYu+4dqZa/j+ehB9A30MTYxwruHM+1/3ZuaA6WRlZGFlZ0VCTAIVazgTHx1PXEQs5aqUY8pP3zO44YACHbcgozUcSpbAoaQ9/1y+jpm5KfuPbOOzT0cQ9iicpMfJAHw25BOqVKvEOM/vNdut3rgEoVZz/txlfli+oUB5Czpao0nTBiQnpbBqzUI8GnYGwMLCnMePkwAY+mV/qlVzYdTIyZiamlC7jiuurlWo7lqFMaOnF+iYT7zKaA3zRq6oktOosOQbrrcbAWQXZ1VKGpE/7HrpdsbVylNp7QSuNhuK0tqC6vsXc6OLJ1mxiZRfNJLY7Ud4fOJyvnMUdLRGk6YNSEpKYdWaBTRp2EWz3MmpFEt9ZlOlijOtmvckNiYOpVLJsRO7+WLwt/zzzw1sbK1JiE9ErX71ntSCjtbwaOJOcnIKy1fNo4XH8z+/02eOJTExCa/5Pnw2+GPc6tZkxLAJlChhy6/b19C+1fsFviruTYzW2OH4cb7XfTf859c+XmHR2ZazibkJrg1r4vvrQQCyMrNISUymU98u7FjxO1kZWQAkxCQAEHo1hLiI7OvV7wXdw9DIAH3Douu1iYyI5p/L1wFITkrhVlAIjqUcNIUZwNTMJMc3bccubbh39z43bwQXWc6nBZwIJC4uPseyJ4UZwMz037wpKamcOnmOtLSMoowIQNLpa6jik/Je8Rm27zQn7o/jABiVL0layCOyYhMBePz3Jay7eLzRnC/zos8ZYNa8iUybNC/H90Sbts24+s9N/vnnBgBxsfEFKsyv42TAWeLiEl76+ju9OrPz9z8BqFrNBf9jpwCIjo4lIeExbnW1O0xSraeX74cuy7M4BwcHExycXTwePHjAn3/+yfnz5ws9WMlyjiTGJvC11zd47V3CV/O+xsjEiNIVS+PasAbzdi9k5tY5uNSu/Ny2Hl2aEHI1RFPAi1qZsqWpWbs6F85lt8rGThpB4D++9PqgGwtmLwfAxNSEYSMHsWjeSq1kzM3kqaO5euNvPvjwHWbNXKLtOC9l378L1Q8updzCr1FamT33uk33ZsTu9gcg/U4Yxi5OGJZxAKUCq46NMCxVoqgja3Tu0pawR+GaIvxEJZeKCCH4fdd6jv69mxHffK6lhC/m0cSdqKgYQkLuAvDPPzfo3LUtSqWScuXLUKdODZzKlNJqRvEKD12Wa3Hetm0b69evZ+3atfz888+sW7eOtLQ0du/ezY4dO166na+vL+PGjWPcuHEFDqbUV+JcsxL7N+9ldJdvSE9N492v3kepr8TMypyx73zLxlk/8u2KsTm2K1ulHP3GD2DVeJ+X7LlwmZqZsmbTEqaOn6tpNc+b6U2Dmu3Yue1PBn6e/SfXt+OGsWblJlKSU7SSMzczpntRo1oztv22myFD+2o7zgtFbd7H1WZfcL3jN2RFxuE0+bMcr5u6VUGdmk7azXsAqBKSuT9hFRVXjKHK9jlkPIhEqLQz6MrExBjPMV8y5wW/+PT1lTT2qM+QQZ50bv8hXbt3oEWromnh50ev97ux4/+tZoCfN2/n0cNwfI9uZ+acCQSeuUBWlkqLCbOH0uX3octy/bv/1KlTLFiwgMzMTIYMGcLKlSsxNTWlR48eTJgwgXffffeF2z0901OvnwvW5xwTFk1MWDS3LgYBELD3BO9++T7RYdGc2hcAwK1LtxBCjaWtJYmxidg52jFu9QSWjlpM+N3wAh33dejr67Nm4xJ2bvuLfX/6Pvf6zt//YtNvK/Ga60Nd99p0facDE6ePxtLKArVakJ6ewYY1utMHtm3rH2zdvo45s5ZqO8pzsqL//bM7+ueDVNowKcfrNu80J3b38RzLEnwDSfDNnkfX7uMOoKXiXNG5HOUrlOX4yewiV9rJkWN/76Zty3d59CicE3+fITYme/6GQwePUqdODfyPntRK1qcplUq6dm9Pu5b//tyrVComT5ijef7XwV8IuX1HC+n+9Z8YraFUKlEoFBgZGVGyZElMTU0BMDQ0zPMWK68rPiqe6LBoSjs7AVC7aR0e3LrPmYOnqN2kDgClK5ZG30CfxNhETC3NmLhhKpvnbeLG2euFmu1lvJZ9T3BQCKtXbNQsq+hcTvN1h06tuR0UCsC7XfrRuE4HGtfpwNqVm1m2aLVOFGbnShU0X3fu2o5bQbe1FyYX+g7/jm6w7tSY1P+3kAHQ08OmaxNNf7NmGzsrAJRWZtj360z0L4eKJOuzrl0NokrFRtSp0Yo6NVrx6GE4LZu9Q2RkNId9j1OjZjVMTIxRKpU0bdZQa+ckntWyVROCg0IIexShWWZiYoypqUn2662boMpSEXRTu98zKvTy/dBlubac9fX1SU9Px8jIiLlz52qWp6SkaCb0KExrpvzAKO/R6BvoE3EvgmXfLiE9JZ3hC0aw9NByMjOy8PZcAkCX/l0pVaEUvUd8SO8RHwIw/dMpmhOGha1B43q8/9E7XLt6k4P+2wGYO2MJH336HpUqV0CtVvPwfhjjPF9vlMObtG79Epo1b4SdnQ3Xbv7NnFlL6dCxFS6VnVGr1dy/95BRIydr1r989RiWFuYYGBrQtVt7er0zoEgKR4Xlo7FoXBN9W0tqnllHmNcvmHvUxLRGRRCQ/iCSe+NWaNY3b1SDzLAYMu5F5NhPmemDMamePawufOlvpIc+KvTsAGvXL6bp/z/nf27+zdxZS/lp07YXrpsQn8iKZT9y2H8nCMGhA0c5eOBokeR84od1XjRt1hBbOxsuXTvG/DnL2LL5d3q914Ud2//KsW4Jezu27liHWq0mLCyCr4Z+V6RZX6S4tJxzHUqXmZmJgYHBc8sTExOJj4+nXLlyL9gqJznxUeGTEx8VDTnxUeF7E0PpNjh9mu91Bzz86bWPV1hybTm/qDADWFpaau48K0mSpEt0fRRGfsnLtyVJKlaKS7eGLM6SJBUruj5ELr9kcZYkqVhRyZazJEmS7pEtZ0mSJB0ki7MkSZIOkqM1JEmSdJAcrSFJkqSDZLeGJEmSDtLunHhvjizOkiQVK7JbQ5IkSQfJbg1JkiQdJEdr5NOt9KjCPsQbl5Cue3cnyU1aVtHf1+91NY+5pO0Ir2yUQxNtR3gl88P8tR1BK9TFpDzLlrMkScXKmzwhuGLFCs6fP4+VlRVeXl4AJCUlsXjxYqKiorC3t2fUqFGYm5sDsHPnTvz8/FAoFAwcOBA3NzcAQkJC8PHxISMjg7p16zJw4MA8b1iis3ffliRJKog3eQ/BVq1aMWHChBzLdu3aRa1atfD29qZWrVrs2rULyL4BdkBAAIsWLWLixImsW7dOc+f0NWvWMHToULy9vQkPD+fixYt5HlsWZ0mSihW1Xv4feXF1ddW0ip8IDAykZcuWALRs2ZLAwEDN8iZNmmBgYICDgwOOjo4EBwcTFxdHamoqVapUQU9PjxYtWmi2yY3s1pAkqVh5lT5nX19ffH3/vRnz0zenfpmEhARsbLLvYWljY0NiYiIAsbGxVK5cWbOera0tsbGxKJVK7OzsNMvt7OyIjY3NM5sszpIkFSuvcjowP8U438d9yR3/crkTYK5kt4YkScXKm+xzfhErKyvi4uIAiIuL09yyz87OjpiYGM16sbGx2NraPrc8JiYGW1vbPI8ji7MkScWKCpHvR0G4u7tz7NgxAI4dO0aDBg00ywMCAsjMzCQyMpKwsDBcXFywsbHBxMSEoKAghBD4+/vj7u6e53Fkt4YkScXKm7xCcMmSJVy7do3Hjx/zxRdf0Lt3b3r27MnixYvx8/OjRIkSeHp6AlC2bFk8PDzw9PREoVAwaNAgFIrs9u/gwYNZsWIFGRkZuLm5Ubdu3TyPrScK2iGSTzVLNi7M3ReKO48jtB3hlbyNF6EY6b/4zu66TF6EUvgy0h+89j48K3yU73UX3fn1tY9XWGTLWZKkYqV4XB8oi7MkScWMnPhIkiRJBxX0RJ+ukcVZkqRipbhMfKSzQ+kMjQz5Zf86tvttZtexnxk2ZjAAVWtUZsvetfx+eBO/HVhPzbquAFjZWPLjDh/OhPgxYfZorWResWoeoXcCORO4X7PMxsaKP/Zs5uJlP/7Ysxlr6+wxkQYGBqz8YT6nz+zj5Km9NG/eSCuZnzAyMuLkiT85d/YQly76MXVKzs/Qc9RQsjIeYmdno6WE2Vaums+dO2cJDDygWWZjY8WePZu5dPkIe576jOu71+Hkqb2cPLWXU6f20b1HR61k9hjYia8PzOPrg/Px+KwTAI7VyzFkx3SG75/Lp2u/xcjcBACnOpUYtnd29mPfHKp3zHvIVVFQKBScOb2fnTs3AFCntivH/f8g8MwBTgb8hbu7m1bzPU28wkOX6WxxzkjP4LN3h/Nem76837YvTdt4ULt+DUZPGc7Khet4v20/ls9fzejJwzXrL5u7moXTlmkt85bN2+nZc0COZZ6jv+To0RO41W7D0aMn8Bz9JQADP8s+o9yoYWd6dO/L7LkT85ylqjClp6fTrkNv6ru3p757Bzp2aEWjhvUAKFOmNO3atuDu3dc/k/66ftr8Oz179s+xbPToLzl6NIA6tVtz9GgAo0d/BcC1qzdp1rQ7Ho270LNnP5Z5z0KpVBZpXocqZXD/qDWr3pmMT+dxVGtTD7sKjvSc+zkH5/3C8k7juHYgkGZDugEQefM+K7tPwqfLBDb2m8c7swahUGr/x/Trrwdx40aw5vnsOROZOWsxDRp2ZPr3XsyZPVGL6XJSI/L90GXa/1/PRWpKKgD6Bvro6+sjRPalkOYWZgCYW5oTGRH1/3XTuHDmEunp2htWduLEGeJi43Ms69qtPVu2bAdgy5btdOveAYBq1Spz9EgAAFFRMSTEJ1Kvfu0izfus5OTseawNDPTRNzDQXHbqtXAa4ybMKvBlqG/SiRNniI1NyLEs+zP+HYAtW36nW/f2AKSmpqFSZU8gaWRkpJX89i5O3L8QTGZaBmqVmtDT16ne0Z0SzqW4c/oGALf/vkKNztkXMjxZD8DAyEAnmndOTqXo3LktP67/WbNMCIGlRfaEQFaWFoSF6c7w08K+QrCovHJxXr58eWHkeCGFQsHvhzfhf3UfJ4+d4cr5q8ybvITRU4bje3433079miWzVhZZnoJwcChBRHj2L5CI8Cjs7bMnQLly5TrdurVHqVRSvnwZ3OrWooxTKW1GRaFQcDbwIGEPL3P4sD9nAi/QrVt7Hj4M4/Lla1rNlhsHB3vC//8Zh4dHYW9fQvOaewM3As8e5EzgAUaMnKQp1kUl8uZ9KjSshom1OQbGhlRp7YZVKTsigx5QrX19AGp0aYxVqX8nxinjVomvD85n+IF57J60TlOstcVr4TTGj5+FWv3vb4pvv53GnDmTuB18hrlzJzNp8hwtJsxJvMI/XZbrCcF58+bleC6E4OrVqyQnJwMwduzYF2737ExPBaVWq3m/bT8sLM1ZumEeLtWc+aBvT+ZNWYrvX0fo2KMt3y+eyOcffP3axypqmzZupWrVShw/8Qf37j3k9OlzZBVx4XiWWq3GvUEHrKws2b5tHbVqVWfCuBF06vKxVnO9jrOBF2ng3oGqVSuxeo0XBw8cJT09vciOH3X7EcdX7WHgT+PJSE4j/Ppd1CoVO75bTbep/Wg94l1u+J5DlZml2ebBxdss6/Ad9pVK857Xl9w6eoms9Mwiy/y0Ll3aEhkVzYULV2jRwkOzfMiQfowZM52du/by/nvd+OGHhXTu3EcrGZ/1nxitERsbi5OTE23btkVPTw8hBCEhIXTv3j3XnT4909Of61//CsHHiUkEnjhPs9aN6dG7C3MmLgLgwB+Hmb5oQh5ba1dkZDQlHe2JCI+ipKM9UVHZE6CoVCrGjZ2pWc/X73duB4dqK2YOCQmJHPMPoEf3jlSoUI7zZw8BUKZMKQJPH8CjaVciInTn9mORkVE4Oma3nh0d7YmKin5unZs3b5OcnIprjSpcOH+lSPOd23qUc1uPAtB+zIckhMUQffsRG/rNBcCuoiNVWz9/OW/U7UdkpKbhUKUMj65o53ujiUcDunXtQKeObTA2NsLS0oIN673p2rUdnp5TAPh9+5+sWrVAK/leRNe7K/Ir126NOXPm4OzszI4dOzA1NaVGjRoYGhri6uqKq6troQazsbPGwjK7T8vI2IjGLRoQGnyXqPBoGjTJPlHVqLk7d0PuF2qO17X3L18++eQ9AD755D3++jO70JmYGGNqmn2GvnWbZqiyVDlOuBS1EiVssbLKHuVgbGxM2zbNuXjxH0qXqYNLlca4VGnMgwdhNGjUUacKMzz5jN8H4JNP3td8xuXLl9GcACxb1okqVZy5p4WTmmZ22Z+rVWk7XDs14PIfJzXL9PT0aDW8F2e2ZP+laVPGXnMC0NqpBCWcSxP/4PlfNkVl0uS5OFdqQJWqHnzadxhHjp5gwMARhIVFaFrSrVs3JVhHGhYAaiHy/dBlubacFQoF3bp1w8PDg40bN2JlZVVkfXb2JUswy3sySqUSPYUeB3Yf5tihEyQmJDFu5ij09ZWkp2cw/dt/+7oOBO7E3MIUA0MD2nRuyZAPRxASdKdI8gKs37CU5i0aY2dnw81bAcyauYRFXivZtHk5/fr35sH9R/T9dFj2+7O3Y9cfmxBqNY8ehTN4kGeR5XyRUqVK8uO6JSiViuy+/t/38Nfe1++aetM2bPDWfMZBt04yc+ZivLxWsnmzj+Yz/vTT7NEaTZo0wHP0l2RlZaFWq/nmm8nExMQVeeY+K7/B1MYcVZaKPZPXk5aYjMfATjTqm33i8tqBQM5vy57lrHyDqjT/sgfqrCyEWrBn8npS4h4Xeea8fPHldyzymo6+vj5pael8+dWLuzi1QbdLbv690sRH58+f58aNG3z8cf77IOXER4VPTnxUNOTER4XvTUx89HH5Xvle9+e7O1/7eIXlla4QrFevHvXq1SusLJIkSa9N10dh5Je8fFuSpGIlSxZnSZIk3SNbzpIkSTqouAylk8VZkqRiRRemGXgTZHGWJKlY0fUJjfJLFmdJkoqV/8Tl25IkSW8b2XKWJEnSQbLPWZIkSQfJ0RqSJEk6SI5zliRJ0kGyz1mSJEkHqUTx6NiQxVmSpGLlTXZrDBs2DGNjYxQKBUqlkrlz55KUlMTixYuJiorC3t6eUaNGYW6ePff8zp078fPzQ6FQMHDgQNzc3Ap87EIvzo6GVoV9iDcuwqDo5/x9HQot3rW7oKyNzLQd4ZXNfXRM2xFeyUn7htqOoBVvehL9qVOnYmlpqXm+a9cuatWqRc+ePdm1axe7du3i008/5cGDBwQEBLBo0SLi4uKYMWMGS5cuRaEo2H20dfru25IkSa9KvMKjIAIDA2nZsiUALVu2JDAwULO8SZMmGBgY4ODggKOjI8HBBb+7kezWkCSpWHmVE4LP3oz66fufPjFr1iwA2rdvT7t27UhISMDGxgYAGxsbEhMTgex7rlauXFmzna2tLbGxsQV+H7I4S5JUrLxKcX5RMX7ajBkzsLW1JSEhgZkzZ1K6dOmXrvumL36RxVmSpGLlTY7WsLW1BcDKyooGDRoQHByMlZUVcXFx2NjYEBcXp+mPtrOzIyYmRrNtbGysZvuCkH3OkiQVK+IV/uUmLS2N1NRUzdeXL1+mXLlyuLu7c+xY9snhY8eO0aBBAwDc3d0JCAggMzOTyMhIwsLCcHFxKfD7kC1nSZKKlTfVvZCQkMDChQsBUKlUNGvWDDc3NypVqsTixYvx8/OjRIkSeHp6AlC2bFk8PDzw9PREoVAwaNCgAo/UgFe8+3ZBtCvbsTB3XyguJd7RdoRXkp6Vqe0Ir+xtHEr3KKngJ3e04W0cStfg4evfDbteqWb5Xvd82N+vfbzCIlvOkiQVK3JWOkmSJB2kKibz0sniLElSsfKmrxDUFlmcJUkqVuSUoZIkSTpItpwlSZJ0kGw5F4GfAjaSmpyKSqVGpVIxrOvXTFoxgTLOZQAwtzQjKTGZLzp9haW1BVN+mEzVOlU4sO0Qyyf7FHnepctn075TK6KjYmjh0R2AmrWqsWDxdIyNjMhSqfjOcxoXzl+hZesmTJ42GgMDAzIzM5k2eQF/+58q0rw+K+fRqXNroqJiaNygMwAzZo2jc+e2ZGRmEhpyl6+++I6EhMeUK+dE4PlD3LoVAkDgmYuMGjmpSPMClHIqyeIVs7EvWQKhVvPzxt/58YctAAz4/GP6D/4IlUqF30F/Zk9bjLWNFas2LKJO3Zps+2U3U8bOLvLMTxgZGXHUbzuGRkbo6yvZseMvpn/vhY2NNb9sWUn58mW5e/c+H338BfHxCUWarYLXcKzbuZMZncDVtiMBKO35IfYftycrNnvuiAdzfyLB7zyGZeypdXQZaSGPAEg6H8TdcasA0DPQp9zMz7FsUhOhVvNw3hbi9hbt97VsOReR0b2/IzEuUfN85lf//nANnTyE5MRkADLSM9iwcCMVqlagQtUKRR0TgF9/3sG6NT+xfNU8zbIp349h4VwfDvv60659C6Z+P4ae3foRGxPHJx9+SUR4JNWqV2brjnXUrt6iSPNu+el3Vv+wiR/WLNQsO+L3N9OmLEClUjF9xlg8v/2KqZOz309o6F2aeXQr0ozPUmWpmDl5If9cvo6ZuSl/+f3G8aMnKWFvR4fOrenY/D0yMjKxK5F92Wx6egZes5dTtboLVapXzmPvhSs9PZ12HXqTnJyCvr4+/kd3sn//EXr16ozfkb+Zv8CH78YMY+x3wxg/oWh/iURv9SNy/V4qLh2ZY3nEmj2E/7D7ufXT7kZwtYPnc8tLjXifrJgErjQfBnp66FubF1rmlykuk+2/1Zdvt+zWgiO7jwCQlprOP4FXyUjP0FqekwFniYt7psUjBBaW2RdcWFhaEB4eCcCVy9eJ+P/XN67fwsjYEENDgyLNG3AikLjY+BzL/A7/jUqlAiDwzAWcnByLNFNeIiOi+efydQCSk1IIDgrFsVRJ+n72ISuWriMjI/uCnJjo7AtGUlNSCTx9gTQtfl88LTk5BQADA330DQwQQtC9e0c2bd4GwKbN2+jRo1OR50o6fY2s+MevvR/7j9oStmx79hMhyIp7/X2+qjd1+ba2vVJxvnHjBn/++SeXLl0qrDw5CAHztsxmxV/L6fpx5xyv1WpUk7joOB7eeVQkWQpq4rjZTP3+Oy5ePcr0mWOZOX3Rc+t0f6cjVy5f1xQWXdG33wccOnhU87x8+bIcD9jD3v2/4NGkgfaC/V+ZsqWpUbsaF85dpmKl8jT0qMfuQ1vYumc9tevW0Ha8F1IoFJwNPEjYw8scPuzPmcALlHQoofmlHR4eiYO9nZZT/sthYBdqHFpMBa/hKK3+varTqJwDrge8qPr7TMwbVgdAaWkKgNN3H+O6fyGVfhiDfomiv9mGEOp8P3RZrt0a48ePZ86cOUD2vKcHDhygYcOG/P7774SGhtKzZ88XbvfsHKkF9c27o4iJiMXazop5P8/l3u37XDn9DwBt3mnNkd1HX/sYhW3goD5MnjCHP/84yDu9OrNk+Szef2eg5vWq1VyYPP1bevf6TIspn/ftmK/Iysrit1+z/6QND4+iRrVmxMbG4+ZWk59/W0Uj9048fpyklXymZib8sHEx0yfMI+lxMvr6SqysLHmn/SfUqVeTFT8upFndznnvqIip1WrcG3TAysqS7dvWUaNGVW1HeqnITft5tGQbCIHTdx9TdspA7oxeTmZkHJcaDkEV9xjTWs5U/nE8V1qPQE+pxLB0CZICr3N/+npKDulB2SkDCB2xtEhzF5cbvObacn7y5y3A4cOHmTx5Mh988AETJ07k+PHjL92uXbt2zJ07l7lz575WuJiI7D9N42MSOLH/BNXcqmWHVipo1qkpR//Q/dsGfdinF3/+cRCA3Tv3Ua9ebc1rpUqXZOOW5QwfOpY7ofe1FfE5H3/yLp06t2HwZ6M0yzIyMoj9fxfIxYv/EBpyDxeXilrJp6+vzw8bF7Pz97/Y/+dhAMIeRbDvz+wGwaXz/yDUAls7G63ky4+EhESO+QfQsUMrIiKjcXR0AMDR0YHIqJg8ti4aWdEJoFaDEERtOYiZW3afvcjIQvX/7oqUKyGk3QnH2Lk0WXGPUaWkEbfvNABxf57ArKZzkecWQuT7octyLc5CCJKSknj8+DFCCM28pcbGxiiVykINZmxihImZiebr+i3qc+fmHQDqN6/Hvdv3iQ6PLtQMb0J4eCRNmmVPQNO8ZWNCQu4AYGllwc9bVzNz+iLOnD6vxYQ5tWvfgm9GDeXD3kNITU3TLLcrYauZYatChbJUcqnAnTv3tJJxgfd0goNCWLtik2bZwb/8aNKiEQAVK5XHwNCA2BjduhdkiRK2WFn9+zPUtk1zbt68zZ97DtKv7wcA9Ov7AXv2HNBmTA0Dh39/udl0bkzqzbsA6Ntawv+/F4zKlcS4YinS70UAEH8oEIsmNQGwaFab1FsPijh1dss5vw9dlmu3RkpKCuPGjUMIgZ6eHvHx8VhbW5OWllbov3Vs7G2YtmYqAEqlEr/dRwg8ehaAVj1avrBL46eAjZhamGFgoE/Tjh6M/WQC924VXQH5YZ0XTZs1xNbOhkvXjjF/zjI8R0xm1rwJKJX6pKen4zlyCgCDP/+Uis7lGD3mK0aP+QqAD3p9RnR00c189uOGpTRr3gg7OxuuB51g9syljP72CwyNDNm9J7vwPRky17RpQyZO+oYslQqVSsU3IyY9f/KzCDRoVJf3PurB9atB7DuWfRJt/gxvftuykwXLZnDoxA4yMjLx/GqiZpsTF/djYWGOgYEBHbu24dP3hnDrZkiRZy9VqiQ/rluCUqlAoVDw++97+GuvLydPnePXn1cxcEAf7t9/yId9hhZ5NmcfTyw8aqBva0mds2t4uPBXLJrUxNS1IghB+oNI7o7NHi5n0dgVp2/7IFQqhErNnfGrUMVnd289mLUZZ++RKKd9RlZsIqGjlhX5e1GpdbsvOb8KNGVoeno6CQkJODg45LmunDK08MkpQ4uGnDK08L2JKUMdravne93w+OuvfbzCUqBxzkZGRvkqzJIkSUVN1/uS80vnL0KRJEl6Fbrel5xfsjhLklSsyJazJEmSDiouJwRlcZYkqViR3RqSJEk6SHZrSJIk6SA5ZagkSZIO0vXZ5vJLFmdJkooV2XKWJEnSQWodnwo0v2RxliSpWJEnBCVJknSQLM6SJEk6qHiU5gLOSqcrfH19adeunbZj5NvblhfevsxvW16QmaUXe6tv8PomboVVlN62vPD2ZX7b8oLMLL3YW12cJUmSiitZnCVJknTQW12c37Y+r7ctL7x9md+2vCAzSy/2Vp8QlCRJKq7e6pazJElScSWLsyRJkg56Ky9CuXjxIuvXr0etVtO2bVt69uyp7Ui5WrFiBefPn8fKygovLy9tx8lTdHQ0Pj4+xMfHo6enR7t27ejSpYu2Y+UqIyODqVOnkpWVhUqlonHjxvTu3VvbsfKkVqsZN24ctra2jBs3Tttx8jRs2DCMjY1RKBQolUrmzp2r7UjF1ltXnNVqNevWrWPSpEnY2dkxfvx43N3dKVOmjLajvVSrVq3o1KkTPj4+2o6SL0qlkr59++Ls7Exqairjxo2jdu3aOv0ZGxgYMHXqVIyNjcnKymLKlCm4ublRpUoVbUfL1d69e3FyciI1NVXbUfJt6tSpWFpaajtGsffWdWsEBwfj6OhIyZIl0dfXp0mTJgQGBmo7Vq5cXV0xNzfXdox8s7GxwdnZGQATExOcnJyIjY3Vcqrc6enpYWxsDIBKpUKlUqGnp6flVLmLiYnh/PnztG3bVttRJB301rWcY2NjsbOz0zy3s7Pj1q1bWkxUvEVGRhIaGoqLi4u2o+RJrVYzduxYwsPD6dixI5UrV9Z2pFxt2LCBTz/99K1qNQPMmjULgPbt28shdYXorSvOLxr5p+stpLdVWloaXl5eDBgwAFNTU23HyZNCoWDBggUkJyezcOFC7t27R7ly5bQd64XOnTuHlZUVzs7OXL16Vdtx8m3GjBnY2tqSkJDAzJkzKV26NK6urtqOVSy9dcXZzs6OmJgYzfOYmBhsbGy0mKh4ysrKwsvLi+bNm9OoUSNtx3klZmZmuLq6cvHiRZ0tzjdv3uTs2bNcuHCBjIwMUlNT8fb2ZsSIEdqOlitbW1sArKysaNCgAcHBwbI4F5K3rs+5UqVKhIWFERkZSVZWFgEBAbi7u2s7VrEihGDVqlU4OTnRrVs3bcfJl8TERJKTk4HskRtXrlzByclJy6le7uOPP2bVqlX4+PjwzTffULNmTZ0vzGlpaZoumLS0NC5fvqyzv/yKg7eu5axUKvnss8+YNWsWarWa1q1bU7ZsWW3HytWSJUu4du0ajx8/5osvvqB37960adNG27Fe6ubNm/j7+1OuXDnGjBkDQJ8+fahXr56Wk71cXFwcPj4+qNVqhBB4eHhQv359bccqVhISEli4cCGQfdK1WbNmuLm5aTdUMSYv35YkSdJBb123hiRJ0n+BLM6SJEk6SBZnSZIkHSSLsyRJkg6SxVmSJEkHyeIsSZKkg2RxliRJ0kH/Axa2quej47DkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = eclf_genre.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')\n",
    "plt.title('Review Genre Classification');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f4cde3",
   "metadata": {},
   "source": [
    "### Saving the Genre Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f22d5382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "file_name = 'genre_classifier_best.pkl'\n",
    "\n",
    "with open(file_name, 'wb') as file:\n",
    "    pickle.dump(eclf_genre,file)\n",
    "with open('tfidf_genre_best.pkl', 'wb') as output:\n",
    "    pickle.dump(tf, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5d2eac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = [\"\"\"Australian servers are dead, don't buy unless you want your soul to noclip through your body in yankland servers\"\"\",\n",
    "      \"\"\"Texture changing simulator. No real sense of accomplishment and a very weird visual/graphic style.\"\"\",\n",
    "      \"\"\"♥♥♥♥♥♥♥ horrific i cant even get past the first ♥♥♥♥♥♥♥ car because i cannot pick up or use the phone in the game period ♥♥♥♥♥♥♥ WOULD LIKE A REFUND OR A FIXTURE ASAP>>> FIX THE PHONE ISSUE OR SOMETHING OR THERE IS A REFUND HAPPENING!!!!<<<<\"\"\",\n",
    "      \"\"\"This game is fantastic, just needs to work on communication, graphics, map, characters, economy, how the units aim, realistic unit movement, anti aliasing, the ai, diplomacy, rational decision making by ai, the HUD, the tutorial, time to load, synchronization, the launcher, understandability and getting a new developer team.\"\"\"]\n",
    "\n",
    "rev_return = eclf_genre.predict(tf.transform(rev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "13c7fe8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'action'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_return[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055cb559",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d206683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "id2word = corpora.Dictionary(fd)\n",
    "\n",
    "corpus = []\n",
    "for text in fd:\n",
    "    new = id2word.doc2bow(text)\n",
    "    corpus.append(new)\n",
    "\n",
    "print (corpus[0][0:20])\n",
    "\n",
    "word = id2word[[0][:1][0]]\n",
    "print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f7632f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10,\n",
    "                                           random_state=100,\n",
    "                                           update_every=5,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bec72e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('account', 0.0434924),\n",
       " ('solo', 0.03515456),\n",
       " ('path', 0.034367513),\n",
       " ('polish', 0.024815334),\n",
       " ('biggest', 0.023588581),\n",
       " ('immedi', 0.018600047),\n",
       " ('setup', 0.017668996),\n",
       " ('imo', 0.015337193),\n",
       " ('pre', 0.015182037),\n",
       " ('de', 0.012638664)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topic(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d6d40d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el10122535118620928176626185\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el10122535118620928176626185_data = {\"mdsDat\": {\"x\": [-0.19757845314123662, 0.09071624871962264, -0.35206812354162587, -0.2691727696817822, 0.302823464497098, 0.3071483967215029, 0.10029775088857798, -0.03906655646958921, -0.010271794069174419, 0.0671718360766068], \"y\": [-0.43762717307537724, -0.4510050587231109, -0.09042082991713793, 0.3369891420890032, -0.07843592192655775, 0.1681773879053926, 0.26271805373890156, 0.035289813728424026, 0.1616776707733436, 0.09263691540711909], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [47.41533780924716, 46.08213317913024, 2.0581980620652374, 1.723978132108556, 0.93697449544863, 0.7856172476151774, 0.4283956501357205, 0.3208628970220327, 0.22220818904962872, 0.02629433817759798]}, \"tinfo\": {\"Term\": [\"game\", \"play\", \"good\", \"fun\", \"get\", \"like\", \"time\", \"recommend\", \"love\", \"bad\", \"10\", \"money\", \"2\", \"worth\", \"look\", \"run\", \"graphic\", \"3\", \"bug\", \"long\", \"great\", \"never\", \"could\", \"terribl\", \"hard\", \"level\", \"free\", \"gameplay\", \"enough\", \"also\", \"game\", \"play\", \"fun\", \"great\", \"best\", \"buy\", \"friend\", \"updat\", \"year\", \"ever\", \"fix\", \"mod\", \"amaz\", \"crash\", \"dont\", \"commun\", \"shit\", \"dlc\", \"cool\", \"ye\", \"fuck\", \"server\", \"cant\", \"releas\", \"suck\", \"addict\", \"bought\", \"onlin\", \"multiplay\", \"version\", \"good\", \"like\", \"hour\", \"dev\", \"better\", \"get\", \"got\", \"nice\", \"recommend\", \"love\", \"time\", \"10\", \"realli\", \"still\", \"would\", \"bad\", \"bore\", \"money\", \"even\", \"one\", \"tri\", \"peopl\", \"much\", \"want\", \"work\", \"make\", \"enjoy\", \"go\", \"wast\", \"enemi\", \"design\", \"item\", \"move\", \"space\", \"gener\", \"skill\", \"place\", \"entir\", \"balanc\", \"high\", \"random\", \"weapon\", \"allow\", \"limit\", \"ship\", \"reward\", \"main\", \"resourc\", \"quest\", \"often\", \"attack\", \"power\", \"upgrad\", \"side\", \"abil\", \"simpli\", \"area\", \"craft\", \"point\", \"fight\", \"either\", \"hit\", \"option\", \"system\", \"combat\", \"rather\", \"less\", \"build\", \"differ\", \"becom\", \"interest\", \"map\", \"basic\", \"level\", \"charact\", \"end\", \"mechan\", \"two\", \"around\", \"find\", \"base\", \"turn\", \"way\", \"part\", \"enough\", \"take\", \"use\", \"someth\", \"thing\", \"feel\", \"seem\", \"also\", \"complet\", \"make\", \"well\", \"need\", \"go\", \"kid\", \"master\", \"forev\", \"realiti\", \"parti\", \"refus\", \"w\", \"keyboard\", \"paint\", \"evolv\", \"cup\", \"adult\", \"rich\", \"dri\", \"potato\", \"spare\", \"usag\", \"ark\", \"elder\", \"audienc\", \"skyrim\", \"coffe\", \"nasa\", \"teen\", \"\\u2018\", \"boi\", \"alpha\", \"ring\", \"infin\", \"circl\", \"dark\", \"soul\", \"audio\", \"averag\", \"brain\", \"check\", \"burn\", \"press\", \"replac\", \"comput\", \"beauti\", \"easi\", \"requir\", \"terribl\", \"launcher\", \"week\", \"pass\", \"delet\", \"restart\", \"randomli\", \"mobil\", \"practic\", \"cash\", \"manual\", \"idk\", \"unbalanc\", \"tabl\", \"pirat\", \"anti\", \"bullshit\", \"weak\", \"rocket\", \"nerf\", \"economi\", \"coop\", \"scari\", \"rogu\", \"avatar\", \"wtf\", \"r\", \"oppon\", \"buff\", \"lame\", \"account\", \"solo\", \"path\", \"polish\", \"biggest\", \"immedi\", \"setup\", \"imo\", \"pre\", \"de\", \"vanilla\", \"door\", \"c\", \"steep\", \"truck\", \"wipe\", \"un\", \"killer\", \"own\", \"occur\", \"clip\", \"influenc\", \"se\", \"dragon\", \"websit\", \"bom\", \"irl\", \"bonus\", \"laughabl\", \"jogo\", \"e\", \"planet\", \"plane\", \"com\", \"alien\", \">\", \"v\", \"aliv\", \"civil\", \"crew\", \"promot\", \"www\", \"nation\", \"wiki\", \"galaxi\", \"refin\", \"men\", \"\\u201d\", \"\\u201c\", \"paywal\", \"i\", \"vampir\", \"bang\", \"hardest\", \"pilot\", \"worthwhil\", \"german\", \"soft\", \"translat\", \"dare\", \"agent\", \"http\", \"\\u2019\", \"it\", \"t\", \"don\", \"race\", \"empir\", \"fish\", \"boat\", \"mob\", \"n\", \"eso\", \"arm\", \"linux\", \"subscript\", \"victori\", \"trial\", \"h\", \"lvl\", \"patient\", \"void\", \"web\", \"la\", \"pit\", \"el\", \"su\", \"bowl\", \"pod\", \"solar\", \"_\", \"ha\", \"juego\", \"smell\", \"700\", \"brake\", \"l\", \"faster\", \"ball\", \"session\", \"sold\", \"peac\", \"bed\", \"relationship\", \"blame\", \"95\", \"announc\", \"squar\", \"women\", \"destruct\", \"trait\", \"fellow\", \"tedium\", \"unreal\", \"suicid\", \"unus\", \"capac\", \"cock\", \"wound\", \"wasteland\", \"offend\", \"flame\", \"5000\", \"subscrib\", \"grave\", \"soni\", \"oddli\", \"2k\", \"kick\", \"factorio\", \"children\", \"fi\", \"dad\", \"sci\", \"27\", \"newli\", \"infrastructur\", \"fist\", \"tt\", \"lego\", \"healthi\", \"i5\", \"1060\", \"sentenc\", \"q\", \"34\", \"nuanc\", \"yellow\", \"summar\", \"ts\", \"leap\", \"basketbal\", \"salvag\", \"fusion\", \"pun\", \"netflix\", \"emperor\", \"cent\", \"nostalg\", \"inclus\", \"daddi\", \"zen\", \"mommi\", \"immacul\", \"bloodborn\", \"primal\", \"preachi\", \"offenc\", \"stile\", \"amiga\", \"garena\", \"telah\", \"terimakasih\", \"sangat\", \"membuat\", \"bay\", \"absenc\", \"nyx\", \"oda\", \"keren\", \"tutu\", \"workabl\", \"sharehold\", \"bagu\", \"ballz\", \"94\", \"famous\", \"consent\", \"rs\", \"ini\", \"smite\", \"dishearten\", \"gaijin\"], \"Freq\": [57571.0, 25627.0, 17422.0, 17476.0, 15939.0, 17458.0, 14350.0, 7388.0, 7049.0, 5539.0, 6089.0, 4512.0, 5548.0, 4178.0, 5857.0, 5129.0, 3572.0, 4586.0, 3538.0, 3864.0, 9295.0, 4230.0, 4874.0, 1641.0, 3759.0, 3378.0, 3175.0, 4405.0, 3839.0, 6257.0, 57570.23116047622, 25626.101900909616, 17475.45079431901, 9294.674877705678, 5653.105165472654, 5526.776473639563, 4778.490195436955, 4629.18668881847, 4564.375319914557, 4449.193166351168, 4067.7976533278347, 3283.8354701528087, 3136.6937106921905, 3039.2235413716344, 2968.1555138177737, 2909.7814995833332, 2794.029658177708, 2644.5867372673956, 2625.731838906288, 2643.552099223484, 2589.296586039808, 2372.3837529618154, 2284.884890097289, 2194.2389442086346, 2121.947090350211, 1912.8600933658731, 1789.0052385520473, 1755.0398124798573, 1715.1110639199644, 1692.4738157193492, 17307.757339541095, 17188.66403714778, 8711.428700720131, 3027.546709822699, 5854.168409965972, 15558.517020063875, 4265.210177719106, 3467.7918025835565, 7210.705325568753, 6883.840851774534, 13515.92544102464, 5904.234417527243, 8920.815285099765, 6676.216879219822, 7486.0605826013725, 5162.497072606687, 3956.3421782454566, 4281.720509842539, 8465.365070497499, 8765.795084829624, 5863.167826604193, 4946.5478730009645, 6448.95035293952, 6338.290317775991, 5153.017372553646, 6985.4538193913, 5080.339357619554, 5369.902605847978, 2992.2393651314333, 2773.904860318663, 2574.7770736131038, 2120.4814249148594, 2087.341720277741, 1895.3740392493005, 1892.2691335086831, 1888.6440305351439, 1863.1017503154696, 1802.0875898959187, 1727.2301993222595, 1665.598519622464, 1666.8404360661564, 1636.465411213879, 1523.5994624836821, 1492.111137603609, 1469.794469974765, 1464.4683518537317, 1438.6953186525961, 1416.217937819575, 1387.8483890779119, 1385.448924891151, 1360.6618362471684, 1360.4953914736552, 1363.623223491616, 1334.6785690827937, 1326.818868052541, 1326.024816436752, 1295.8904028651934, 1289.5967958402687, 3962.8799443695275, 2426.949583616695, 2102.8903215720384, 1946.8462830385263, 2233.5437372105025, 3431.1407063819015, 2330.1738694830333, 1426.406833996531, 2000.5082155768207, 5349.984834273193, 3327.0462367319046, 2153.2263390561247, 2722.810212317283, 3425.3220171921453, 2679.904372057421, 3136.586187701567, 3415.4492743797546, 3334.203591255742, 3094.84637790739, 2081.560859445601, 3278.043840800884, 3691.8409083603233, 3659.624674174597, 2641.227512916323, 4555.16702229554, 2274.688610107618, 2996.1962113381314, 3513.488914183358, 3935.914056958214, 2986.0068566356285, 4087.6745498328846, 3848.5525600900755, 2685.2906716564803, 3594.3127538042513, 2715.1970833998585, 3947.137845294452, 2902.87204825852, 2929.6180726701095, 2884.7293386828164, 841.0521978917036, 656.1857868530253, 565.3730581722309, 486.00358656070466, 472.8116308640444, 465.37988697149405, 457.64730007013225, 445.48955621451876, 435.65426304449085, 426.449040399248, 370.847600044793, 367.3684704526268, 362.7825089166741, 347.14809300198203, 337.0790679558775, 333.08930394698535, 332.7802896406575, 324.94426107574867, 319.04878255460676, 304.25408521651747, 250.6197772220607, 237.10211647351903, 230.24766758269678, 229.4362515954866, 229.2438677136095, 225.21930913208485, 219.112830915927, 209.82745173314242, 208.6955734313045, 207.44683943114947, 392.45300218474466, 337.7855677596881, 275.6121309839498, 279.5864330138323, 269.9129479181828, 287.1546109516312, 260.988273154336, 262.0421862189687, 257.49017959851017, 272.796781078754, 269.8412698697934, 276.08577952770685, 270.8771610251168, 1640.5607958472754, 1302.2065544816155, 850.0428152539693, 848.4197824439755, 750.3074696645239, 589.6668812774636, 579.9010323407939, 482.8682484021148, 447.65260825220696, 419.99099398028324, 362.4839718401195, 341.60380637997014, 341.32474683662673, 311.1992158836315, 304.81223801227543, 304.76506480449365, 304.1626886719606, 299.5390281535946, 287.49771521246817, 281.8361926612213, 272.4522923225405, 272.2458795191811, 267.44371558256864, 267.2525868985305, 265.9408801060029, 264.9291949149675, 261.16597370654154, 253.20390815819172, 240.3979260958692, 239.65759834988847, 870.7670767551712, 703.8340847024514, 688.0765378454761, 496.83111680208293, 472.2701348869625, 372.39399951497273, 353.75331331893415, 307.06798857803335, 303.96158046797757, 253.0403768859287, 242.90131947383404, 233.72401084568975, 233.62417946103184, 224.1978890474305, 214.3916650712984, 207.00151350216086, 185.79577231350638, 180.11649648200174, 175.58850735470295, 168.12822768737743, 151.44128308350466, 151.03889957941095, 120.85177398268931, 107.20802710523573, 104.63785374969706, 104.03927565662865, 102.64681057273327, 99.72884026121184, 97.98524988697409, 96.44727039779849, 99.09244781474668, 761.6524074030475, 424.2326272334496, 421.1267634894457, 413.43796062236817, 394.7454636688376, 342.14176868429126, 283.1159742066062, 215.87035014318442, 210.7478793690206, 159.94952826768605, 159.87382811542355, 158.66159383080372, 148.10913018520156, 143.59258894454402, 134.53685270019128, 129.7101081197741, 125.35508242497029, 124.33259881114971, 119.98829808628645, 117.88544572569361, 112.37259229932111, 108.8033191762977, 104.89191794876453, 104.68246835358472, 104.14682176813626, 91.0299177517236, 89.3713386721806, 84.32186335669368, 82.09678386053949, 80.02317502221163, 309.58241715592595, 417.0018087241535, 184.1863969265981, 254.10649392415613, 96.01996016162897, 352.773429798477, 318.9680697634056, 208.76365590543767, 201.62694870527403, 190.4760014217393, 187.56550557794785, 156.79930413007733, 154.45138809264617, 153.69332480454858, 144.68139136758197, 144.03335467413336, 123.3736918582202, 115.71194995651835, 104.03772416844664, 101.9638515902586, 86.4017125497091, 82.27520373668266, 74.49892539463453, 70.77338363771574, 64.12302791630867, 61.91775173385732, 60.71739108352638, 55.413313817658064, 52.9388578186296, 50.1160652595469, 45.76113574578223, 45.74532571015721, 39.308296773028886, 37.85151004745004, 34.068139536606516, 154.41522407478618, 312.87885942813216, 289.5526889043492, 200.92271412656373, 150.0936287954893, 137.52284936910527, 126.10626174728074, 124.0287956766391, 102.29528188198609, 92.64881090051149, 87.40992511757246, 84.1678592236865, 80.27045211404602, 70.95682701599719, 67.26020527294837, 66.54244148623503, 63.67918916075156, 56.70210040457104, 54.43921196915107, 54.432686205288555, 53.93296721737858, 53.60730478243162, 51.070918518595775, 45.000412886876454, 35.71198732609122, 35.56693594176369, 33.4346872967009, 32.68207899589978, 32.00519381999642, 28.566448299404914, 26.06148882450285, 413.2380696248041, 271.5629372835252, 134.207060339643, 113.97589729592032, 53.67805100399681, 52.398286530846406, 49.97780810460286, 39.55572730952767, 39.07182088687264, 28.962867977813527, 27.041494468021693, 26.964341217943804, 26.367341551515057, 26.330220877620295, 25.628494435344315, 23.496725482912794, 23.447778107924275, 23.38933443807734, 22.569298975643232, 21.703807325550397, 21.172437116696145, 18.56105110413005, 18.41730107889896, 17.616071375700773, 15.806799470976257, 14.771294294261004, 12.462197788806217, 11.815512269511721, 10.831172112428291, 10.364282740617929, 6.684919990929481, 4.311908334198722, 4.080928938199415, 3.150398308049476, 2.5937316216972124, 0.9610377402156922, 0.8941251255996995, 0.8567611924484575, 0.49034264880350126, 0.28239180068210284, 0.012156005513300502, 0.011939061335465162, 0.011939052137415479, 0.011846167165692438, 0.011846167165692438, 0.011846167165692438, 0.011846166143686918, 0.011846166143686918, 0.01208962727675498, 0.012178615341428674, 0.011803039554736388, 0.01181261574646269, 0.011846158989648276, 0.011811918738697766, 0.011842429691504276, 0.011757060548378377, 0.011846161033659316, 0.011824771480122523, 0.011905320845214137, 0.01176845795394206, 0.011965137806318586, 0.011940002602549465, 0.011846130373493703, 0.012150534717749713, 0.011905274854965718, 0.011828861546215229], \"Total\": [57571.0, 25627.0, 17422.0, 17476.0, 15939.0, 17458.0, 14350.0, 7388.0, 7049.0, 5539.0, 6089.0, 4512.0, 5548.0, 4178.0, 5857.0, 5129.0, 3572.0, 4586.0, 3538.0, 3864.0, 9295.0, 4230.0, 4874.0, 1641.0, 3759.0, 3378.0, 3175.0, 4405.0, 3839.0, 6257.0, 57571.132998073124, 25627.003739304757, 17476.35262509306, 9295.57670940006, 5654.007002228791, 5527.678314063204, 4779.392027630272, 4630.088538706009, 4565.277171763015, 4450.095003461223, 4068.6995013269852, 3284.737350347175, 3137.595536579333, 3040.1253637799373, 2969.0573448270993, 2910.6898527458834, 2794.931481466482, 2645.488613355942, 2626.6336789403745, 2644.4695659490153, 2590.1984220055906, 2373.285609555822, 2285.7867185701216, 2195.1407923925, 2122.848921919326, 1913.7619224122288, 1789.907080271822, 1755.941652693122, 1716.0128962620313, 1693.3757290986412, 17422.009381056523, 17458.598499045765, 8787.760968720066, 3029.3957869625465, 5910.911329220047, 15939.963857098699, 4301.854077486728, 3489.4529669783897, 7388.332492097272, 7049.382389820831, 14350.770824570336, 6089.4297187429775, 9395.771934462091, 6961.749237494133, 8176.041933633238, 5539.532059100517, 4116.829993663114, 4512.122751324276, 10146.735801085855, 10799.104922445178, 7073.198365497373, 5646.29324344331, 8282.794228256815, 8142.977137679554, 6077.48857358941, 10933.348308731287, 5946.783168573109, 8255.388588159594, 2993.125419728673, 2774.7909052173413, 2575.6683754821424, 2121.36746424072, 2088.227768820489, 1896.2601042587723, 1893.155279339325, 1889.5300689230587, 1863.9877916381495, 1802.9736403737747, 1728.123525414115, 1666.4845751922087, 1667.72731157816, 1637.3514254806516, 1524.4855191703646, 1492.9971797936735, 1470.6805656587271, 1465.3544082159995, 1439.5820502471536, 1417.103977947972, 1388.734407281249, 1386.3349615198315, 1361.5478696825646, 1361.3814361643063, 1364.5182380426463, 1335.5646088299927, 1327.7048919804479, 1326.910856411304, 1296.7764401543277, 1290.4832613851001, 3976.7062140479375, 2430.9573331039032, 2104.4095238067994, 1948.9306654869895, 2239.285773094129, 3461.395325699117, 2347.875974388853, 1428.5796433574726, 2023.4466837548612, 5695.235962839524, 3461.5971305273733, 2200.9092618461605, 2841.9795686175808, 3647.269871771259, 2838.185911936178, 3378.5694230964787, 3721.7308217894215, 3728.5608676753263, 3522.38409939961, 2180.2988391662134, 3855.919661098457, 4497.062015856871, 4471.876157626416, 3074.1590920654344, 7072.809741727987, 2498.8315393992466, 3839.6911720467247, 4931.265275241537, 5972.683397334344, 4013.101085294493, 7354.942487385083, 6593.360129433981, 3441.3928900169976, 6257.185851019999, 3574.1940707875397, 10933.348308731287, 5114.272045018194, 6558.571862441157, 8255.388588159594, 841.944198053707, 657.0777654406667, 566.2651180007019, 486.8955845960804, 473.70371860753914, 466.27195705561434, 458.539263618347, 446.38159208916215, 436.546214654973, 427.3410094982042, 371.73952173129226, 368.2604281418615, 363.67444192175356, 348.0400688576843, 337.97100611267757, 333.98128215946457, 333.6722366206007, 325.8362081851209, 319.9411403716339, 305.14600389689707, 251.51194654287966, 237.99402353870005, 231.13954893635184, 230.32813480164864, 230.1357662818033, 226.11121473456367, 220.00484034769693, 210.71941368079308, 209.587449377151, 208.33905084572052, 769.3155870360981, 705.4828574017939, 495.337496024544, 632.9308422905975, 684.8891702450298, 1052.6372240680162, 563.4740515465545, 581.678750762828, 531.5315454652618, 1241.0089930753002, 1376.1948968293384, 2802.7929607145284, 2347.900112416796, 1641.4663451639194, 1303.1122787642785, 850.9483670660996, 849.325339075237, 751.2130325439399, 590.5724528446862, 580.8066011428828, 483.7738440522275, 448.55817511567903, 420.89659322696394, 363.38959433701314, 342.5093544914988, 342.23032783930677, 312.10479917565755, 305.7178117701131, 305.6706181353916, 305.068230542161, 300.44463212378383, 288.40328143635264, 282.74179155562086, 273.3578861877173, 273.1514047798501, 268.34924815302804, 268.1581326738949, 266.84673350817957, 265.8347666333963, 262.0716775701395, 254.10947953761595, 241.3034991117027, 240.56312538630894, 871.6782714336811, 704.745237730431, 688.9877656104053, 497.7422748649576, 473.18134247023653, 373.305191246173, 354.6645297622525, 307.97915549405565, 304.8728127833225, 253.9514893515059, 243.81259431569535, 234.63520304792303, 234.53534828838775, 225.1090906967591, 215.30282662799277, 207.91276197206344, 186.70696651914807, 181.02764852028633, 176.49965439699113, 169.0395105959482, 152.35250147938504, 151.95032991150333, 121.76298600908808, 108.11921019085126, 105.54905254240583, 104.95036328417822, 103.55796512176181, 100.64009972256319, 98.89650209610309, 97.35833527551048, 792.5140791682302, 762.5694039696755, 425.1496636527123, 422.0438043344393, 414.3550378857322, 395.662408492872, 343.05867657104767, 284.0329877763167, 216.78732551890207, 211.66489185780992, 160.86654380908607, 160.79073023542026, 159.57854302674926, 149.02611894990642, 144.509525200543, 135.453825751873, 130.6270540630317, 126.27197228260425, 125.24952686592633, 120.90527377380869, 118.80232439214343, 113.29004909500746, 109.72025412106098, 105.80888352625077, 105.59946857075221, 105.06383194374695, 91.94687402322093, 90.28839006335828, 85.23889060413245, 83.01379246102532, 80.94027224134818, 323.44100272639815, 460.1763573491042, 236.1938377081027, 487.15872581009233, 323.44640356877727, 353.7063622516809, 319.90115729044487, 209.69663160064306, 202.55995179510307, 191.40898798116464, 188.49845757276952, 157.73249149692023, 155.38439964132428, 154.62631586757382, 145.61439659419432, 144.9664381218186, 124.30669631682645, 116.64497892655486, 104.97067463078082, 102.89683978941231, 87.33470413902032, 83.20818501128221, 75.43194515978887, 71.70642891324508, 65.05599752157501, 62.85071608857419, 61.650476025264396, 56.34659321048204, 53.871909230147764, 51.04898928200724, 46.69415582435429, 46.67826723052272, 40.24126891522537, 38.784473319361226, 35.00104301938188, 172.980985061256, 313.82763717155194, 290.5013700056314, 201.8714488817242, 151.04236112995395, 138.4715696608549, 127.05501626011021, 124.9775197953185, 103.24403129101742, 93.59755798779423, 88.35867583116817, 85.11661890512012, 81.21916654268841, 71.90553467638793, 68.20899360275268, 67.49117228804081, 64.62801264411527, 57.65078671737492, 55.38790801801011, 55.38142996490674, 54.88178251353378, 54.55597322649335, 52.019689549078684, 45.94918651750902, 36.66076011384214, 36.51564040174218, 34.383480232482746, 33.63078753705227, 32.95391646698129, 29.51517959626382, 27.010264422109522, 414.20428253364594, 272.5290914675712, 135.17320614428664, 114.94212214945317, 54.644159243881404, 53.3644034954758, 50.943918785934535, 40.52189859995155, 40.03814587744314, 29.929127535391565, 28.00769992648249, 27.930622240200975, 27.333458086926168, 27.296385720342766, 26.59474773482879, 24.463124721807944, 24.41397710654339, 24.355667491823983, 23.535507981733705, 22.670013469058027, 22.13866982386459, 19.527347905425067, 19.383467104196658, 18.582290978785483, 16.772896314426056, 15.737600064422493, 13.428475758231732, 12.78168376165539, 11.797509220735842, 11.330676739646838, 7.704573656474405, 5.3314797445179885, 5.100580281849416, 4.170022232981051, 3.6133677381209854, 1.9806123573328913, 1.9137306745046612, 1.876350876092197, 1.510023345917118, 1.301965246887266, 1.0505180315837648, 1.0336932977084758, 1.0371362790636651, 1.0330300693233792, 1.0330300693233792, 1.0330300693233792, 1.0330984101909384, 1.033218280823288, 1.0565741898794156, 1.0650905103710058, 1.0323186711930061, 1.0335445153486622, 1.0370399068379934, 1.035369207294936, 1.0399531407706974, 1.0326128492447983, 1.041390832987789, 1.0408233748015052, 1.0490457198491934, 1.037816422829343, 1.0575769249725333, 1.0655749436690536, 1.0501891020851775, 1.1853699274366254, 1.0930469371492553, 2.048302795490234], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.8678, -3.6772, -4.06, -4.6914, -5.1886, -5.2112, -5.3567, -5.3885, -5.4026, -5.4281, -5.5177, -5.7318, -5.7777, -5.8092, -5.8329, -5.8528, -5.8933, -5.9483, -5.9555, -5.9487, -5.9694, -6.0569, -6.0945, -6.135, -6.1685, -6.2722, -6.3392, -6.3583, -6.3814, -6.3946, -4.0697, -4.0766, -4.7562, -5.8131, -5.1537, -4.1762, -5.4703, -5.6773, -4.9453, -4.9917, -4.317, -5.1452, -4.7324, -5.0223, -4.9078, -5.2794, -5.5455, -5.4665, -4.7849, -4.75, -5.1521, -5.3221, -5.0569, -5.0742, -5.2813, -4.977, -5.2955, -5.24, -5.7963, -5.8721, -5.9465, -6.1407, -6.1564, -6.2529, -6.2545, -6.2565, -6.2701, -6.3034, -6.3458, -6.3821, -6.3814, -6.3998, -6.4712, -6.4921, -6.5072, -6.5108, -6.5286, -6.5443, -6.5646, -6.5663, -6.5843, -6.5845, -6.5822, -6.6036, -6.6095, -6.6101, -6.6331, -6.638, -5.5153, -6.0057, -6.149, -6.2261, -6.0887, -5.6594, -6.0464, -6.5372, -6.1989, -5.2152, -5.6902, -6.1253, -5.8906, -5.6611, -5.9065, -5.7492, -5.664, -5.6881, -5.7626, -6.1592, -5.7051, -5.5862, -5.595, -5.9211, -5.3761, -6.0705, -5.795, -5.6357, -5.5222, -5.7984, -5.4843, -5.5446, -5.9045, -5.613, -5.8934, -5.5193, -5.8266, -5.8174, -5.8329, -3.9568, -4.205, -4.354, -4.5053, -4.5328, -4.5486, -4.5654, -4.5923, -4.6146, -4.636, -4.7757, -4.7851, -4.7977, -4.8417, -4.8712, -4.8831, -4.884, -4.9078, -4.9261, -4.9736, -5.1675, -5.223, -5.2523, -5.2558, -5.2567, -5.2744, -5.3019, -5.3452, -5.3506, -5.3566, -4.7191, -4.8691, -5.0725, -5.0582, -5.0934, -5.0315, -5.127, -5.123, -5.1405, -5.0827, -5.0936, -5.0708, -5.0898, -3.1115, -3.3425, -3.769, -3.7709, -3.8938, -4.1347, -4.1514, -4.3345, -4.4103, -4.474, -4.6213, -4.6806, -4.6814, -4.7738, -4.7946, -4.7947, -4.7967, -4.812, -4.8531, -4.873, -4.9068, -4.9076, -4.9254, -4.9261, -4.931, -4.9348, -4.9491, -4.9801, -5.032, -5.0351, -3.1352, -3.348, -3.3706, -3.6963, -3.747, -3.9846, -4.0359, -4.1775, -4.1876, -4.371, -4.4119, -4.4504, -4.4508, -4.492, -4.5367, -4.5718, -4.6799, -4.7109, -4.7364, -4.7798, -4.8843, -4.887, -5.11, -5.2298, -5.254, -5.2598, -5.2732, -5.3021, -5.3197, -5.3355, -5.3085, -3.0929, -3.6781, -3.6854, -3.7038, -3.7501, -3.8931, -4.0825, -4.3537, -4.3777, -4.6535, -4.654, -4.6616, -4.7304, -4.7614, -4.8265, -4.8631, -4.8972, -4.9054, -4.941, -4.9586, -5.0065, -5.0388, -5.0754, -5.0774, -5.0826, -5.2172, -5.2356, -5.2937, -5.3205, -5.346, -3.9931, -3.6953, -4.5124, -4.1906, -5.1638, -3.2561, -3.3568, -3.7807, -3.8155, -3.8724, -3.8878, -4.067, -4.0821, -4.087, -4.1474, -4.1519, -4.3067, -4.3708, -4.4772, -4.4973, -4.6629, -4.7119, -4.8111, -4.8625, -4.9611, -4.9961, -5.0157, -5.1071, -5.1528, -5.2076, -5.2985, -5.2988, -5.4505, -5.4883, -5.5936, -4.0823, -3.0871, -3.1646, -3.53, -3.8216, -3.9091, -3.9958, -4.0124, -4.205, -4.3041, -4.3623, -4.4001, -4.4475, -4.5708, -4.6243, -4.6351, -4.679, -4.7951, -4.8358, -4.8359, -4.8452, -4.8512, -4.8997, -5.0262, -5.2574, -5.2615, -5.3233, -5.3461, -5.367, -5.4807, -5.5724, -2.4415, -2.8613, -3.5661, -3.7295, -4.4825, -4.5066, -4.5539, -4.7878, -4.8001, -5.0995, -5.1681, -5.171, -5.1934, -5.1948, -5.2218, -5.3086, -5.3107, -5.3132, -5.3489, -5.388, -5.4128, -5.5444, -5.5522, -5.5967, -5.7051, -5.7728, -5.9428, -5.9961, -6.0831, -6.1271, -4.4314, -4.8699, -4.9249, -5.1837, -5.3781, -6.371, -6.4432, -6.4858, -7.0439, -7.5957, -10.7412, -10.7592, -10.7592, -10.767, -10.767, -10.767, -10.767, -10.767, -10.7466, -10.7393, -10.7706, -10.7698, -10.767, -10.7699, -10.7673, -10.7745, -10.767, -10.7688, -10.762, -10.7736, -10.757, -10.7591, -10.767, -10.7416, -10.762, -10.7685], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.7462, 0.7462, 0.7462, 0.7461, 0.7461, 0.7461, 0.746, 0.746, 0.746, 0.746, 0.746, 0.7459, 0.7459, 0.7459, 0.7459, 0.7459, 0.7459, 0.7459, 0.7459, 0.7459, 0.7459, 0.7458, 0.7458, 0.7458, 0.7458, 0.7458, 0.7457, 0.7457, 0.7457, 0.7457, 0.7396, 0.7306, 0.7375, 0.7456, 0.7366, 0.722, 0.7377, 0.74, 0.7219, 0.7225, 0.6863, 0.7153, 0.6944, 0.7043, 0.6581, 0.6757, 0.7065, 0.6938, 0.5651, 0.5376, 0.5586, 0.6139, 0.496, 0.4957, 0.5812, 0.2982, 0.5888, 0.3162, 0.7744, 0.7744, 0.7744, 0.7743, 0.7743, 0.7743, 0.7743, 0.7743, 0.7743, 0.7743, 0.7742, 0.7742, 0.7742, 0.7742, 0.7742, 0.7742, 0.7741, 0.7741, 0.7741, 0.7741, 0.7741, 0.7741, 0.7741, 0.7741, 0.7741, 0.7741, 0.7741, 0.7741, 0.7741, 0.7741, 0.7713, 0.7731, 0.774, 0.7737, 0.7722, 0.766, 0.7672, 0.7732, 0.7633, 0.7122, 0.7351, 0.7528, 0.7319, 0.712, 0.7174, 0.7004, 0.6889, 0.663, 0.6453, 0.7284, 0.6124, 0.5774, 0.5743, 0.623, 0.3347, 0.6808, 0.5267, 0.4358, 0.3577, 0.4791, 0.1873, 0.2364, 0.5267, 0.2204, 0.4999, -0.2441, 0.2084, -0.0312, -0.2767, 3.8823, 3.882, 3.8818, 3.8815, 3.8815, 3.8814, 3.8814, 3.8813, 3.8813, 3.8812, 3.8809, 3.8809, 3.8809, 3.8808, 3.8807, 3.8807, 3.8807, 3.8806, 3.8805, 3.8804, 3.8798, 3.8796, 3.8795, 3.8795, 3.8795, 3.8794, 3.8793, 3.8791, 3.8791, 3.879, 3.2103, 3.1469, 3.2971, 3.0663, 2.9522, 2.5843, 3.1137, 3.0859, 3.1586, 2.3684, 2.2541, 1.5657, 1.7237, 4.06, 4.0598, 4.0595, 4.0595, 4.0593, 4.059, 4.059, 4.0587, 4.0585, 4.0584, 4.058, 4.0579, 4.0579, 4.0576, 4.0576, 4.0576, 4.0576, 4.0575, 4.0574, 4.0573, 4.0572, 4.0572, 4.0572, 4.0572, 4.0571, 4.0571, 4.0571, 4.057, 4.0568, 4.0568, 4.6692, 4.669, 4.6689, 4.6684, 4.6683, 4.6678, 4.6677, 4.6673, 4.6673, 4.6667, 4.6665, 4.6664, 4.6664, 4.6662, 4.666, 4.6659, 4.6654, 4.6652, 4.6651, 4.6649, 4.6643, 4.6643, 4.6628, 4.6618, 4.6616, 4.6616, 4.6614, 4.6612, 4.661, 4.6609, 2.5911, 4.8453, 4.8443, 4.8443, 4.8442, 4.8441, 4.8438, 4.8432, 4.8422, 4.8421, 4.8407, 4.8407, 4.8407, 4.8403, 4.8401, 4.8397, 4.8394, 4.8392, 4.8391, 4.8388, 4.8387, 4.8383, 4.8381, 4.8378, 4.8377, 4.8377, 4.8364, 4.8362, 4.8356, 4.8353, 4.8351, 4.8027, 4.7479, 4.5978, 4.1956, 3.632, 5.4502, 5.45, 5.4484, 5.4483, 5.448, 5.4479, 5.4469, 5.4469, 5.4468, 5.4465, 5.4464, 5.4453, 5.4448, 5.444, 5.4438, 5.4421, 5.4416, 5.4404, 5.4398, 5.4384, 5.4379, 5.4376, 5.4362, 5.4354, 5.4344, 5.4327, 5.4327, 5.4294, 5.4285, 5.4259, 5.3393, 5.7389, 5.7386, 5.7372, 5.7356, 5.735, 5.7344, 5.7343, 5.7327, 5.7317, 5.7311, 5.7307, 5.7302, 5.7286, 5.7279, 5.7278, 5.7271, 5.7253, 5.7246, 5.7246, 5.7245, 5.7244, 5.7235, 5.721, 5.7157, 5.7156, 5.7139, 5.7133, 5.7127, 5.7092, 5.7062, 6.107, 6.1058, 6.1021, 6.1009, 6.0915, 6.091, 6.0902, 6.0852, 6.0849, 6.0765, 6.0742, 6.0741, 6.0733, 6.0733, 6.0723, 6.069, 6.0689, 6.0688, 6.0674, 6.0658, 6.0647, 6.0586, 6.0582, 6.0559, 6.05, 6.0459, 6.0346, 6.0307, 6.0239, 6.0202, 8.1016, 8.0313, 8.0205, 7.9632, 7.912, 7.5204, 7.4826, 7.4596, 7.1188, 6.7152, 3.7844, 3.7825, 3.7792, 3.7753, 3.7753, 3.7753, 3.7753, 3.7751, 3.7731, 3.7724, 3.7724, 3.772, 3.7714, 3.7702, 3.7683, 3.7682, 3.7673, 3.766, 3.7649, 3.7641, 3.7618, 3.7522, 3.7588, 3.6631, 3.7238, 3.0893]}, \"token.table\": {\"Topic\": [1, 3, 9, 1, 2, 3, 9, 9, 1, 2, 3, 9, 8, 7, 8, 6, 7, 2, 5, 1, 3, 6, 6, 6, 2, 3, 1, 2, 3, 1, 8, 4, 2, 3, 7, 1, 2, 2, 3, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 2, 8, 6, 1, 2, 1, 2, 9, 1, 2, 3, 1, 2, 8, 1, 1, 2, 5, 8, 10, 7, 3, 5, 5, 1, 2, 1, 7, 1, 2, 3, 7, 4, 1, 2, 3, 1, 2, 4, 2, 3, 1, 5, 1, 8, 4, 10, 1, 2, 1, 2, 3, 9, 3, 6, 5, 8, 3, 6, 1, 2, 1, 1, 2, 1, 3, 1, 4, 1, 2, 3, 2, 1, 6, 3, 9, 10, 6, 2, 3, 5, 4, 2, 8, 1, 2, 1, 2, 1, 3, 6, 1, 5, 5, 3, 1, 2, 5, 1, 2, 3, 4, 1, 2, 7, 3, 9, 7, 1, 2, 2, 1, 2, 1, 2, 3, 2, 7, 1, 2, 1, 3, 9, 8, 1, 2, 8, 9, 1, 2, 1, 2, 7, 9, 1, 8, 3, 1, 2, 3, 1, 1, 1, 9, 6, 6, 1, 1, 2, 3, 2, 6, 1, 2, 3, 1, 2, 1, 3, 1, 2, 1, 3, 8, 1, 7, 7, 1, 2, 3, 6, 9, 2, 1, 2, 1, 2, 3, 6, 6, 9, 4, 10, 5, 5, 10, 3, 5, 9, 1, 2, 5, 3, 6, 2, 5, 7, 3, 9, 3, 5, 5, 7, 7, 4, 5, 4, 9, 9, 1, 2, 1, 2, 3, 1, 2, 2, 7, 1, 2, 3, 1, 2, 3, 1, 3, 7, 2, 1, 2, 4, 1, 2, 3, 1, 2, 6, 7, 4, 1, 10, 1, 3, 2, 1, 2, 3, 1, 7, 3, 6, 1, 2, 3, 4, 9, 1, 2, 3, 9, 1, 2, 10, 9, 5, 8, 8, 2, 1, 2, 1, 4, 1, 2, 5, 3, 1, 2, 3, 4, 5, 7, 6, 8, 1, 2, 6, 4, 7, 2, 6, 6, 1, 7, 1, 2, 5, 3, 2, 4, 5, 2, 3, 6, 9, 9, 2, 4, 7, 2, 4, 1, 2, 3, 1, 2, 1, 3, 6, 3, 8, 1, 2, 3, 1, 2, 3, 2, 4, 2, 3, 3, 4, 4, 1, 2, 3, 9, 4, 9, 5, 1, 2, 9, 1, 8, 5, 2, 1, 2, 2, 2, 3, 7, 6, 7, 8, 5, 1, 2, 8, 1, 2, 3, 2, 3, 8, 5, 1, 2, 7, 8, 7, 1, 8, 9, 1, 2, 3, 6, 4, 1, 2, 8, 3, 4, 1, 2, 1, 2, 3, 8, 6, 1, 2, 7, 5, 9, 9, 1, 2, 1, 2, 5, 4, 8, 8, 1, 2, 3, 1, 2, 6, 6, 5, 1, 7, 7, 3, 1, 2, 2, 8, 1, 2, 4, 2, 7, 5, 4, 1, 2, 6, 5, 8, 1, 2, 1, 3, 6, 1, 2, 8, 4, 6, 1, 1, 9, 10, 3, 3, 6, 6, 6], \"Freq\": [0.9695489188138203, 0.03021629421777489, 0.9401906036760862, 0.83031219961605, 0.12958421348468416, 0.040010702911821815, 0.987120578798542, 0.9970925396370133, 0.7954433421628074, 0.1548149048617306, 0.049715208885175464, 0.9772468058837174, 0.9597632286456056, 0.9797735214063197, 0.9936156668972906, 0.9983258240392481, 0.9794513212355221, 0.9994690898672547, 0.99922187869549, 0.9996018718925767, 0.9965773456892415, 0.9883831346830108, 0.9967297661139917, 0.996363141533651, 0.9996815193294661, 0.9954326443631473, 0.39202926977151087, 0.5743796149852467, 0.03340159697604801, 0.9998101933240311, 0.9846231757279357, 0.9978060759013005, 0.999401253654612, 0.9974336548114818, 0.9910904849874254, 0.14964004717764967, 0.8501214465309109, 0.9995976126181356, 0.9962444079808946, 0.24427789329722868, 0.19784490531511084, 0.5571958557854142, 0.9968268919875926, 0.5213839774432846, 0.03633888327635014, 0.44238640510339294, 0.9318476623886859, 0.027619661438486813, 0.04043662851124867, 0.9993498581567856, 0.9982741217171484, 0.9934355408959746, 0.18135564837074175, 0.8184484254462575, 0.0556693623682369, 0.9442651338409803, 0.9539199253404255, 0.5587871323834472, 0.24487810612902694, 0.19619314140901267, 0.021354810402576793, 0.9782320595052731, 0.9916963824714299, 0.9998218958292068, 0.9903718181427098, 0.009474004409974675, 0.9975034043733226, 0.9879505742321236, 0.532949360773429, 0.9972356243663136, 0.9950855390526819, 0.9909446403572241, 0.9936397149413825, 0.9609335352903391, 0.03886485481457387, 0.9994932249378642, 0.989448969947972, 0.5270925803525942, 0.07884487351534651, 0.39422436757673257, 0.9713996231818706, 0.9945980927897805, 0.9367618792478491, 0.00028258276900387607, 0.0627333747188605, 0.06040136040798718, 0.9393816226242192, 0.9964983881138243, 0.5359607938841325, 0.4631979046482073, 0.9998772877101986, 0.9977174089437065, 0.9996558215323721, 0.9839330562319777, 0.9978698016534421, 0.9085512465855486, 0.08221980972091746, 0.917583824172984, 0.2963984104554525, 0.4293976971982838, 0.27264853782280407, 0.9918035083063094, 0.9935727323308575, 0.9963682124080938, 0.9911225515416427, 0.9898091227483894, 0.9958233256284337, 0.997526786737018, 0.007240586890210443, 0.9923863208347254, 0.9997629933861787, 0.24005411653848663, 0.7596118023333231, 0.7792046676500802, 0.21998228983295956, 0.9997587486426238, 0.99578473784245, 0.45465110757575294, 0.49835177811439707, 0.04698335001572537, 0.9996255190597502, 0.9996298298111832, 0.9968587522853976, 0.9980106453899545, 0.9744323293037189, 0.7194206247325858, 0.9877876623754867, 0.4887461093159382, 0.5095438160953398, 0.9962532633538176, 0.9983852349581422, 0.9997405040615847, 0.9874066067311327, 0.9995392523589841, 0.00033009882838803967, 0.038710455014614925, 0.9611170435345064, 0.9998153031717939, 0.7018164292302325, 0.29680342381542874, 0.9996438786105154, 0.9972928058549113, 0.9896483687877886, 0.9970116404668637, 0.13248975981625585, 0.7419426549710328, 0.1249189163981841, 0.40602356861560146, 0.4952203104028601, 0.09847320293313357, 0.9950325699153788, 0.000475192679317967, 0.9993302046056846, 0.9837678682703344, 0.9970583952706404, 0.882559817897663, 0.9971830133467549, 0.10567079738881938, 0.8941787779043752, 0.9997149676338299, 0.8542433540954062, 0.14562494973358697, 0.15912738098525514, 0.7802710858131332, 0.060421526004221265, 0.9994599808050588, 0.9953561153446021, 0.8342584419212055, 0.1656690420401118, 0.9997539370596871, 0.9968619686189751, 0.991320719706579, 0.9973627651821515, 0.4161762661423992, 0.5837691138418712, 0.9927224217421416, 0.9882117457237019, 0.0012340817171684086, 0.9983721091892426, 0.17878339172665506, 0.8209804505656847, 0.9966779075308669, 0.9640206111488053, 0.9998280774171804, 0.9858789166486156, 0.9977658556734544, 0.9109987718858937, 0.013540597024228631, 0.07557542525150863, 0.9997087437853549, 0.9995373242468959, 0.9999226025520269, 0.8936233877954415, 0.4882090686014336, 0.996474106465744, 0.9999803200316874, 0.6172229055940838, 0.330063297070172, 0.052664845199642304, 0.9993897598617858, 0.9897019443751639, 0.9761000802439687, 0.015056495871106914, 0.008845691324275312, 0.6504842192046535, 0.34946870994514434, 0.9934560142539879, 0.006486048625531583, 0.991432978240801, 0.008368484693240055, 0.9345791227367857, 0.06521621311700242, 0.9710530167806585, 0.9999379587283189, 0.9944705813101397, 0.9851339892091541, 0.644820245453275, 0.28836020877530943, 0.0665037381861876, 0.9923552399449515, 0.9525070559295098, 0.9997092231158798, 0.0005131018859258007, 0.9990093718975338, 0.9912650140356235, 0.008648391811124714, 0.040192801439577604, 0.9584437266360814, 0.9932465598106051, 0.9776366468763341, 0.9985128742184136, 0.5225395680397056, 0.9965036884651509, 0.9968207085557953, 0.7842244958351372, 0.9971971156722563, 0.9937457857968666, 0.9689557427194341, 0.04152035479178288, 0.9581349669324133, 0.9946120501585198, 0.21592434626947268, 0.7790211708545681, 0.9993553854936634, 0.9860480844124277, 0.985469314291958, 0.9969049080122323, 0.9980585871962436, 0.9988785503173611, 0.9943232510133877, 0.10405768006018605, 0.8902712627371473, 0.9810167276376666, 0.9976591367217871, 0.9909349463620876, 0.9991464444143422, 0.968664198647505, 0.9512151707008498, 0.010872537525513216, 0.9889067085705429, 0.005623681985077101, 0.9284994940624666, 0.06570828424669034, 0.9845578384163826, 0.015407880536040893, 0.9993320953266561, 0.9959494872262867, 0.4659947894784778, 0.4665122739754, 0.0672729845998913, 0.7312479352254978, 0.2306598086597356, 0.03807338070401261, 0.9765394497453224, 0.023406305811734193, 0.9907528970905919, 0.9995956810887898, 0.6388710761571397, 0.36100560309122837, 0.996176020561215, 0.06059326777831047, 0.9390585617226849, 0.9983597596854554, 0.12122471256691798, 0.8786662421419466, 0.9951996616050981, 0.9926388619676351, 0.9984004012169291, 0.9997755222812268, 0.5048943556762455, 0.9489990046798402, 0.05097379053628287, 0.9994120522488873, 0.7786019816838124, 0.20089838696260875, 0.020403742425889954, 0.9994097385490297, 0.9973556411060971, 0.9950698660545295, 0.9963745562794599, 0.524504430560528, 0.44674359928556595, 0.028817249237191797, 0.9973764346913854, 0.9324002036519621, 0.7583870006271624, 0.18817832060449538, 0.053191108211069676, 0.974071080098941, 0.9938520544104177, 0.006018135277571734, 0.7502607515508125, 0.9704449461411869, 0.9938504874257893, 0.9625970184400505, 0.9819763662348984, 0.9990370570194897, 0.8117339411880782, 0.18825634296547605, 0.9994637334949724, 0.9956338522292251, 0.002232854805794286, 0.9976395272288872, 0.9971690913577242, 0.9987487816028717, 0.08924171016891048, 0.9104255185393335, 0.9985144330097139, 0.9984395389914069, 0.9985663524670425, 0.9912840881095302, 0.9925125369179323, 0.9965944658386565, 0.8761500309507737, 0.12379803348182551, 0.9943231857236994, 0.9976520446553083, 0.9901483183035128, 0.9994700653928204, 0.9972958613142607, 0.9992533086605476, 0.9999608327483397, 0.9761016037748395, 0.0032690370624002276, 0.9965533752532386, 0.9985087164533915, 0.9971269543981124, 0.9989852688397174, 0.9987556238039021, 0.9971371249034828, 0.5484126755217643, 0.4504204419645838, 0.9946132751499003, 0.9388434437722192, 0.9443387255849559, 0.999471167937225, 0.9959107463268224, 0.9980029699008403, 0.9995638905874416, 0.9986112397116431, 0.0006999959747779848, 0.9981942600334064, 0.9981606228842198, 0.9494696191250973, 0.0504482232334151, 0.9759983064802578, 0.023956691200527753, 0.9966495907416868, 0.9972720704379341, 0.992178434994394, 0.9994803101484636, 0.5136101560275915, 0.4835084619014323, 0.015332849898347519, 0.8688614942396927, 0.1154222867347827, 0.9992209619299985, 0.999030681431332, 0.999075712873005, 0.9981454789118817, 0.9965859164648072, 0.9951343083568127, 0.9956811577469354, 0.5723276398728591, 0.3801222403787722, 0.047369079185662376, 0.9531313503073469, 0.9949720442210495, 0.9814714138914035, 0.9937338428195976, 0.21938791185108508, 0.7802073421459118, 0.9420832951397986, 0.9994582996877218, 0.9956831494173563, 0.9981263145691565, 0.9995372444060127, 0.9996667247577771, 0.9995772508299038, 0.9993135511651722, 0.9997194705012761, 0.9979645239523747, 0.9691543296549545, 0.985730279801709, 0.9838151414604065, 0.9930988821800983, 0.9989425430772247, 0.25566263550142027, 0.744062991819923, 0.9825452664252453, 0.30759074827017646, 0.21262033290565194, 0.4791044834807357, 0.9993354792119803, 0.9970618648053574, 0.9868813056782152, 0.9950730968113005, 0.9589543909517505, 0.04093798703134346, 0.9864644964844109, 0.9812437476714659, 0.9957806603703716, 0.9996001025270519, 0.9749420393787248, 0.9729943918662626, 0.008378124216176524, 0.9912187650241949, 0.4762308211029353, 0.521390640345455, 0.9964601660129047, 0.2873501872053708, 0.7123932305239715, 0.9902826557955058, 0.9942337274480497, 0.9997158972127005, 0.4441911008282436, 0.5558167187590632, 0.9418309417121272, 0.0468964356149942, 0.011218909560199207, 0.9822751584667291, 0.9854656648467409, 0.8289036581526334, 0.1709269184217182, 0.9894881260982433, 0.9939488642652897, 0.9286264373262135, 0.9666809342019772, 0.14052623402445716, 0.8590967223578504, 0.0449479668747964, 0.9549149697278174, 0.9962134968377007, 0.9964049713329777, 0.9887115726526802, 0.9750560798848621, 0.999764898943744, 0.9996202043855495, 0.9979853384644493, 0.3408853047373452, 0.6590002747771074, 0.9969140073015224, 0.9886128648957898, 0.9966671356007016, 0.9991875818963264, 0.9933333664375027, 0.9847173680591427, 0.9988239532333794, 0.7783394074229336, 0.22154059498121997, 0.9996239984728822, 0.9793426915806805, 0.35586988649648893, 0.6440156269334554, 0.9985200863112753, 0.9991746271083772, 0.9854799739819058, 0.994798129124037, 0.9988855174969435, 0.4323195912414812, 0.5676272154563636, 0.9931145026312378, 0.9956098800121462, 0.9849891768829268, 0.8478831243538808, 0.15203648494139063, 0.946592170603958, 0.053372959303333155, 0.9898744227764646, 0.9156019576178226, 0.08427060496909962, 0.9803980077944017, 0.9968598289683173, 0.9950822399135665, 0.9998224347313118, 0.9997202422295596, 0.9485664751801326, 0.8302503972540731, 0.9950647989221608, 0.09126935647442984, 0.9061743249961248, 0.9900236999117459, 0.989926725150396], \"Term\": [\"10\", \"10\", \"1060\", \"2\", \"2\", \"2\", \"27\", \"2k\", \"3\", \"3\", \"3\", \"34\", \"5000\", \"700\", \"95\", \">\", \"_\", \"abil\", \"account\", \"addict\", \"adult\", \"agent\", \"alien\", \"aliv\", \"allow\", \"alpha\", \"also\", \"also\", \"also\", \"amaz\", \"announc\", \"anti\", \"area\", \"ark\", \"arm\", \"around\", \"around\", \"attack\", \"audienc\", \"audio\", \"audio\", \"audio\", \"avatar\", \"averag\", \"averag\", \"averag\", \"bad\", \"bad\", \"bad\", \"balanc\", \"ball\", \"bang\", \"base\", \"base\", \"basic\", \"basic\", \"basketbal\", \"beauti\", \"beauti\", \"beauti\", \"becom\", \"becom\", \"bed\", \"best\", \"better\", \"better\", \"biggest\", \"blame\", \"bloodborn\", \"boat\", \"boi\", \"bom\", \"bonus\", \"bore\", \"bore\", \"bought\", \"bowl\", \"brain\", \"brain\", \"brain\", \"brake\", \"buff\", \"bug\", \"bug\", \"bug\", \"build\", \"build\", \"bullshit\", \"burn\", \"burn\", \"buy\", \"c\", \"cant\", \"capac\", \"cash\", \"cent\", \"charact\", \"charact\", \"check\", \"check\", \"check\", \"children\", \"circl\", \"civil\", \"clip\", \"cock\", \"coffe\", \"com\", \"combat\", \"combat\", \"commun\", \"complet\", \"complet\", \"comput\", \"comput\", \"cool\", \"coop\", \"could\", \"could\", \"could\", \"craft\", \"crash\", \"crew\", \"cup\", \"dad\", \"daddi\", \"dare\", \"dark\", \"dark\", \"de\", \"delet\", \"design\", \"destruct\", \"dev\", \"dev\", \"differ\", \"differ\", \"dlc\", \"don\", \"don\", \"dont\", \"door\", \"dragon\", \"dri\", \"e\", \"e\", \"e\", \"easi\", \"easi\", \"easi\", \"economi\", \"either\", \"either\", \"el\", \"elder\", \"emperor\", \"empir\", \"end\", \"end\", \"enemi\", \"enjoy\", \"enjoy\", \"enough\", \"enough\", \"enough\", \"entir\", \"eso\", \"even\", \"even\", \"ever\", \"evolv\", \"factorio\", \"faster\", \"feel\", \"feel\", \"fellow\", \"fi\", \"fight\", \"fight\", \"find\", \"find\", \"fish\", \"fist\", \"fix\", \"flame\", \"forev\", \"free\", \"free\", \"free\", \"friend\", \"fuck\", \"fun\", \"fusion\", \"gaijin\", \"galaxi\", \"game\", \"gameplay\", \"gameplay\", \"gameplay\", \"gener\", \"german\", \"get\", \"get\", \"get\", \"go\", \"go\", \"good\", \"good\", \"got\", \"got\", \"graphic\", \"graphic\", \"grave\", \"great\", \"h\", \"ha\", \"hard\", \"hard\", \"hard\", \"hardest\", \"healthi\", \"high\", \"hit\", \"hit\", \"hour\", \"hour\", \"http\", \"http\", \"i\", \"i5\", \"idk\", \"immacul\", \"immedi\", \"imo\", \"inclus\", \"infin\", \"influenc\", \"infrastructur\", \"interest\", \"interest\", \"irl\", \"it\", \"it\", \"item\", \"jogo\", \"juego\", \"keyboard\", \"kick\", \"kid\", \"killer\", \"l\", \"l\", \"la\", \"lame\", \"laughabl\", \"launcher\", \"leap\", \"lego\", \"less\", \"less\", \"level\", \"level\", \"level\", \"like\", \"like\", \"limit\", \"linux\", \"long\", \"long\", \"long\", \"look\", \"look\", \"look\", \"love\", \"love\", \"lvl\", \"main\", \"make\", \"make\", \"manual\", \"map\", \"map\", \"master\", \"mechan\", \"mechan\", \"men\", \"mob\", \"mobil\", \"mod\", \"mommi\", \"money\", \"money\", \"move\", \"much\", \"much\", \"much\", \"multiplay\", \"n\", \"nasa\", \"nation\", \"need\", \"need\", \"need\", \"nerf\", \"netflix\", \"never\", \"never\", \"never\", \"newli\", \"nice\", \"nice\", \"nostalg\", \"nuanc\", \"occur\", \"oddli\", \"offend\", \"often\", \"one\", \"one\", \"onlin\", \"oppon\", \"option\", \"option\", \"own\", \"paint\", \"part\", \"part\", \"parti\", \"pass\", \"path\", \"patient\", \"paywal\", \"peac\", \"peopl\", \"peopl\", \"pilot\", \"pirat\", \"pit\", \"place\", \"plane\", \"planet\", \"play\", \"pod\", \"point\", \"point\", \"polish\", \"potato\", \"power\", \"practic\", \"pre\", \"press\", \"press\", \"promot\", \"pun\", \"q\", \"quest\", \"r\", \"race\", \"random\", \"randomli\", \"rather\", \"rather\", \"realiti\", \"realli\", \"realli\", \"recommend\", \"recommend\", \"refin\", \"refus\", \"relationship\", \"releas\", \"replac\", \"replac\", \"requir\", \"requir\", \"requir\", \"resourc\", \"restart\", \"reward\", \"rich\", \"ring\", \"rocket\", \"rogu\", \"run\", \"run\", \"run\", \"salvag\", \"scari\", \"sci\", \"se\", \"seem\", \"seem\", \"sentenc\", \"server\", \"session\", \"setup\", \"ship\", \"shit\", \"side\", \"simpli\", \"skill\", \"skyrim\", \"smell\", \"soft\", \"solar\", \"sold\", \"solo\", \"someth\", \"someth\", \"soni\", \"soul\", \"soul\", \"soul\", \"space\", \"spare\", \"squar\", \"steep\", \"still\", \"still\", \"su\", \"subscrib\", \"subscript\", \"suck\", \"suicid\", \"summar\", \"system\", \"system\", \"t\", \"t\", \"tabl\", \"take\", \"take\", \"tedium\", \"teen\", \"terribl\", \"thing\", \"thing\", \"time\", \"time\", \"time\", \"trait\", \"translat\", \"tri\", \"tri\", \"trial\", \"truck\", \"ts\", \"tt\", \"turn\", \"turn\", \"two\", \"two\", \"un\", \"unbalanc\", \"unreal\", \"unus\", \"updat\", \"upgrad\", \"usag\", \"use\", \"use\", \"v\", \"vampir\", \"vanilla\", \"version\", \"victori\", \"void\", \"w\", \"want\", \"want\", \"wast\", \"wasteland\", \"way\", \"way\", \"weak\", \"weapon\", \"web\", \"websit\", \"week\", \"well\", \"well\", \"wiki\", \"wipe\", \"women\", \"work\", \"work\", \"worth\", \"worth\", \"worthwhil\", \"would\", \"would\", \"wound\", \"wtf\", \"www\", \"ye\", \"year\", \"yellow\", \"zen\", \"\\u2018\", \"\\u2019\", \"\\u2019\", \"\\u201c\", \"\\u201d\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [10, 7, 8, 1, 6, 2, 9, 4, 3, 5]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el10122535118620928176626185\", ldavis_el10122535118620928176626185_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el10122535118620928176626185\", ldavis_el10122535118620928176626185_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el10122535118620928176626185\", ldavis_el10122535118620928176626185_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "9     -0.197578 -0.437627       1        1  47.415338\n",
       "6      0.090716 -0.451005       2        1  46.082133\n",
       "7     -0.352068 -0.090421       3        1   2.058198\n",
       "0     -0.269173  0.336989       4        1   1.723978\n",
       "5      0.302823 -0.078436       5        1   0.936974\n",
       "1      0.307148  0.168177       6        1   0.785617\n",
       "8      0.100298  0.262718       7        1   0.428396\n",
       "3     -0.039067  0.035290       8        1   0.320863\n",
       "2     -0.010272  0.161678       9        1   0.222208\n",
       "4      0.067172  0.092637      10        1   0.026294, topic_info=             Term          Freq         Total Category  logprob  loglift\n",
       "57           game  57571.000000  57571.000000  Default  30.0000  30.0000\n",
       "144          play  25627.000000  25627.000000  Default  29.0000  29.0000\n",
       "60           good  17422.000000  17422.000000  Default  28.0000  28.0000\n",
       "174           fun  17476.000000  17476.000000  Default  27.0000  27.0000\n",
       "59            get  15939.000000  15939.000000  Default  26.0000  26.0000\n",
       "...           ...           ...           ...      ...      ...      ...\n",
       "9593           rs      0.011940      1.065575  Topic10 -10.7591   3.7522\n",
       "3628          ini      0.011846      1.050189  Topic10 -10.7670   3.7588\n",
       "18546       smite      0.012151      1.185370  Topic10 -10.7416   3.6631\n",
       "5110   dishearten      0.011905      1.093047  Topic10 -10.7620   3.7238\n",
       "29970      gaijin      0.011829      2.048303  Topic10 -10.7685   3.0893\n",
       "\n",
       "[423 rows x 6 columns], token_table=      Topic      Freq  Term\n",
       "term                       \n",
       "1         1  0.969549    10\n",
       "1         3  0.030216    10\n",
       "6694      9  0.940191  1060\n",
       "2         1  0.830312     2\n",
       "2         2  0.129584     2\n",
       "...     ...       ...   ...\n",
       "130       3  0.995065     ‘\n",
       "1351      3  0.091269     ’\n",
       "1351      6  0.906174     ’\n",
       "1352      6  0.990024     “\n",
       "1353      6  0.989927     ”\n",
       "\n",
       "[503 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[10, 7, 8, 1, 6, 2, 9, 4, 3, 5])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=30)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8aa7cac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_vis(fd):\n",
    "    \"\"\"\n",
    "    Takes a frequency distribution\n",
    "    Returns a visualization of LDA (Latent Dirichlet Allocation) topic model\n",
    "    \"\"\"\n",
    "    def make_lda(fd):\n",
    "        id2word = corpora.Dictionary(fd)\n",
    "        corpus = [id2word.doc2bow(text) for text in fd]\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=25,\n",
    "                                               random_state=100,\n",
    "                                               update_every=5,\n",
    "                                               chunksize=100,\n",
    "                                               passes=20,\n",
    "                                               alpha=\"auto\")\n",
    "        return model, corpus, id2word\n",
    "    \n",
    "    model, corpus, id2word = make_lda(fd)\n",
    "    vis = pyLDAvis.gensim_models.prepare(model, corpus, id2word, mds=\"mmds\", R=25)\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fbfe8699",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews = df_big[df_big['voted_up']==1]['review_stemmed']\n",
    "negative_reviews = df_big[df_big['voted_up']==0]['review_stemmed']\n",
    "\n",
    "pos_fd = FrequencyDistribution(positive_reviews)\n",
    "neg_fd = FrequencyDistribution(negative_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2c6eb733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el101225321843237923524736060\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el101225321843237923524736060_data = {\"mdsDat\": {\"x\": [-0.14765677133427058, -0.508885979705495, -0.33934666830178606, 0.1590988288139705, -0.23285612617927764, 0.4509588585681598, 0.03874348463902987, 0.30428515469744294, -0.3931824849063246, 0.2954103536729893, -0.003692327529809628, -0.26252904661768334, 0.31070724271666733, -0.003376966473405306, 0.13860907459758157, -0.23979893995547122, 0.06754894335730835, 0.31031451373416147, 0.0689020010754981, 0.17576256093024845, -0.09734433069552985, -0.1351625852759246, 0.20435573746711314, -0.035745412618282846, -0.12511911467691023], \"y\": [0.5118442726394011, 0.22688590705231038, 0.40545612807518605, 0.45114307205171417, -0.38247981232880024, 0.08811288028886857, 0.2981611137502816, 0.2511033571148516, -0.10219241711266333, -0.31334617325301106, -0.3944995877724713, -0.202795989610801, -0.1565936552569826, 0.16034338904150736, -0.3096844349181909, 0.013712155906953426, -0.23745039978630644, -0.02070307403294011, 0.05101079920454697, -0.10379888614349231, -0.2501148272119961, 0.10335832339260613, 0.08591683980405834, -0.11633809419488415, -0.0570508866997462], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [46.15032953822804, 24.125326939308852, 14.96755237123151, 2.1551330131835447, 1.2946408342685352, 1.0105599164820118, 0.919348931708166, 0.8541753372256825, 0.8385582283050536, 0.7617407109702519, 0.5961370194621544, 0.5800523329765435, 0.5617541516594634, 0.5389454383664156, 0.501105048392795, 0.4936767572463173, 0.46783806990938354, 0.4639717897737955, 0.45171185418800325, 0.4503347141559579, 0.43203097829006126, 0.4219457482240099, 0.3994179725161674, 0.3457416311370891, 0.21797067279018736]}, \"tinfo\": {\"Term\": [\"game\", \"best\", \"play\", \"fun\", \"good\", \"10\", \"like\", \"great\", \"ever\", \"cool\", \"love\", \"get\", \"awesom\", \"ye\", \"time\", \"recommend\", \"realli\", \"look\", \"new\", \"highli\", \"kill\", \"space\", \"one\", \"friend\", \"still\", \"use\", \"find\", \"differ\", \"map\", \"end\", \"without\", \"system\", \"set\", \"help\", \"point\", \"turn\", \"fight\", \"person\", \"part\", \"pay\", \"open\", \"win\", \"design\", \"skill\", \"mean\", \"let\", \"anoth\", \"option\", \"sometim\", \"item\", \"charact\", \"actual\", \"someth\", \"way\", \"take\", \"around\", \"start\", \"combat\", \"thing\", \"everyth\", \"base\", \"player\", \"level\", \"also\", \"know\", \"go\", \"make\", \"mani\", \"think\", \"world\", \"need\", \"want\", \"give\", \"build\", \"first\", \"tri\", \"feel\", \"game\", \"play\", \"fun\", \"great\", \"friend\", \"amaz\", \"nice\", \"addict\", \"super\", \"simul\", \"relax\", \"favorit\", \"solo\", \"good\", \"sandbox\", \"grindi\", \"puzzl\", \"platform\", \"hang\", \"hr\", \"ps4\", \"platinum\", \"mob\", \"trial\", \"2020\", \"like\", \"love\", \"recommend\", \"realli\", \"multiplay\", \"hour\", \"surviv\", \"enjoy\", \"get\", \"graphic\", \"better\", \"buy\", \"pretti\", \"would\", \"time\", \"one\", \"lot\", \"much\", \"content\", \"far\", \"updat\", \"develop\", \"wait\", \"dev\", \"add\", \"hope\", \"earli\", \"perfect\", \"dlc\", \"access\", \"improv\", \"releas\", \"alreadi\", \"fix\", \"old\", \"ad\", \"type\", \"support\", \"im\", \"version\", \"pleas\", \"futur\", \"purchas\", \"sinc\", \"replay\", \"fan\", \"mode\", \"steam\", \"keep\", \"look\", \"work\", \"team\", \"see\", \"new\", \"come\", \"year\", \"well\", \"still\", \"bit\", \"review\", \"bug\", \"though\", \"w\", \"spare\", \"potato\", \"cup\", \"ark\", \"adult\", \"audienc\", \"\\u2018\", \"boi\", \"usag\", \"nasa\", \"coffe\", \"dri\", \"teen\", \"infin\", \"eargasm\", \"do\", \"leaderboard\", \"ms\", \"mehh\", \"deaf\", \"terrarium\", \"grandma\", \"haunt\", \"torment\", \"don\", \"press\", \"paint\", \"averag\", \"audio\", \"kid\", \"rich\", \"comput\", \"dark\", \"master\", \"brain\", \"short\", \"soul\", \"group\", \"soon\", \"eat\", \"path\", \"sort\", \"p\", \"hear\", \"busi\", \"question\", \"pure\", \"rage\", \"slowli\", \"surprisingli\", \"roll\", \"count\", \"weak\", \"advanc\", \"block\", \"immedi\", \"clearli\", \"legend\", \"answer\", \"respons\", \"odd\", \"enter\", \"best\", \"ever\", \"cost\", \"imagin\", \"brother\", \"pl\", \"ubisoft\", \"remark\", \"pretend\", \"redempt\", \"masterpiec\", \"ps5\", \"farcri\", \"gta\", \"implor\", \"mod\", \"arpg\", \"tactic\", \"moba\", \"f2p\", \"rt\", \"ww2\", \"milsim\", \"dinosaur\", \"wallpap\", \"style\", \"server\", \"endless\", \"ship\", \"depth\", \"choos\", \"pack\", \"20\", \"guy\", \"pro\", \"kinda\", \"con\", \"seri\", \"voic\", \"act\", \"board\", \"lead\", \"faction\", \"stun\", \"cross\", \"murder\", \"yea\", \"motion\", \"narr\", \"captiv\", \"evolut\", \"scaveng\", \"react\", \"z\", \"plagu\", \"sniper\", \"ambush\", \"deepli\", \"throughout\", \"rais\", \"cool\", \"dont\", \"strategi\", \"cant\", \"ive\", \"ass\", \"faster\", \"promis\", \"complic\", \"c\", \"project\", \"girl\", \"size\", \"session\", \"budget\", \"vision\", \"express\", \"95\", \"daughter\", \"unreal\", \"minimum\", \"goofi\", \"unexpect\", \"breaker\", \"entiti\", \"comput\", \"yeah\", \"expans\", \"11\", \"scare\", \"rid\", \"drug\", \"irl\", \"chines\", \"10\", \"q\", \"van\", \"outta\", \"boomer\", \"introvert\", \"craze\", \"8\", \"9\", \"kojima\", \"pizza\", \"ceram\", \"intellectu\", \"oct\", \"20\", \"19\", \"ptsd\", \"12\", \"13\", \"mod\", \"ship\", \"style\", \"kill\", \"space\", \"hate\", \"god\", \"planet\", \"focu\", \"obvious\", \"wife\", \"hey\", \"dr\", \"rise\", \"tl\", \"babi\", \"horribl\", \"bang\", \"glori\", \"somebodi\", \"mouth\", \"winter\", \"daddi\", \"sacrific\", \"dear\", \"gimmick\", \"brrrrrr\", \"mommi\", \"alert\", \"shit\", \"fuck\", \"can\", \"suck\", \"not\", \"prefer\", \"playabl\", \"screen\", \">\", \"cheap\", \"tend\", \"factorio\", \"greatli\", \"visit\", \"american\", \"shitti\", \"nut\", \"nuanc\", \"undeni\", \"infrastructur\", \"woke\", \"144\", \"ambiti\", \"uniron\", \"owner\", \"realist\", \"funni\", \"social\", \"that\", \"men\", \"construct\", \"\\u201d\", \"\\u201c\", \"opposit\", \"linux\", \"foot\", \"void\", \"bush\", \"limb\", \"foe\", \"network\", \"doesn\", \"prison\", \"scam\", \"\\u2013\", \"out\", \"isol\", \"horrifi\", \"bottl\", \"sluggish\", \"\\u2019\", \"physic\", \"it\", \"v\", \"t\", \"don\", \"car\", \"drive\", \"vehicl\", \"listen\", \"com\", \"truck\", \"l\", \"www\", \"h\", \"nation\", \"aswel\", \"shut\", \"grown\", \"bake\", \"euro\", \"disgust\", \"began\", \"rebuild\", \"deliveri\", \"properti\", \"url\", \"playstat\", \"til\", \"drill\", \"exceed\", \"http\", \"v\", \"youtub\", \"sim\", \"object\", \"expens\", \"poor\", \"wheel\", \"thoroughli\", \"sadli\", \"outstand\", \"million\", \"earth\", \"profit\", \"notch\", \"benefit\", \"piss\", \"script\", \"driver\", \"chore\", \"scienc\", \"underwhelm\", \"corrupt\", \"slaughter\", \"fiction\", \"nonsens\", \"10x\", \"zen\", \"trash\", \"apocalyps\", \"physic\", \"guess\", \"wow\", \"trust\", \"mad\", \"self\", \"hurt\", \"neat\", \"hot\", \"rip\", \"sorri\", \"bar\", \"desper\", \"grim\", \"dive\", \"paus\", \"hesit\", \"\\ud83d\\udc4d\", \"ex\", \"slave\", \"nativ\", \"swap\", \"dirti\", \"dang\", \"healthi\", \"snipe\", \"admit\", \"kid\", \"rais\", \"pass\", \"pve\", \"0\", \"skin\", \"histori\", \"thousand\", \"greatest\", \"horror\", \"meh\", \"local\", \"polit\", \"hilari\", \"sport\", \"logist\", \"japanes\", \"ga\", \"forgot\", \"pump\", \"proud\", \"cheaper\", \"length\", \"cinemat\", \"reinstal\", \"mama\", \"overpr\", \"laugh\", \"onlin\", \"vr\", \"e\", \"race\", \"bot\", \"divers\", \"g\", \"xd\", \"countless\", \"customis\", \"fellow\", \"appar\", \"skeleton\", \"newest\", \"solar\", \"accessori\", \"induc\", \"photo\", \"philosophi\", \"batteri\", \"youtu\", \"easter\", \"brainer\", \"circuit\", \"pod\", \"card\", \"op\", \"co\", \"word\", \"epic\", \"biggest\", \"forest\", \"everywher\", \"evil\", \"closest\", \"hill\", \"wholesom\", \"lord\", \"stock\", \"doom\", \"haul\", \"pipe\", \"north\", \"fist\", \"silent\", \"bastard\", \"pocket\", \"netflix\", \"lurk\", \"suspens\", \"awesom\", \"ok\", \"rpg\", \"shooter\", \"intuit\", \"perspect\", \"stabl\", \"3rd\", \"60fp\", \"rx\", \"fsr\", \"tactic\", \"militari\", \"phenomin\", \"fking\", \"thi\", \"looter\", \"flatten\", \"charect\", \"gameit\", \"sailor\", \"\\u2804\\u2804\\u2804\\u2804\\u2804\\u2804\\u28c0\\u28c0\\u28c0\\u28e4\\u28f6\\u28ff\\u28ff\\u28f6\\u28f6\\u28f6\\u28e4\\u28c4\\u28e0\\u28f4\\u28f6\\u28ff\\u28ff\\u28ff\\u28ff\\u28f6\\u28e6\\u28c4\\u2804\\u2804\", \"\\u2804\\u2804\\u283b\\u28e6\\u28c0\\u28c0\\u28c0\\u28c0\\u28c0\\u28c0\\u28e4\\u28e4\\u28e4\\u28e4\\u28f6\\u28fe\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u287f\\u280b\\u2804\", \"\\u2804\\u2804\\u28e0\\u28f4\\u28fe\\u28ff\\u283f\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28e6\", \"\\u2804\\u2804\\u28ff\\u2801\\u2804\\u2810\\u281b\\u281b\\u281b\\u281b\\u2809\\u2809\\u2809\\u2809\\u2804\\u2804\\u28e0\\u28fe\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u28ff\\u287f\", \"squad\", \"diablo\", \"teach\", \"coop\", \"overwatch\", \"wallpap\", \"modern\", \"recomend\", \"involv\", \"overwhelm\", \"style\", \"choos\", \"pvp\", \"mod\", \"server\", \"multipl\", \"ship\", \"room\", \"depth\", \"ton\", \"download\", \"longer\", \"stress\", \"ball\", \"peac\", \"virtual\", \"lover\", \"calm\", \"girlfriend\", \"whilst\", \"divis\", \"mile\", \"dump\", \"yell\", \"covid\", \"daunt\", \"dc\", \"excus\", \"shower\", \"justic\", \"5000\", \"oddli\", \"harm\", \"va\", \"artist\", \"highli\", \"man\", \"combin\", \"aw\", \"id\", \"flesh\", \"prime\", \"lone\", \"@\", \"prove\", \"amazon\", \"assist\", \"underr\", \"steamcommun\", \"pov\", \"67\", \"fragil\", \"bb\", \"filedetail\", \"sharedfil\", \"unpredict\", \"unforgett\", \"pride\", \"6th\", \"witti\", \"sharp\", \"licens\", \"mmo\", \"creativ\", \"happi\", \"insan\", \"rock\", \"stuck\", \"downsid\", \"joke\", \"sold\", \"ez\", \"attract\", \"ear\", \"everybodi\", \"cure\", \"sh\", \"thorough\", \"herb\", \"simp\", \"85\", \"soni\", \"stain\", \"kilomet\", \"bloodborn\", \"inconveni\", \"don\\u00b4t\", \"ds\", \"na\", \"lol\", \"gon\", \"alright\", \"app\", \"wont\", \"haha\", \"copi\", \"banger\", \"awhil\", \"2021\", \"omg\", \"nostalg\", \"enthusiast\", \"truth\", \"2014\", \"jerk\", \"east\", \"usd\", \"prologu\", \"inclus\", \"j\", \"shred\", \"san\", \"peep\", \"cri\", \"alert\", \"lie\", \"crash\", \"train\", \"scratch\", \"glitch\", \"itch\", \"killer\", \"fi\", \"sci\", \"ps\", \"heck\", \"dad\", \"data\", \"nonetheless\", \"hello\", \"gen\", \"wich\", \"resolut\", \"absorb\", \"geforc\", \"nvidia\", \"1080\", \"panel\", \"primal\", \"reluct\", \"exot\", \"depress\", \"ye\", \"digit\", \"350\", \"mach\", \"daryl\", \"dixon\", \"reconsid\", \"macbook\", \"squid\", \"\\u035c\\u0296\", \"\\u0361\\u00b0\", \"lewd\", \"\\u2591\\u2591\\u2591\\u2591\", \"ja\", \"fir3e4\", \"stuf\", \"funi\", \"\\u2580\\u2588\\u258c\\u2591\\u2591\\u2591\\u2584\\u2591\\u2580\\u2588\\u2580\\u2591\\u2580\", \"\\u2584\\u2591\\u2590\\u2591\\u2591\\u2591\\u2584\\u2584\\u2591\\u2588\\u2591\\u2580\\u2580\", \"goob\", \"\\u2584\\u2590\", \"\\u258c\\u2591\\u2591\\u2591\\u2580\\u2588\\u2584\", \"\\u2590\", \"\\u2591\\u2591\\u2591\\u2588\\u2580\\u2584\\u2584\\u2584\\u2588\\u2591\\u2580\\u2580\", \"\\u2591\\u2591\\u2591\\u258c\\u2591\\u2584\\u2584\\u2584\\u2590\\u258c\\u2580\\u2580\\u2580\", \"wan\", \"style\", \"mod\", \"700\", \"beta\", \"air\", \"multipl\", \"log\", \"cuz\", \"okay\", \"toxic\", \"choos\", \"broke\", \"ship\", \"clean\", \"specif\"], \"Freq\": [41602.0, 5303.0, 16413.0, 15550.0, 16229.0, 4419.0, 10731.0, 9444.0, 2825.0, 2024.0, 6980.0, 8718.0, 1534.0, 1320.0, 8087.0, 5334.0, 6191.0, 4020.0, 4056.0, 1146.0, 1260.0, 1243.0, 7001.0, 4024.0, 5018.0, 3336.389875617683, 2593.535371917339, 2583.818248772554, 2050.2009046440003, 2029.5989141350246, 2016.2100860401156, 1911.6690176658337, 1820.4512997099841, 1791.3212842566056, 1627.8066552686603, 1590.3586153467872, 1536.1548970033348, 1454.7315776482928, 1436.9269586473306, 1420.2877243098642, 1396.0030569157875, 1385.0512961415745, 1339.0350008470489, 1321.1188928293825, 1320.324666310385, 1308.0795149646085, 1271.18467292562, 1254.7757312448803, 1250.8063683168255, 1248.2158118842433, 2307.6065104549907, 1899.3086020138985, 2131.80803340436, 3815.809636248148, 2942.359611869838, 2201.0116846174433, 3084.576417311219, 1780.2733715790976, 4123.60521018674, 1906.1383365927388, 2399.9729782051772, 3154.0752489685287, 2055.2996690079376, 3333.728296552237, 2263.309151640232, 3703.4553703921197, 4159.85506761798, 2591.3419287442557, 2465.594351637091, 2485.771622082432, 2962.5649610304963, 3039.410034055145, 2382.827973058599, 2608.625675778288, 2466.845624135678, 2397.8422992192754, 2414.387033342583, 41600.85781852709, 16412.16514648897, 15549.466197620533, 9443.22904113682, 4023.1795162909675, 3745.787602567139, 3354.287604263167, 2115.7783984571815, 1146.362441993016, 1029.724016576467, 1028.7439628026705, 986.6421083530731, 534.7757684682806, 16184.741851994588, 449.7367843639125, 360.0548024444538, 330.9471251871346, 327.79610212937814, 262.76526951465354, 214.02435404507477, 137.34159112747247, 132.02141815435004, 102.68752245626025, 101.20405093434633, 90.26854327048345, 10531.58199479712, 6868.013077482872, 5195.056413404941, 5792.063900487724, 974.4761447488834, 4386.720938206878, 1852.6657528881005, 3810.2985891520775, 7446.108505561997, 2347.9384653411325, 3285.231279868852, 2307.2536831689577, 2613.858119119928, 4004.8528622599906, 6286.838414313563, 5131.817874712581, 3758.5910235878514, 2940.3786588443886, 2487.9860147447757, 2471.561955995226, 2257.0646149949166, 1813.0880740713928, 1771.6648136361564, 1506.496563052198, 1502.0443086791663, 1432.9651868330081, 1420.9156325348158, 1348.067169997732, 1295.912430708117, 1254.6654850558893, 1251.1714066368752, 1226.07356356434, 1193.7846179920834, 1166.3026225477402, 1127.5124519795932, 1089.68833949335, 1073.4647010635485, 1007.4133111573868, 953.6923241838111, 952.4115166650516, 911.656630064364, 861.6531760091023, 815.111965759256, 2011.055598062552, 942.3249948381039, 1130.853632422717, 1514.6541827869037, 1382.249586349499, 2508.883142807018, 3694.4766994150145, 2748.717163007367, 1335.747786739084, 2825.2699195489367, 3526.44442577866, 2622.0928447035576, 2073.33680549714, 2753.2113533617744, 3093.9384471184994, 1873.5233330703284, 1507.153458059728, 1456.8812464011992, 1471.7314814409558, 390.9533216800451, 289.2504704257475, 286.5873217361366, 266.56827654904714, 254.7584339719772, 251.91764213369936, 241.8475168036661, 235.56158060148803, 233.97456021304373, 230.47460893799914, 225.91168566494565, 221.28458960329047, 220.67887076142725, 219.50888534945895, 213.795567876342, 206.6098463174996, 201.11410368705586, 198.83610191584864, 189.4699379728662, 188.5638719689658, 186.4239783248763, 185.21771942044975, 181.6385989341924, 20.36731947234461, 10.354548434390612, 207.46491787331476, 216.76948739866612, 225.6075480772081, 236.41537399049724, 220.3988640814915, 241.0486625499536, 213.3894854671514, 229.96336425298188, 224.51828551024252, 220.9157723677006, 218.95925158554604, 222.65994116119938, 218.30401479863198, 576.9980742048958, 509.23918622057454, 478.468060321831, 427.2663052226182, 422.37636627956124, 347.6948129573211, 342.1878597137736, 328.1819739951031, 311.00819748407525, 276.1063271175694, 276.0330129827402, 263.97139482019406, 263.8005403196334, 262.0657568497381, 244.66388309365678, 238.37909573923508, 207.4909558213953, 205.0612634375506, 199.92716909748137, 185.33634566710523, 184.5634362326768, 171.88762354405662, 164.76369515236246, 158.21509341027823, 150.28603337321772, 5301.942657092248, 2824.217202343104, 337.75714832362894, 277.4545504721533, 102.30474959109345, 82.7817778532521, 41.69825377808933, 41.09890807632654, 37.713477138107784, 22.449956858004047, 273.25323788027475, 12.059411302201214, 10.350866199652534, 47.97928699077177, 5.100170652319171, 0.05559855501671742, 0.0555984944688539, 0.055598504560164484, 0.0555984894231986, 0.0555984944688539, 0.0555984944688539, 0.0555984894231986, 0.055598484377543306, 0.0555984894231986, 0.0555984894231986, 0.05559851465147507, 0.055598504560164484, 0.0555984944688539, 0.05559849951450919, 0.0555984944688539, 0.0555984944688539, 0.0555984944688539, 0.0555984944688539, 780.93276504673, 703.648681231425, 666.6333575912761, 662.0276315894183, 640.8469775981182, 555.5812573347774, 493.9264540056513, 391.4988936234369, 325.8027520519759, 256.01559829385803, 253.76908382098864, 210.3312312872937, 169.51784388080648, 133.93242289530593, 106.1728667345603, 83.39031264627337, 80.42338496840169, 77.72563103159236, 64.10705479508562, 58.507356363623025, 56.57685052784409, 54.21143915663761, 51.11229361542044, 46.38681053999744, 45.870122552141865, 186.27020298779888, 46.956603772265446, 2022.7533567759765, 984.2118227686539, 903.982135069473, 628.4977150874485, 410.0440634827432, 261.39887394435436, 248.01337165061642, 203.995304410693, 184.24397038531336, 179.96520324074936, 177.67887232170995, 161.72715437406004, 153.37383008060425, 120.84617366517168, 74.29053083018188, 65.19316767952613, 61.632559399981744, 48.40111424498056, 48.038427510152005, 45.26819401710786, 44.622417026944355, 35.80238161902113, 31.766316918843067, 23.411033938837683, 20.8945060356905, 346.49090820002573, 454.4382953151832, 430.00103165905625, 304.14464097940424, 132.47848193534156, 84.42013697615353, 82.74226636312753, 63.56745600889962, 57.99082159119963, 4241.413525584014, 32.47793531972235, 20.70056587618505, 14.32273016959522, 12.172114107147559, 9.22120674532752, 8.780884361751637, 812.9320634402023, 670.6501583214474, 4.445577470366955, 4.2124280271359495, 0.6196041854045933, 0.5772105578825986, 0.05850544193258465, 0.058505454493173174, 0.058505429371996126, 0.058505429371996126, 0.05850543774572181, 0.05850543355885897, 0.058505483801213065, 0.05850545030631033, 0.05850545030631033, 1258.5671870601334, 1242.1979113956072, 806.4163273468913, 677.312024908016, 558.8569242082136, 235.6869393350905, 230.62749827350964, 170.27048335652086, 159.95380239183328, 146.68736850566123, 142.00704318853766, 129.6281024977905, 126.16363950904483, 123.80045198730473, 84.18457031462192, 54.55024334620175, 49.09067907873195, 35.01743248861036, 30.80043775507414, 28.170855837823023, 26.778114500896667, 22.31173113387044, 20.869148679466967, 16.680296310837143, 15.775497469347814, 22.678014178027908, 631.2784707227714, 541.1685707583272, 491.76572202063426, 473.85734845390965, 434.72574760647836, 301.59653306681895, 293.8655806544614, 292.1687992362089, 229.489955034373, 180.62821932240092, 151.65175334123688, 133.67186370996478, 100.33622781723143, 76.64270936183107, 69.7314790554999, 62.39335294995895, 61.27667336997806, 37.97780082122962, 25.141691331334872, 21.594428695008695, 20.39373206434227, 18.971326185070097, 16.756696406997367, 14.43505617596555, 14.243022863366754, 911.2556806857538, 542.3913509439371, 349.0210162838658, 342.30712179671553, 120.83075274565597, 94.38207641482096, 73.62381855364427, 70.07438161295599, 59.152885246576666, 56.89961163534161, 52.56164488836992, 52.050612708790965, 42.90386743511223, 37.69360260181125, 36.97632002203151, 36.72555789927855, 34.47531816719406, 31.15300993483951, 31.113332252380612, 29.920620346592358, 24.28182823886276, 22.52034851390292, 21.511800190159185, 21.006680071270996, 20.66208863717371, 321.4851613703832, 351.03268872317494, 147.18692393625196, 138.74600189823, 165.4195233724467, 66.94590949133567, 730.3674705271479, 455.6138241955434, 409.0021623331824, 320.8159760773127, 257.13676203295404, 165.2026730859377, 137.30221711833815, 104.73228880923357, 99.16951991296749, 89.46596028825864, 77.08969128563335, 60.876156619856545, 57.927775943522384, 56.150413810153296, 45.40193008742148, 43.023365448686896, 41.53148288491238, 40.58362477010841, 39.0760719493673, 38.49369041062608, 35.62727413922456, 34.670282700162204, 32.04126105661504, 31.790149209685826, 31.078769518195827, 221.8088570092717, 62.978481281493174, 72.38031952884607, 847.2323682370603, 369.5097557971031, 215.79221303123387, 195.1041245309214, 176.4813487034407, 160.04717759845678, 124.19889542571366, 122.79456084392129, 101.18971780839566, 94.63775829014789, 91.8439798434501, 84.59254240138269, 83.79016312089638, 83.2943496431443, 77.52591628085645, 72.08595493878087, 63.245757363247456, 53.04740185906595, 51.60097427372709, 46.61374693719582, 44.549219581594436, 39.45386921768947, 39.205262582053756, 35.386647905254904, 32.937477681228685, 138.10527089666903, 70.57263485841095, 82.54104028980882, 490.6483768096379, 354.1267650551389, 194.86428633964752, 194.576744566767, 177.71950821246867, 153.01591091132912, 150.882969533126, 142.5784253542755, 124.4549926849697, 116.69794411073713, 99.70486891548323, 92.93200886859951, 83.46857644744209, 74.03206583957703, 72.72501026701467, 70.39851263161593, 65.14463929505098, 63.25124774010974, 62.416502434053584, 60.79475269260075, 58.963377640567295, 50.33874365723502, 44.392963322063075, 40.09960022978188, 28.498847920817536, 82.3046256334692, 310.3866172749627, 57.53541473902765, 381.6022022959743, 308.4207323872266, 294.0699469527614, 272.63889523968595, 211.1906687915949, 188.14765707104323, 179.21889690637005, 174.1477753195283, 166.5703416436479, 118.50623953196484, 99.42762365542934, 93.17325719199869, 85.86013205323289, 80.96561327517718, 80.8377145568087, 77.7417487893494, 63.79644207115789, 61.335159054105574, 52.811579767673884, 52.65941482183474, 48.45707667870673, 47.72696460354297, 29.06263638169105, 28.088875127832427, 19.889261588769493, 149.3709827882297, 715.6745720136398, 548.5950572331943, 435.8402286119131, 347.7857628518093, 243.48660969181648, 193.26506443372045, 150.05468046221162, 138.95997308058477, 91.35218167762814, 75.67952770812883, 57.952476010174735, 51.682846096457965, 46.93560248940876, 44.23149533801368, 37.932968347681665, 36.19091307113454, 31.886334598081017, 28.73383121463793, 25.99711229744761, 20.670970380455213, 19.81398225804898, 19.803975559926315, 18.202097775139833, 17.522395765188644, 15.702199262964392, 610.3879446940372, 515.7465950411439, 416.7602526247648, 348.9092421822088, 315.8445904076072, 235.22661969027655, 212.30036659926867, 95.48461103693863, 74.92680561429374, 61.73522096046287, 54.176218133140345, 49.53135850919205, 48.72086163014395, 48.649615674229636, 44.322708386134664, 36.323752173725524, 35.96032995430806, 28.420482454081764, 25.35290685979932, 22.589937735086192, 19.93086995115507, 18.994786477745993, 18.113312884842458, 15.79511622224332, 13.953059314661383, 1533.0210922654444, 703.5584528163844, 418.58085924997414, 345.4305180957379, 110.54283263705837, 102.79004397944092, 92.79494852571572, 53.53818218207492, 15.978114924851678, 12.71326437694573, 1.2791514096063323, 0.05201257024010296, 0.05201255219717629, 0.05201254768644462, 0.05201254768644462, 0.05201254768644462, 0.05201254768644462, 0.05201254768644462, 0.05201254768644462, 0.05201254768644462, 0.05201254768644462, 0.05201254768644462, 0.05201254768644462, 0.05201254768644462, 0.05201254768644462, 0.052012556707907956, 0.052012556707907956, 0.05201255219717629, 0.05201255219717629, 0.05201255219717629, 0.05201255219717629, 0.05201255219717629, 0.05201255219717629, 0.05201255219717629, 0.05201255219717629, 0.05201256572937129, 0.052012556707907956, 0.052012556707907956, 0.05201257024010296, 0.052012556707907956, 0.05201255219717629, 0.05201255219717629, 0.05201255219717629, 0.05201255219717629, 773.9900848023048, 463.66004340112556, 282.32608999250556, 263.06035532624117, 231.2189320886003, 136.61659243696113, 130.08370261676774, 126.57766438235218, 90.1908385042731, 84.30306856063409, 81.59462789169942, 81.2539384335713, 61.17400090627552, 54.538371000383904, 39.22596762954578, 35.2559596625066, 34.5116984352714, 32.113672261736916, 29.8636892514453, 27.15747505516695, 25.3272040139005, 24.90661941495109, 20.80933641637532, 20.111382136465714, 19.22155863976963, 26.255493452098914, 1144.8738162773739, 607.1613852483003, 426.75225237385814, 120.06485336550689, 87.89220540823662, 84.5585242750793, 72.45351799042975, 69.61517618007043, 53.5976629509729, 50.912391483495455, 49.31234717609811, 34.38166032968805, 29.455906704589236, 17.922767183130144, 17.775480768974017, 15.898426366622582, 13.613880545764484, 13.362251483089592, 12.322164810860997, 12.322164810860997, 10.887643356634365, 8.283157480628594, 7.744257012747399, 5.376812263083905, 4.799667402627794, 29.947024874897334, 9.100166404570956, 490.5030679320857, 481.9270279619975, 439.9947663456254, 307.30629829538634, 243.33658689839572, 227.39296166250764, 186.3831820500903, 146.5341547554965, 60.650766946240786, 57.4829160077568, 41.934595823235085, 39.84324139551194, 35.10617998317654, 33.23242239023874, 31.768074325996672, 26.20017549542523, 13.42001107698026, 10.208891019225073, 9.854630902956377, 9.843385936899582, 9.719682681044286, 8.240451890742877, 8.10041781607704, 7.265875221974799, 6.057727934024717, 12.678411767794067, 569.3263471115685, 354.9203667907579, 292.0826675346186, 240.86779987626153, 173.33562292585685, 145.39672365016162, 145.24070503195037, 118.9340790747923, 84.84548949667908, 83.6075756990844, 83.51649676751084, 49.27739006539399, 48.259478856505254, 44.5928452842042, 41.31422838169169, 26.782605270412983, 22.117648585897047, 20.791623069454737, 20.773584948320448, 18.913424382897844, 16.61582151134422, 11.757444624945073, 9.69438940241425, 9.516598631087058, 9.428269425553031, 133.88910894294855, 25.3056889204496, 15.285088888684932, 417.05433220503454, 260.8552141114702, 229.48643366386065, 208.41837512930726, 160.95339534111642, 109.55626460500308, 79.70722744641903, 74.85437649826116, 73.14189797280268, 72.37442170774611, 67.47278239905602, 56.77159643074128, 56.59676793677898, 48.478964547919226, 48.00260630892546, 35.8929498674832, 33.688377089065085, 25.498647989998936, 20.69104117545784, 10.927420777952317, 9.460180658866994, 5.371514971047559, 5.208918921102761, 5.186697678959259, 3.662653368929431, 74.7054093707694, 1319.170250992115, 34.17744618932916, 15.174028811474411, 4.372830411061595, 2.448872412798865, 1.9163910057403004, 0.9284594650511769, 0.8487345400968467, 0.4251126508517995, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609685689801, 0.03359609685689801, 0.03359609685689801, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323, 0.03359609468027323], \"Total\": [41602.0, 5303.0, 16413.0, 15550.0, 16229.0, 4419.0, 10731.0, 9444.0, 2825.0, 2024.0, 6980.0, 8718.0, 1534.0, 1320.0, 8087.0, 5334.0, 6191.0, 4020.0, 4056.0, 1146.0, 1260.0, 1243.0, 7001.0, 4024.0, 5018.0, 3337.8196653733753, 2594.965161673031, 2585.2532033845705, 2051.6306943996924, 2031.0287038907172, 2017.6398757958082, 1913.0988242548972, 1821.8810894656767, 1792.751074105713, 1629.2364450243529, 1591.7884051024798, 1537.5846867590274, 1456.1636580870966, 1438.3567484123646, 1421.7175140655568, 1397.43284667148, 1386.4810858972671, 1340.4647906027415, 1322.548682585075, 1321.7544560660776, 1309.509304720301, 1272.6144626813125, 1256.2055210005728, 1252.236158072518, 1249.6456016399359, 2314.87114111549, 1914.401221467922, 2156.56574335367, 3922.1699678918144, 3010.90638709506, 2244.193438044859, 3196.3037121671764, 1835.2476069834738, 4734.015960543241, 1986.5645045240628, 2600.373243131381, 3774.8332456621383, 2234.1945108158993, 4198.2954165347155, 2539.161291072844, 4960.070100888157, 6135.416938538307, 3259.958314198326, 3021.9935895271333, 3073.7766086340057, 4160.007219174793, 4355.876020739446, 2870.884978256754, 3674.1249034583784, 3321.890953875651, 3252.298896404099, 3962.1622401157065, 41602.30308840202, 16413.610416363896, 15550.91146749545, 9444.674311011737, 4024.6247861658894, 3747.232872442061, 3355.732874138089, 2117.2236683321034, 1147.8077118679387, 1031.1692864513896, 1030.189232677593, 988.0873782279958, 536.2210383432033, 16229.461622554953, 451.1820542388346, 361.50007231937593, 332.3923950620567, 329.2413720043003, 264.2105393895757, 215.4696239199971, 138.7868610023948, 133.46668802927238, 104.13279233118261, 102.64932080926869, 91.71381314540581, 10731.91964020973, 6980.745530706285, 5334.174251320923, 6191.6349672871, 1017.8025241438095, 4843.263242477612, 2020.4672075703672, 4306.802881132803, 8718.667301794367, 2591.4353128499965, 3729.4924562424694, 2612.9943756678376, 3026.9518574090066, 4865.209505242653, 8087.5307306513905, 7001.514992176173, 5016.668551695761, 5027.577979045103, 2489.418631753176, 2472.9945730036266, 2258.497232003317, 1814.5206910797933, 1773.0974306445569, 1507.9291800605986, 1503.4769256875668, 1434.3978038414086, 1422.3482495432163, 1349.4997870061325, 1297.3450477165175, 1256.0981020642898, 1252.6040236452757, 1227.5061805727405, 1195.217235000484, 1167.7352395561406, 1128.9450689951946, 1091.131707480389, 1074.897318071949, 1008.8459281657874, 955.1249411922117, 953.8441336734522, 913.0892470727646, 863.0857930175029, 816.5445827676566, 2025.643025473479, 944.0964573389099, 1135.5967225120019, 1530.106356093994, 1399.4857923551344, 2606.6247307430126, 4020.5924763856356, 2982.2535309461555, 1364.6155580402333, 3130.6901140162627, 4056.271654833049, 3023.2554710324393, 2355.0469947245138, 4097.444040122697, 5018.616441803135, 2535.9354923708643, 1736.2364591646112, 1665.0074610087786, 2130.3877117236357, 392.4098283935964, 290.7069771392988, 288.04382844968785, 268.0247832625984, 256.21494068552863, 253.37414884725075, 243.3040235172175, 237.01808731503942, 235.43106692659512, 231.93111565155053, 227.36819237849704, 222.74109631684186, 222.13537747497864, 220.96539206301034, 215.2520745898934, 208.066353031051, 202.57061040060725, 200.29260862940004, 190.9264446864176, 190.0203786825172, 187.88048503842768, 186.67422613400115, 183.0951056477438, 21.82382618589603, 11.811055147942024, 275.8076066039453, 318.8546925259164, 351.95930363332064, 454.9162969447944, 371.80285056890546, 552.8306835744635, 350.97013163899857, 577.8518145146335, 645.8754357386218, 659.4602362866962, 612.0013474770952, 856.5254062365938, 672.1183499153252, 578.4283852131671, 510.66949722884533, 479.8983713301018, 428.696616230889, 423.80667728783203, 349.1251239655919, 343.6181707220444, 329.61228500337387, 312.43850849234605, 277.53663812584017, 277.463323991011, 265.40170582846486, 265.2308513279042, 263.49606785800887, 246.09419410192768, 239.809406747506, 208.9212668296662, 206.4915744458215, 201.35748010575227, 186.76665667537614, 185.9937472409477, 173.31835346567155, 166.19400616063336, 159.64540441854913, 151.71634438148863, 5303.39878163394, 2825.6733268847975, 339.21327286532255, 278.9106750138469, 103.76087413278718, 84.23790239494582, 43.15437831978304, 42.55503261802025, 39.169601679801495, 23.90608139969776, 293.4435025578753, 13.515535843894922, 11.806990741346242, 60.38129291133248, 6.556295194012882, 1.5117247707347208, 1.5117232665795397, 1.511723589132938, 1.5117231786735141, 1.5117233249795083, 1.5117233342367915, 1.5117231991989406, 1.511723063670401, 1.511723215460982, 1.5117232470688025, 1.511724093518524, 1.5117238760220748, 1.5117234592082684, 1.5117238568983593, 1.511723567434926, 1.5117235814880092, 1.5117237105952332, 1.5117236071829656, 782.3696037251899, 705.085519909885, 668.070196269736, 663.4644702678783, 642.2838162765781, 557.0180960132374, 495.36329268411095, 392.93573230189656, 327.23959073043557, 257.4524369723177, 255.20592249944838, 211.76806996575343, 170.95468255926622, 135.36926157376567, 107.60970541302008, 84.82715132473315, 81.86022364686147, 79.16246971005214, 65.5438934735454, 59.94419504208279, 58.01368920630385, 55.648277835097375, 52.549132293880206, 47.823649218457206, 47.30696123060163, 226.88436591002315, 105.8677542257485, 2024.206115140031, 985.6645811327082, 905.4348934335273, 629.9504734515028, 411.4968218467973, 262.85163230840845, 249.4661300146706, 205.44806277474717, 185.69672874936754, 181.41796160480354, 179.13163068576412, 163.1799127381142, 154.82658844465843, 122.2989320292259, 75.7432891942361, 66.64592604358035, 63.085317764035956, 49.853872609034774, 49.49118587420622, 46.72095238116207, 46.07517539099857, 37.255139983075345, 33.21907528289728, 24.86379230289189, 22.34726439974471, 577.8518145146335, 455.89151291435167, 431.4542492582247, 305.5978585785727, 133.93169953451005, 85.87335457532207, 84.19548396229607, 65.02067360806817, 59.44403919036817, 4419.788915921987, 33.931152918890895, 22.15378347535359, 15.775947768763752, 13.625331706316091, 10.674424344496051, 10.234101960920169, 1027.4348275293914, 849.6410190325198, 5.898795069535491, 5.665645626304485, 2.072821784573129, 2.0304281570511344, 1.5117230651719114, 1.5117236071829656, 1.5117230558464863, 1.5117230573131195, 1.5117232862212207, 1.5117231731223673, 1.5117247707347208, 1.5117238568983593, 1.511724093518524, 1260.0172007912743, 1243.647925126748, 807.8663410780321, 678.7620386391568, 560.3069379393544, 237.13695306623106, 232.0775120046502, 171.7204970876614, 161.40381612297384, 148.13738223680178, 143.45705691967822, 131.07811622893107, 127.61365324018544, 125.25046571844534, 85.63458404576254, 56.00025707734237, 50.54069280987257, 36.467446219750975, 32.25045148621474, 29.620869568963634, 28.228128232037278, 23.76174486501105, 22.31916241060758, 18.130310041977754, 17.225511200488423, 49.38362500867702, 632.7308566955143, 542.62095673107, 493.21810799337686, 475.30973442665226, 436.17813357922097, 303.04891903956155, 295.317966627204, 293.6211852089515, 230.94234100711566, 182.08060529514358, 153.10413931397954, 135.12424968270744, 101.78861378997414, 78.09509533457377, 71.18386502824261, 63.84573892270165, 62.72905934272076, 39.43018679397232, 26.594077304077572, 23.046814667751395, 21.84611803708497, 20.423712157812798, 18.209082379740067, 15.887442148708244, 15.695408836109449, 912.7076762274774, 543.8433464856607, 350.4730118255892, 343.75911733843895, 122.28274828737943, 95.83407195654442, 75.07581409536772, 71.52637715467944, 60.604880788300115, 58.351607177065056, 54.01364043009337, 53.50260825051441, 44.35586297683568, 39.1455981435347, 38.42831556375496, 38.177553441002, 35.92731370891751, 32.60500547656296, 32.565327794104064, 31.37261588831581, 25.733823780586214, 23.972344055626372, 22.963795731882637, 22.458675612994448, 22.114084178897162, 349.491390147064, 434.96092731532383, 177.67406621925278, 203.11244739585217, 370.73047309815496, 275.8076066039453, 731.8151622175335, 457.0615158859289, 410.4498540235679, 322.2636677676982, 258.58445372333955, 166.65036477632322, 138.74990880872366, 106.17998049961912, 100.61721160335304, 90.9136519786442, 78.5373829760189, 62.3238483102421, 59.37546763390794, 57.59810550053885, 46.84962177780704, 44.47105713907245, 42.97917457529793, 42.03131646049396, 40.52376363975285, 39.941382101011634, 37.074965829610115, 36.11797439054776, 33.488952747000596, 33.237840900071376, 32.526461208581374, 238.21705422520245, 203.11244739585217, 417.5356318325843, 848.679294013657, 370.95668157369965, 217.23913880783044, 196.55105030751798, 177.92827448003726, 161.49410337505336, 125.64582120231027, 124.2414866205179, 102.63664358499227, 96.0846840667445, 93.29090562004671, 86.0394681779793, 85.237088897493, 84.74127541974092, 78.97284205745306, 73.53288071537749, 64.69268313984408, 54.49432763566257, 53.04790005032371, 48.06067271379244, 45.99614535819106, 40.900794994286095, 40.65218835865038, 36.833573681851526, 34.38440345782531, 183.42886004850868, 97.70553373408295, 434.96092731532383, 492.09899686161344, 355.5773851071144, 196.31490639162305, 196.02736461874252, 179.1701282644442, 154.46653096330465, 152.33358958510152, 144.02904540625104, 125.90561273694527, 118.1485641627127, 101.15548896745881, 94.38262892057509, 84.91919649941767, 75.4826858915526, 74.17563031899024, 71.8491326835915, 66.59525934702656, 64.70186779208532, 63.86712248602916, 62.24537274457633, 60.41399769254287, 51.7893637092106, 45.84358337403865, 41.550220281757454, 29.949467972793116, 89.64221559573242, 552.8306835744635, 105.8677542257485, 383.0511230600573, 309.86965315130965, 295.51886771684445, 274.087816003769, 212.63958955567793, 189.59657783512625, 180.66781767045308, 175.59669608361133, 168.01926240773093, 119.95516029604794, 100.87654441951244, 94.62217795608179, 87.30905281731599, 82.41453403926027, 82.2866353208918, 79.1906695534325, 65.24536283524098, 62.78407981818866, 54.26050053175697, 54.10833558591782, 49.90599744278981, 49.17588536762605, 30.511557145774137, 29.537795891915515, 21.33818235285258, 240.03773797440132, 717.1323960171956, 550.0528812367501, 437.29805261546875, 349.2435868553649, 244.94443369537208, 194.72288843727605, 151.51250446576722, 140.41779708414037, 92.8100056811838, 77.13735171168449, 59.41030001373038, 53.14067010001361, 48.3934264929644, 45.68931934156932, 39.39079235123731, 37.64873707469018, 33.344158601636664, 30.19165521819358, 27.45493630100326, 22.128794384010863, 21.27180626160463, 21.261799563481965, 19.659921778695484, 18.980219768744295, 17.160023266520042, 611.8453682418096, 517.2040185889164, 418.21767617253715, 350.36666572998115, 317.30201395537955, 236.6840432380489, 213.75779014704102, 96.94203458471104, 76.38422916206615, 63.192644508235254, 55.63364168091273, 50.98878205696444, 50.17828517791634, 50.10703922200202, 45.78013193390705, 37.78117572149791, 37.417753502080444, 29.877906001854157, 26.810330407571712, 24.047361282858585, 21.388293498927464, 20.452210025518387, 19.57073643261485, 17.252539770015712, 15.410482862433772, 1534.4808027427596, 705.0181632936994, 420.0405697272891, 346.89022857305287, 112.00254311437334, 104.2497544567559, 94.2546590030307, 54.997892659389876, 17.43782540216664, 14.172974854260692, 2.7388618869212946, 1.511723589132938, 1.511723142834359, 1.5117230250014064, 1.5117230250014064, 1.5117230250014064, 1.5117230250014064, 1.5117230250014064, 1.5117230250014064, 1.5117230250014064, 1.5117230250014064, 1.5117230250014064, 1.5117230250014064, 1.5117230250014064, 1.5117230250014064, 1.511723287840365, 1.511723291267357, 1.5117231690610429, 1.5117231978904921, 1.5117232370337153, 1.5117232470688025, 1.5117232507870881, 1.5117232640665186, 1.5117232715181768, 1.5117232818444206, 1.511724093518524, 1.5117235814880092, 1.5117235888368152, 1.5117247707347208, 1.5117238760220748, 1.5117235655312926, 1.5117238568983593, 1.5117234244323725, 1.511723567434926, 775.4457398661458, 465.11569846496644, 283.78174505634644, 264.51601039008204, 232.6745871524412, 138.07224750080204, 131.53935768060865, 128.03331944619313, 91.64649356811404, 85.75872362447502, 83.05028295554035, 82.70959349741224, 62.629655970116445, 55.994026064224826, 40.6816226933867, 36.71161472634752, 35.96735349911232, 33.56932732557784, 31.31934431528623, 28.613130119007877, 26.782859077741428, 26.362274478792017, 22.264991480216246, 21.567037200306643, 20.677213703610555, 45.14415657746026, 1146.3281358957272, 608.6157048666535, 428.2065719922112, 121.51917298385999, 89.34652502658972, 86.01284389343239, 73.90783760878284, 71.06949579842353, 55.051982569325986, 52.366711101848544, 50.7666667944512, 35.83597994804114, 30.91022632294233, 19.377086801483237, 19.22980038732711, 17.352745984975673, 15.068200164117576, 14.816571101442683, 13.776484429214088, 13.776484429214088, 12.341962974987457, 9.737477098981685, 9.19857663110049, 6.831131881436996, 6.253987020980885, 45.51047911994497, 22.286017824637312, 491.95893050895165, 483.3828905388635, 441.4506289224914, 308.7621608722523, 244.7924494752617, 228.8488242393736, 187.83904462695628, 147.99001733236247, 62.10662952310676, 58.938778584622774, 43.39045840010106, 41.299103972377914, 36.562042560042514, 34.688284967104714, 33.22393690286265, 27.65603807229121, 14.875873653846236, 11.66475359609105, 11.310493479822354, 11.299248513765558, 11.175545257910263, 9.696314467608854, 9.556280392943018, 8.721737798840776, 7.513590510890694, 25.096346669107632, 570.7879783066073, 356.3819979857967, 293.54429872965744, 242.32943107130035, 174.79725412089567, 146.85835484520044, 146.7023362269892, 120.39571026983114, 86.30712069171793, 85.06920689412325, 84.97812796254969, 50.739021260432835, 49.7211100515441, 46.05447647924305, 42.77585957673054, 28.244236465451827, 23.57927978093589, 22.25325426449358, 22.235216143359292, 20.37505557793669, 18.077452706383063, 13.219075819983916, 11.156020597453093, 10.9782298261259, 10.889900620591874, 192.4220781325618, 49.38362500867702, 172.28167249196804, 418.5131441072972, 262.31402601373287, 230.94524556612328, 209.8771870315699, 162.41220724337904, 111.0150765072657, 81.16603934868165, 76.31318840052379, 74.6007098750653, 73.83323361000873, 68.93159430131864, 58.230408333003915, 58.05557983904161, 49.93777645018186, 49.46141821118809, 37.351761769745835, 35.14718899132772, 26.957459892261564, 22.14985307772047, 12.386232680214947, 10.918992561129624, 6.830326873310188, 6.66773082336539, 6.645509581221888, 5.1214652711920605, 105.39959348402688, 1320.648377917768, 35.65557311498208, 16.65215573712733, 5.850957336714514, 3.9269993384517843, 3.39451793139322, 2.4065863907040965, 2.326861465749766, 1.903239576504719, 1.5117230203331926, 1.5117230203331926, 1.5117230203331926, 1.5117230203331926, 1.5117230203331926, 1.5117230203331926, 1.5117230203331926, 1.5117230203331926, 1.5117230203331926, 1.5117230203331926, 1.5117230203331926, 1.5117230203331926, 1.5117230203331926, 1.5117230203331926, 1.5117230203331926, 1.5117230203331926, 1.5117234401172395, 1.511724093518524, 1.5117247707347208, 1.51172304118968, 1.5117232007322412, 1.5117233159882013, 1.5117235655312926, 1.5117232796245854, 1.5117230978729723, 1.5117233739815061, 1.5117234599947569, 1.5117235814880092, 1.511723127288594, 1.5117238568983593, 1.5117231499568577, 1.5117233436213988], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\"], \"logprob\": [25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.0231, -5.2749, -5.2787, -5.51, -5.5201, -5.5267, -5.58, -5.6289, -5.645, -5.7407, -5.764, -5.7987, -5.8531, -5.8654, -5.8771, -5.8943, -5.9022, -5.936, -5.9495, -5.9501, -5.9594, -5.988, -6.001, -6.0042, -6.0062, -5.3917, -5.5865, -5.471, -4.8888, -5.1487, -5.439, -5.1015, -5.6512, -4.8112, -5.5829, -5.3525, -5.0792, -5.5075, -5.0238, -5.4111, -4.9187, -4.8025, -5.2758, -5.3255, -5.3174, -5.1419, -5.1163, -5.3597, -5.2691, -5.325, -5.3534, -5.3465, -1.8512, -2.7813, -2.8353, -3.334, -4.1872, -4.2587, -4.3691, -4.8299, -5.4427, -5.55, -5.551, -5.5927, -6.2052, -2.7952, -6.3784, -6.6008, -6.6851, -6.6947, -6.9158, -7.121, -7.5646, -7.6041, -7.8554, -7.8699, -7.9843, -3.2249, -3.6524, -3.9316, -3.8228, -5.6052, -4.1007, -4.9627, -4.2416, -3.5716, -4.7258, -4.3899, -4.7432, -4.6185, -4.1918, -3.7408, -3.9438, -4.2553, -4.5008, -4.1904, -4.1971, -4.2879, -4.5069, -4.53, -4.6921, -4.6951, -4.7422, -4.7506, -4.8032, -4.8427, -4.8751, -4.8778, -4.8981, -4.9248, -4.9481, -4.9819, -5.016, -5.031, -5.0945, -5.1493, -5.1507, -5.1944, -5.2508, -5.3064, -4.4033, -5.1613, -4.9789, -4.6867, -4.7782, -4.1821, -3.7951, -4.0908, -4.8124, -4.0633, -3.8416, -4.1379, -4.3728, -4.0892, -3.9725, -4.4741, -4.6917, -4.7256, -4.7155, -4.1031, -4.4044, -4.4136, -4.486, -4.5313, -4.5425, -4.5833, -4.6097, -4.6164, -4.6315, -4.6515, -4.6722, -4.6749, -4.6803, -4.7066, -4.7408, -4.7678, -4.7792, -4.8274, -4.8322, -4.8436, -4.8501, -4.8696, -7.0577, -7.7342, -4.7367, -4.6928, -4.6528, -4.6061, -4.6762, -4.5866, -4.7085, -4.6337, -4.6577, -4.6739, -4.6828, -4.666, -4.6858, -3.2042, -3.3291, -3.3914, -3.5046, -3.5161, -3.7107, -3.7267, -3.7685, -3.8222, -3.9412, -3.9415, -3.9862, -3.9868, -3.9934, -4.0621, -4.0882, -4.2269, -4.2387, -4.2641, -4.3399, -4.344, -4.4152, -4.4575, -4.4981, -4.5495, -0.7385, -1.3683, -3.492, -3.6886, -4.6863, -4.8981, -5.5838, -5.5983, -5.6843, -6.203, -3.7039, -6.8245, -6.9772, -5.4435, -7.685, -12.2039, -12.2039, -12.2039, -12.2039, -12.2039, -12.2039, -12.2039, -12.2039, -12.2039, -12.2039, -12.2039, -12.2039, -12.2039, -12.2039, -12.2039, -12.2039, -12.2039, -12.2039, -2.5592, -2.6634, -2.7175, -2.7244, -2.7569, -2.8997, -3.0173, -3.2497, -3.4334, -3.6745, -3.6833, -3.871, -4.0867, -4.3224, -4.5546, -4.7962, -4.8324, -4.8665, -5.0591, -5.1505, -5.1841, -5.2268, -5.2857, -5.3827, -5.3939, -3.9925, -5.3705, -1.534, -2.2543, -2.3394, -2.7028, -3.1299, -3.5801, -3.6327, -3.8281, -3.9299, -3.9534, -3.9662, -4.0603, -4.1133, -4.3517, -4.8382, -4.9688, -5.025, -5.2666, -5.2742, -5.3336, -5.3479, -5.5682, -5.6878, -5.993, -6.1067, -3.2983, -3.0087, -3.0639, -3.4102, -4.2413, -4.6919, -4.712, -4.9756, -5.0674, -0.7751, -5.6472, -6.0976, -6.4659, -6.6286, -6.9062, -6.9551, -2.4271, -2.6195, -7.6358, -7.6897, -9.6064, -9.6773, -11.9664, -11.9664, -11.9664, -11.9664, -11.9664, -11.9664, -11.9664, -11.9664, -11.9664, -1.8939, -1.907, -2.339, -2.5135, -2.7057, -3.5691, -3.5908, -3.8943, -3.9568, -4.0433, -4.0758, -4.167, -4.1941, -4.213, -4.5986, -5.0325, -5.138, -5.4758, -5.6041, -5.6934, -5.7441, -5.9265, -5.9934, -6.2174, -6.2732, -5.9102, -2.3388, -2.4928, -2.5885, -2.6256, -2.7118, -3.0774, -3.1034, -3.1092, -3.3506, -3.5901, -3.7649, -3.8911, -4.178, -4.4474, -4.5419, -4.653, -4.6711, -5.1495, -5.562, -5.7141, -5.7713, -5.8436, -5.9677, -6.1168, -6.1302, -1.9443, -2.4632, -2.904, -2.9234, -3.9648, -4.2118, -4.4602, -4.5096, -4.679, -4.7179, -4.7972, -4.8069, -5.0002, -5.1297, -5.1489, -5.1557, -5.2189, -5.3202, -5.3215, -5.3606, -5.5694, -5.6447, -5.6906, -5.7143, -5.7309, -2.9862, -2.8983, -3.7675, -3.8265, -3.6507, -4.5553, -2.1336, -2.6055, -2.7134, -2.9562, -3.1775, -3.6199, -3.8049, -4.0757, -4.1303, -4.2332, -4.3821, -4.6183, -4.6679, -4.6991, -4.9115, -4.9654, -5.0006, -5.0237, -5.0616, -5.0766, -5.154, -5.1812, -5.2601, -5.2679, -5.2906, -3.3253, -4.5843, -4.4452, -1.9437, -2.7735, -3.3113, -3.4121, -3.5124, -3.6102, -3.8638, -3.8751, -4.0687, -4.1356, -4.1656, -4.2478, -4.2573, -4.2633, -4.335, -4.4078, -4.5386, -4.7145, -4.7421, -4.8438, -4.8891, -5.0105, -5.0168, -5.1193, -5.191, -3.7576, -4.429, -4.2724, -2.4171, -2.7432, -3.3405, -3.342, -3.4326, -3.5823, -3.5963, -3.653, -3.7889, -3.8533, -4.0106, -4.081, -4.1884, -4.3084, -4.3262, -4.3587, -4.4362, -4.4657, -4.479, -4.5053, -4.5359, -4.6941, -4.8198, -4.9215, -5.263, -4.2024, -2.875, -4.5605, -2.6535, -2.8665, -2.9141, -2.9898, -3.2452, -3.3607, -3.4093, -3.438, -3.4825, -3.823, -3.9985, -4.0635, -4.1452, -4.2039, -4.2055, -4.2445, -4.4422, -4.4816, -4.6312, -4.6341, -4.7172, -4.7324, -5.2285, -5.2625, -5.6077, -3.5915, -1.9709, -2.2368, -2.4669, -2.6926, -3.0491, -3.2801, -3.5332, -3.61, -4.0294, -4.2177, -4.4845, -4.599, -4.6954, -4.7547, -4.9083, -4.9554, -5.082, -5.1861, -5.2862, -5.5154, -5.5578, -5.5583, -5.6426, -5.6807, -5.7904, -2.1218, -2.2902, -2.5033, -2.681, -2.7806, -3.0753, -3.1779, -3.9769, -4.2193, -4.413, -4.5436, -4.6333, -4.6498, -4.6512, -4.7444, -4.9434, -4.9534, -5.1887, -5.303, -5.4184, -5.5436, -5.5917, -5.6392, -5.7762, -5.9002, -1.1741, -1.9529, -2.4722, -2.6643, -3.8037, -3.8764, -3.9787, -4.5287, -5.7379, -5.9664, -8.2629, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -11.4654, -1.8545, -2.3669, -2.863, -2.9336, -3.0627, -3.5888, -3.6378, -3.6652, -4.0041, -4.0716, -4.1043, -4.1084, -4.3923, -4.5071, -4.8367, -4.9434, -4.9647, -5.0367, -5.1094, -5.2044, -5.2741, -5.2909, -5.4706, -5.5047, -5.55, -5.2382, -1.4215, -2.0557, -2.4083, -3.6765, -3.9884, -4.0271, -4.1816, -4.2216, -4.483, -4.5344, -4.5664, -4.927, -5.0816, -5.5785, -5.5867, -5.6983, -5.8534, -5.8721, -5.9531, -5.9531, -6.0769, -6.3503, -6.4176, -6.7824, -6.896, -5.0651, -6.2562, -2.2455, -2.2631, -2.3542, -2.7131, -2.9465, -3.0142, -3.2131, -3.4537, -4.3358, -4.3894, -4.7048, -4.756, -4.8825, -4.9374, -4.9825, -5.1751, -5.8442, -6.1177, -6.153, -6.1541, -6.1668, -6.3319, -6.349, -6.4577, -6.6396, -5.901, -2.0416, -2.5142, -2.709, -2.9018, -3.2308, -3.4066, -3.4077, -3.6075, -3.9452, -3.9599, -3.961, -4.4886, -4.5095, -4.5885, -4.6648, -5.0983, -5.2897, -5.3515, -5.3524, -5.4462, -5.5757, -5.9216, -6.1145, -6.133, -6.1423, -3.489, -5.155, -5.6592, -2.2085, -2.6778, -2.8059, -2.9022, -3.1606, -3.5453, -3.8634, -3.9262, -3.9493, -3.9599, -4.03, -4.2027, -4.2058, -4.3606, -4.3705, -4.6612, -4.7246, -5.0031, -5.212, -5.8505, -5.9946, -6.5606, -6.5914, -6.5956, -6.9435, -3.9282, -0.5956, -4.2488, -5.0608, -6.305, -6.8848, -7.13, -7.8546, -7.9444, -8.6358, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737, -11.1737], \"loglift\": [25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.7728, 0.7727, 0.7727, 0.7726, 0.7726, 0.7726, 0.7725, 0.7725, 0.7725, 0.7724, 0.7724, 0.7723, 0.7723, 0.7723, 0.7723, 0.7722, 0.7722, 0.7722, 0.7722, 0.7722, 0.7722, 0.7721, 0.7721, 0.7721, 0.7721, 0.7701, 0.7654, 0.7617, 0.7458, 0.7502, 0.7538, 0.7377, 0.7429, 0.6352, 0.7319, 0.6931, 0.5936, 0.6898, 0.5427, 0.6583, 0.4811, 0.3847, 0.5437, 0.5698, 0.5609, 0.4338, 0.4134, 0.5869, 0.4308, 0.4757, 0.4685, 0.2779, 1.4219, 1.4218, 1.4218, 1.4218, 1.4215, 1.4215, 1.4215, 1.4212, 1.4206, 1.4205, 1.4205, 1.4204, 1.4192, 1.4191, 1.4187, 1.4179, 1.4176, 1.4175, 1.4164, 1.4152, 1.4114, 1.411, 1.4079, 1.4077, 1.406, 1.4031, 1.4056, 1.3955, 1.3552, 1.3784, 1.3229, 1.3352, 1.2994, 1.2641, 1.3232, 1.2951, 1.2975, 1.2752, 1.2273, 1.17, 1.1112, 1.1332, 0.8855, 1.8987, 1.8987, 1.8987, 1.8985, 1.8985, 1.8983, 1.8983, 1.8983, 1.8983, 1.8982, 1.8982, 1.8981, 1.8981, 1.8981, 1.8981, 1.8981, 1.898, 1.898, 1.898, 1.8979, 1.8978, 1.8978, 1.8977, 1.8976, 1.8975, 1.8921, 1.8974, 1.8951, 1.8891, 1.8869, 1.8611, 1.8147, 1.8177, 1.8779, 1.7966, 1.7593, 1.7569, 1.7719, 1.5017, 1.4156, 1.5965, 1.7578, 1.7658, 1.5294, 3.8336, 3.8323, 3.8322, 3.8319, 3.8316, 3.8316, 3.8313, 3.8312, 3.8311, 3.831, 3.8309, 3.8308, 3.8307, 3.8307, 3.8305, 3.8303, 3.8301, 3.83, 3.8297, 3.8296, 3.8295, 3.8295, 3.8293, 3.7682, 3.7057, 3.5526, 3.4514, 3.3926, 3.1828, 3.3144, 3.0073, 3.3397, 2.9159, 2.7807, 2.7437, 2.8095, 2.4901, 2.7128, 4.3445, 4.3441, 4.344, 4.3436, 4.3436, 4.3428, 4.3428, 4.3426, 4.3423, 4.3418, 4.3418, 4.3415, 4.3415, 4.3415, 4.3411, 4.341, 4.3401, 4.34, 4.3398, 4.3392, 4.3392, 4.3386, 4.3383, 4.3379, 4.3375, 4.5944, 4.5942, 4.5904, 4.5894, 4.5805, 4.5772, 4.5603, 4.5598, 4.5568, 4.5318, 4.5234, 4.4807, 4.463, 4.3648, 4.3435, 1.2918, 1.2918, 1.2918, 1.2918, 1.2918, 1.2918, 1.2918, 1.2918, 1.2918, 1.2918, 1.2918, 1.2918, 1.2918, 1.2918, 1.2918, 1.2918, 1.2918, 1.2918, 4.6874, 4.6872, 4.6871, 4.6871, 4.687, 4.6867, 4.6864, 4.6856, 4.6849, 4.6837, 4.6836, 4.6825, 4.6808, 4.6786, 4.6758, 4.6722, 4.6716, 4.6709, 4.6671, 4.665, 4.6642, 4.6631, 4.6615, 4.6588, 4.6584, 4.492, 3.8763, 4.7621, 4.7613, 4.7612, 4.7605, 4.7593, 4.7572, 4.7569, 4.7557, 4.7549, 4.7547, 4.7546, 4.7538, 4.7534, 4.7508, 4.7434, 4.7407, 4.7395, 4.7332, 4.733, 4.7312, 4.7308, 4.723, 4.7181, 4.7026, 4.6956, 4.2513, 4.778, 4.7779, 4.7765, 4.7703, 4.7642, 4.7638, 4.7586, 4.7565, 4.74, 4.7375, 4.7134, 4.6846, 4.6685, 4.6349, 4.6281, 4.5471, 4.5447, 4.4984, 4.4849, 3.5737, 3.5234, 1.5294, 1.5294, 1.5294, 1.5294, 1.5294, 1.5294, 1.5294, 1.5294, 1.5294, 4.8762, 4.8762, 4.8755, 4.8752, 4.8747, 4.8712, 4.8711, 4.8688, 4.8683, 4.8675, 4.8672, 4.8662, 4.8659, 4.8657, 4.8602, 4.8511, 4.8482, 4.8367, 4.8313, 4.8271, 4.8246, 4.8144, 4.8101, 4.794, 4.7894, 4.0991, 5.1202, 5.1198, 5.1195, 5.1194, 5.1191, 5.1177, 5.1175, 5.1175, 5.1161, 5.1144, 5.1129, 5.1116, 5.1081, 5.1037, 5.1018, 5.0994, 5.099, 5.0849, 5.0663, 5.0574, 5.0537, 5.0487, 5.0393, 5.0266, 5.0254, 5.1482, 5.1471, 5.1457, 5.1456, 5.1379, 5.1345, 5.1303, 5.1293, 5.1256, 5.1246, 5.1226, 5.1223, 5.1165, 5.112, 5.1113, 5.111, 5.1086, 5.1043, 5.1042, 5.1024, 5.0917, 5.0873, 5.0845, 5.083, 5.0819, 5.0663, 4.9354, 4.9616, 4.7687, 4.3428, 3.734, 5.1799, 5.1787, 5.1783, 5.1774, 5.1762, 5.1731, 5.1714, 5.1681, 5.1674, 5.1658, 5.1633, 5.1584, 5.1572, 5.1564, 5.1505, 5.1488, 5.1476, 5.1468, 5.1455, 5.1449, 5.142, 5.141, 5.1377, 5.1373, 5.1363, 5.1105, 4.0109, 3.4294, 5.2216, 5.2194, 5.2166, 5.2159, 5.2151, 5.2143, 5.2117, 5.2116, 5.2091, 5.2081, 5.2077, 5.2064, 5.2062, 5.2061, 5.2048, 5.2034, 5.2007, 5.1964, 5.1957, 5.1927, 5.1913, 5.1873, 5.1871, 5.1832, 5.1803, 4.9395, 4.898, 3.5614, 5.2932, 5.292, 5.2887, 5.2887, 5.288, 5.2867, 5.2865, 5.286, 5.2845, 5.2838, 5.2817, 5.2806, 5.2789, 5.2767, 5.2764, 5.2757, 5.2741, 5.2734, 5.2731, 5.2725, 5.2718, 5.2677, 5.264, 5.2606, 5.2465, 5.2107, 4.7189, 4.6863, 5.3073, 5.3064, 5.3061, 5.3057, 5.3042, 5.3034, 5.303, 5.3028, 5.3024, 5.2989, 5.2966, 5.2956, 5.2943, 5.2933, 5.2933, 5.2926, 5.2886, 5.2877, 5.284, 5.2839, 5.2816, 5.2811, 5.2624, 5.2607, 5.2407, 4.8367, 5.3628, 5.3621, 5.3615, 5.3606, 5.3588, 5.3573, 5.3551, 5.3544, 5.349, 5.3457, 5.34, 5.337, 5.3342, 5.3324, 5.3271, 5.3253, 5.3201, 5.3153, 5.3102, 5.2967, 5.2938, 5.2938, 5.2878, 5.2849, 5.276, 5.3707, 5.3703, 5.3696, 5.3689, 5.3685, 5.3669, 5.3663, 5.358, 5.3538, 5.3498, 5.3466, 5.3441, 5.3436, 5.3436, 5.3407, 5.3338, 5.3334, 5.3231, 5.3172, 5.3106, 5.3025, 5.2992, 5.2957, 5.2848, 5.2738, 5.3989, 5.3978, 5.3964, 5.3957, 5.3868, 5.3858, 5.3843, 5.373, 5.3125, 5.2912, 4.6385, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 2.0304, 5.4011, 5.3998, 5.3978, 5.3974, 5.3967, 5.3923, 5.3918, 5.3915, 5.3869, 5.3858, 5.3853, 5.3852, 5.3794, 5.3766, 5.3665, 5.3625, 5.3616, 5.3586, 5.3553, 5.3507, 5.3471, 5.3461, 5.3353, 5.3331, 5.3299, 4.8609, 5.4432, 5.442, 5.441, 5.4324, 5.428, 5.4274, 5.4246, 5.4238, 5.4177, 5.4163, 5.4154, 5.403, 5.3962, 5.3664, 5.3658, 5.3569, 5.3429, 5.3411, 5.3329, 5.3329, 5.3191, 5.2827, 5.2723, 5.205, 5.1798, 5.0259, 4.5488, 5.4651, 5.465, 5.4647, 5.4633, 5.4621, 5.4617, 5.4603, 5.4582, 5.4443, 5.443, 5.4339, 5.4322, 5.4274, 5.4252, 5.4232, 5.414, 5.3651, 5.3347, 5.3303, 5.3301, 5.3285, 5.3054, 5.3028, 5.2854, 5.2527, 4.7852, 5.5204, 5.5188, 5.5179, 5.5169, 5.5145, 5.5129, 5.5129, 5.5107, 5.5058, 5.5056, 5.5056, 5.4937, 5.4931, 5.4907, 5.4882, 5.4698, 5.4589, 5.455, 5.4549, 5.4485, 5.4386, 5.4057, 5.3825, 5.38, 5.3788, 5.1602, 4.8543, 3.1007, 5.6637, 5.6617, 5.6609, 5.6603, 5.6582, 5.654, 5.6491, 5.6479, 5.6475, 5.6473, 5.6458, 5.6419, 5.6418, 5.6376, 5.6373, 5.6274, 5.6248, 5.6116, 5.5991, 5.5419, 5.5238, 5.427, 5.4203, 5.4194, 5.332, 5.323, 6.1274, 6.0862, 6.0356, 5.8374, 5.6563, 5.5568, 5.1761, 5.12, 4.6296, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322, 2.322]}, \"token.table\": {\"Topic\": [16, 4, 9, 24, 14, 9, 11, 23, 2, 23, 25, 19, 20, 19, 21, 21, 4, 9, 22, 4, 9, 8, 11, 21, 24, 3, 17, 7, 1, 3, 3, 3, 2, 13, 15, 4, 5, 10, 23, 3, 23, 1, 2, 3, 4, 2, 21, 11, 7, 11, 1, 5, 1, 14, 23, 17, 4, 1, 3, 8, 20, 8, 21, 13, 22, 4, 1, 4, 1, 4, 21, 19, 23, 10, 13, 20, 10, 23, 15, 1, 2, 3, 18, 17, 21, 13, 14, 6, 1, 2, 3, 18, 1, 2, 3, 5, 22, 7, 4, 9, 17, 12, 1, 4, 17, 8, 6, 10, 8, 3, 4, 1, 2, 3, 12, 5, 2, 3, 8, 20, 11, 8, 7, 13, 18, 9, 1, 2, 11, 16, 9, 14, 16, 17, 5, 18, 18, 4, 13, 1, 2, 21, 1, 3, 8, 4, 8, 7, 12, 3, 8, 23, 14, 6, 5, 17, 20, 24, 9, 22, 5, 23, 7, 4, 22, 17, 24, 10, 15, 1, 4, 25, 24, 8, 20, 20, 4, 10, 7, 13, 5, 24, 1, 15, 3, 3, 1, 25, 15, 13, 15, 17, 20, 25, 3, 4, 12, 4, 12, 8, 22, 18, 20, 22, 10, 4, 13, 13, 14, 9, 1, 22, 20, 17, 22, 4, 3, 14, 23, 17, 5, 1, 1, 2, 3, 5, 23, 8, 18, 13, 6, 22, 1, 2, 3, 18, 18, 7, 15, 13, 20, 24, 9, 14, 8, 22, 7, 11, 1, 3, 3, 6, 8, 2, 1, 2, 3, 17, 24, 14, 1, 21, 1, 1, 2, 3, 18, 3, 21, 10, 12, 12, 18, 16, 21, 2, 19, 11, 2, 12, 3, 17, 16, 2, 24, 24, 1, 2, 3, 4, 10, 8, 20, 1, 3, 24, 10, 1, 2, 3, 10, 23, 2, 4, 8, 4, 2, 3, 4, 2, 16, 11, 15, 2, 5, 13, 4, 6, 15, 7, 13, 23, 2, 22, 20, 10, 18, 4, 15, 5, 24, 24, 1, 22, 15, 10, 21, 16, 18, 16, 3, 10, 12, 16, 15, 1, 2, 3, 2, 4, 13, 15, 21, 3, 6, 5, 6, 3, 23, 22, 17, 4, 11, 22, 9, 9, 19, 9, 12, 4, 12, 24, 1, 8, 23, 16, 23, 22, 20, 1, 3, 4, 15, 10, 24, 22, 7, 1, 2, 3, 9, 13, 1, 16, 7, 4, 5, 16, 1, 1, 4, 1, 21, 1, 23, 1, 2, 3, 12, 12, 13, 16, 16, 23, 21, 20, 1, 2, 3, 4, 18, 1, 2, 3, 2, 4, 20, 18, 25, 25, 15, 1, 2, 3, 16, 21, 1, 2, 3, 1, 1, 2, 4, 4, 6, 1, 16, 4, 12, 20, 14, 8, 22, 2, 1, 3, 10, 7, 10, 4, 1, 2, 3, 4, 1, 2, 7, 23, 7, 4, 13, 15, 15, 1, 2, 3, 4, 18, 12, 1, 3, 17, 2, 24, 14, 18, 23, 11, 14, 11, 11, 24, 14, 10, 5, 20, 19, 3, 23, 1, 2, 3, 17, 18, 1, 12, 1, 12, 14, 9, 16, 11, 5, 1, 4, 24, 1, 16, 5, 15, 1, 20, 23, 3, 1, 19, 17, 17, 12, 14, 18, 14, 9, 6, 7, 10, 2, 2, 2, 11, 1, 2, 3, 13, 3, 18, 17, 1, 16, 14, 4, 21, 11, 1, 4, 6, 1, 2, 3, 21, 24, 21, 12, 7, 14, 8, 23, 8, 13, 16, 21, 24, 2, 6, 16, 3, 5, 2, 16, 9, 5, 17, 5, 7, 15, 7, 12, 1, 2, 3, 13, 2, 4, 25, 6, 16, 2, 3, 24, 6, 3, 24, 5, 1, 3, 1, 4, 9, 15, 10, 22, 5, 19, 19, 10, 14, 23, 2, 12, 9, 7, 24, 14, 24, 11, 14, 1, 3, 15, 7, 8, 1, 22, 21, 1, 21, 11, 11, 19, 1, 3, 4, 20, 23, 13, 18, 14, 22, 2, 1, 3, 8, 17, 1, 16, 14, 15, 5, 12, 15, 7, 12, 17, 22, 2, 10, 1, 3, 1, 22, 5, 15, 5, 1, 2, 4, 10, 4, 16, 19, 22, 1, 2, 3, 3, 4, 21, 1, 2, 3, 18, 8, 20, 22, 7, 11, 2, 3, 5, 2, 4, 18, 15, 1, 4, 12, 1, 2, 1, 3, 4, 11, 4, 12, 1, 2, 3, 1, 2, 3, 22, 14, 1, 2, 3, 16, 5, 7, 13, 1, 2, 3, 4, 10, 20, 4, 24, 1, 14, 1, 2, 3, 2, 13, 15, 23, 1, 3, 6, 11, 21, 14, 8, 21, 11, 21, 8, 3, 13, 4, 23, 1, 12, 13, 20, 9, 13, 3, 20, 8, 11, 7, 12, 17, 4, 3, 1, 2, 1, 2, 5, 1, 2, 3, 14, 20, 18, 24, 10, 1, 10, 1, 21, 11, 23, 18, 1, 3, 1, 2, 1, 2, 3, 15, 13, 17, 25, 7, 9, 2, 3, 20, 17, 1, 2, 13, 7, 14, 12, 4, 4, 12, 12, 12, 15], \"Freq\": [0.9948603358946957, 0.04004716138419408, 0.9595480871772152, 0.8242518666089196, 0.9502200438738597, 0.9947713685364, 0.9302912150929342, 0.9559472437155888, 0.9813134675505348, 0.9884896503841464, 0.9007842730269622, 0.9818558019019025, 0.9483248503505286, 0.9175456016443431, 0.9220442697572531, 0.73194312257198, 0.20731241952561394, 0.7912910660766391, 0.8841347212515314, 0.20950024305875362, 0.7897452982720431, 0.9628138695749223, 0.9915894980597958, 0.9808911047299479, 0.9273870794917337, 0.9991257831991902, 0.9562073736651696, 0.9972478932043511, 0.9919550712279046, 0.007312991573033525, 0.9989628131300462, 0.9990176598906622, 0.9994220410670794, 0.06693274993401258, 0.9147475824315051, 0.9945766020191776, 0.9908038714352971, 0.4657414273650173, 0.506240681918497, 0.9989815784404386, 0.9945139512546076, 0.7941318247566039, 0.1271944794301219, 0.038587089265317884, 0.04001624071958892, 0.9996709912396617, 0.9652002602100264, 0.9336000379082624, 0.9618672090428143, 0.9833689133376938, 0.9987313811616513, 0.9923934572461036, 0.2661057056477686, 0.7266732731150604, 0.9897180643372542, 0.9785348943122696, 0.9952581192873533, 0.9807532464391799, 0.01871496426644505, 0.37657143889333006, 0.5759327888956812, 0.9929555989736601, 0.9487671342962256, 0.9804248255064937, 0.9679547427851598, 0.9946403536679482, 0.40343961798700845, 0.5917114397142791, 0.4770108291511343, 0.5187767542841829, 0.9874984914186195, 0.9990349812522172, 0.9874313287597241, 0.9873551677331238, 0.9722542002614323, 0.9928028790211453, 0.9809121038657813, 0.984855007544663, 0.988577100666969, 0.9229444297426739, 0.03191849486193414, 0.044993540949955356, 0.9350909646439497, 0.9489897929176624, 0.8773959852785504, 0.9772174643888878, 0.9854864952159413, 0.9997362480757086, 0.06059805795335997, 0.8808169043220686, 0.05818486095521732, 0.9928848467559972, 0.1683796773555918, 0.09227364051805266, 0.7389777877385927, 0.9927765844692474, 0.8371458005677317, 0.9950736669058916, 0.9939215034562906, 0.8807125036403584, 0.9920617355290045, 0.9350506842820925, 0.6405214655424774, 0.3578423493719453, 0.9155682409431424, 0.9250399021924296, 0.9830294978959631, 0.937656331339028, 0.9769842422638182, 0.8750711538056694, 0.12432376721878763, 0.7101010631250456, 0.2759840850934443, 0.0136086826969154, 0.9694321587758586, 0.9951085409229896, 0.8828951265577712, 0.11634162049135781, 0.9921840065214028, 0.9820342982692478, 0.9975302853369827, 0.9969037669885124, 0.9772756100094921, 0.997519643878335, 0.9969839303562721, 0.4824341424055118, 0.9970317392646837, 0.0025919369304974447, 0.9940652366934305, 0.9795163615011238, 0.9757075863276439, 0.9738350141362191, 0.9760881708822232, 0.9483557208142304, 0.9905408347141599, 0.9811268460512073, 0.9970884153350926, 0.9921833180062776, 0.9938725870773548, 0.9698963743239627, 0.029423822591850553, 0.997182266524781, 0.13230770731505542, 0.8672770214501883, 0.9908629044744369, 0.39802591983411606, 0.5987694272287137, 0.9977926922487846, 0.9808620053484102, 0.9994301353195155, 0.9994041539885637, 0.9884073089755185, 0.9779305479116182, 0.9964232741983411, 0.9955537589746043, 0.9804977311670313, 0.953376751768995, 0.996384476500673, 0.8794127745030588, 0.9971391404910466, 0.2962238042182043, 0.6963857853550768, 0.9916509133504434, 0.9961765354303286, 0.9513298230596948, 0.9852554996192304, 0.9719780991445647, 0.9452794738118708, 0.9597853562406582, 0.6502801883457433, 0.34836438661379104, 0.5092947127382247, 0.9788700033500102, 0.9698696677425264, 0.9731046795217726, 0.95325115363923, 0.9899910571444231, 0.9258579336231657, 0.9723727502971341, 0.9623982694870405, 0.27514337618764056, 0.7115776970370016, 0.9989072517137262, 0.9853508115170367, 0.9987206427953592, 0.9991619323564239, 0.9995152492672943, 0.9535676201405264, 0.9654492046039105, 0.9669210215877694, 0.980357271683697, 0.9911521010647342, 0.9793301668510107, 0.5891852806266177, 0.9989632305461952, 0.9922466028141931, 0.9463551958119504, 0.7505231728334752, 0.24292295932291227, 0.9983112093458859, 0.798552967626224, 0.9611156224609174, 0.9976012453059558, 0.9902094656060002, 0.992322111950219, 0.9948888039001959, 0.9627580833606819, 0.9976775207515091, 0.9791538057469721, 0.9858010916258713, 0.4383108085425222, 0.5180036828229808, 0.9822476407914537, 0.9970316524217175, 0.9685440155494223, 0.9948749376556244, 0.9990520960365021, 0.9887111658087877, 0.9436822026298768, 0.940654150194833, 0.9960442221863762, 0.9994935059811086, 0.08614278624760675, 0.8846469423271746, 0.02902384981388368, 0.9886871491104946, 0.9771037136917992, 0.9397123345549131, 0.9958966098602745, 0.9605200275344973, 0.9994078130444604, 0.9572769339273834, 0.9594453115715141, 0.033223184975718745, 0.006543960677035511, 0.9799670535796933, 0.9818780764399783, 0.9853153935910551, 0.9736967749129878, 0.9530701726574962, 0.9578744592477855, 0.7810264813274754, 0.9966294241840823, 0.9942959688818939, 0.9827960323811719, 0.9671052127108619, 0.9943584260090967, 0.9916798821429362, 0.002641782897509458, 0.9959521523610656, 0.9995978264511844, 0.8469558602245327, 0.994122929575312, 0.9988995120755961, 0.6092632895137339, 0.1241746223863948, 0.26601636584402466, 0.9762616917705441, 0.9856339010990489, 0.9535266980861463, 0.9989693661931769, 0.8710495091587432, 0.9996280637261392, 0.7426493025370837, 0.2227646874249866, 0.03431780319790334, 0.9324763857792502, 0.9985140128538039, 0.9882245040672382, 0.9952055002329666, 0.9628316895289024, 0.9812336213219092, 0.991776720063246, 0.9809126230413371, 0.9291089743643492, 0.9995962887841187, 0.3651151614381264, 0.9970127273726485, 0.9998770832501075, 0.9966105193755288, 0.9987419639782197, 0.9900172961228494, 0.9849645222076432, 0.9999686774936655, 0.948087552829998, 0.9704533702420705, 0.13132741052802308, 0.8540296059315806, 0.0021792321397663215, 0.012272517839736654, 0.9408955234816239, 0.9927692525488242, 0.9794921898304338, 0.8300576365992185, 0.16963410366085582, 0.9910557833458692, 0.9821383484729203, 0.7465620293021535, 0.24515782544731804, 0.008064402152872303, 0.9974040406816364, 0.9947391288594581, 0.9972604376171565, 0.0026495025528290228, 0.9663096157028119, 0.9940189245153791, 0.9060615900219896, 0.02083787302435581, 0.07254666904775726, 0.9998227243252015, 0.9907685956914847, 0.9824281545511104, 0.9773997331753977, 0.9958504231831781, 0.9975305755220836, 0.9768344117744272, 0.18217562873575863, 0.794948198119674, 0.997766715907526, 0.9982494159810547, 0.9839270878452853, 0.9883959842033109, 0.9954182774374842, 0.9967139498112572, 0.9273410999502351, 0.9976897897794064, 0.9528554713429842, 0.9164295861614447, 0.9626904437270077, 0.9952907882646481, 0.9751706173443252, 0.9611961807687814, 0.9990232474931934, 0.8738982531381463, 0.9742636742501166, 0.9913024601481277, 0.9988413998975176, 0.9828562606449969, 0.9706357227110441, 0.9922893495086971, 0.999025511725084, 0.9900162788914789, 0.9580297724672531, 0.990907026617112, 0.9928552924630688, 0.07825302508358381, 0.9057942507696098, 0.015691899489056384, 0.9931794380420752, 0.06296778393464433, 0.9319232022327361, 0.9905058335021906, 0.9849291841379506, 0.9988222051967279, 0.9931495091976954, 0.9932583576977655, 0.7626258202293775, 0.9987194487523617, 0.9403979795227112, 0.802592345866025, 0.9596883334890721, 0.9941832170849972, 0.9545787700885118, 0.9942928211563418, 0.4925069604296351, 0.843136801530715, 0.9910489254395807, 0.9843023218396569, 0.9594389245636511, 0.16322021900603947, 0.8273576618582001, 0.9913047961889785, 0.9986831453351444, 0.9963624947573604, 0.907779043211101, 0.9843639818802369, 0.9330225606715623, 0.9933102424730511, 0.9334328320749327, 0.036829237008211536, 0.9625474547250287, 0.4359381762997577, 0.5607503512569497, 0.9991927088053755, 0.9908564085239425, 0.8250557494524854, 0.9983980781125822, 0.8912391693888179, 0.03780775972661357, 0.07010188782642933, 0.6781045879451082, 0.9873880363327948, 0.3707750320888766, 0.6207357278791305, 0.996211978117719, 0.993546398750082, 0.9946570932857203, 0.9618082486984701, 0.9988474272654188, 0.9197945792327367, 0.07967077133986722, 0.5384542045341961, 0.4038406534006471, 0.9054938795493345, 0.08706671918743601, 0.01844963498032031, 0.9813714929936036, 9.317997464808239e-05, 0.9707349434453869, 0.9768368474759629, 0.9960787768089044, 0.9920373555110875, 0.9828363521586323, 0.996122144233975, 0.9849514086682566, 0.9937214246956136, 0.030095067010813922, 0.006964147738039586, 0.9187700622970796, 0.04377464292482025, 0.9765180261992112, 0.23122914899528105, 0.7493020440286737, 0.01933554090736402, 0.9838490702446681, 0.015900880430570495, 0.9919292926976919, 0.9273996880046276, 0.4297634451897969, 0.6836488064782436, 0.9947590754957061, 0.6780305302268622, 0.3166858942862484, 0.005052631355055945, 0.9479380283639779, 0.9973452790426967, 0.7947954391671928, 0.2030700813310234, 0.00153376194358779, 0.9992051715719873, 0.16377029888584166, 0.5004092465956274, 0.33512255605343527, 0.06474840926577567, 0.9303324068187767, 0.9986726308672343, 0.9939336574085327, 0.9946301618300528, 0.9895099815358679, 0.9739794839222168, 0.9840540032504377, 0.9766647574995749, 0.9980507915407499, 0.9891216560525921, 0.00914969076773117, 0.9901272509366231, 0.9288548719266065, 0.9850412617817155, 0.9597601046448874, 0.9899100164485771, 0.29457524202962015, 0.584774619559138, 0.09308657209308727, 0.027249701659728537, 0.04126537221484199, 0.9569636318394309, 0.9944155811062078, 0.9968675263415466, 0.9784603007857885, 0.9939824811721271, 0.9789508843062017, 0.9799925249112619, 0.9912455973187941, 0.7122583793466976, 0.18341314324722585, 0.06682680710711505, 0.03749993492341708, 0.9197405555982451, 0.9691558694869817, 0.13016879660189618, 0.8692711682164507, 0.9630259464156137, 0.9994836078427327, 0.9818177711433045, 0.9593579478655837, 0.9371473355014365, 0.9653847219066531, 0.9972989623080062, 0.9879187052175976, 0.963728632546297, 0.9724360709240987, 0.8880827838452255, 0.9974210423447796, 0.9953571029121141, 0.9896933806235017, 0.9431847310006714, 0.9985558339533512, 0.9991628742432652, 0.9657261567678493, 0.2602293935006909, 0.7329842192346573, 0.006570006641620077, 0.9984209386948845, 0.9976720625794802, 0.9989746579416013, 0.9735189514866608, 0.9990403473154515, 0.932624712309788, 0.9900074713021594, 0.8874268731872887, 0.9372869567461688, 0.891980587838596, 0.9967773045009994, 0.3551546974596469, 0.6421196930070416, 0.7320293878668862, 0.9990567372010719, 0.9972559196494184, 0.9960423848319455, 0.9841507202037316, 0.9987919442163687, 0.9922341562463824, 0.8264538230019994, 0.9988886348700656, 0.9992008741046146, 0.9880119194211214, 0.9470063858443519, 0.9605303117837843, 0.8069690355095814, 0.19082173774158193, 0.9621101383865385, 0.9794518620221844, 0.7060095642813913, 0.9853046863733386, 0.9703804340543706, 0.9976674607240079, 0.9962295989816126, 0.9890108307104264, 0.999901885305972, 0.9955371268390598, 0.8355335970468176, 0.1565632073096605, 0.007682458565110245, 0.9690465921909415, 0.998807074909428, 0.9289949583098134, 0.932399668199559, 0.9992410892672277, 0.9813976139814177, 0.9921086643643406, 0.9963761471464049, 0.9360471579238244, 0.9965387798020008, 0.316758706606743, 0.6805607854818142, 0.9701400670509085, 0.13181577335740444, 0.8635750164317173, 0.0039643841611249455, 0.8696997721312568, 0.7498803014780913, 0.9741862612882598, 0.9507742614023892, 0.9984604422027789, 0.9861625781048339, 0.9936826864053437, 0.9325127937601466, 0.9929516844540178, 0.9513942182545941, 0.9767694636171069, 0.9739011468719047, 0.9785429672486222, 0.9871251429026558, 0.8878671285105206, 0.9715838820389654, 0.998108391384557, 0.994463296319301, 0.9958109900143872, 0.9939663238000377, 0.9430861390561315, 0.9953958668562096, 0.9964391991659394, 0.9947260633587075, 0.4439500992887686, 0.5478533140159272, 0.9842487660161267, 0.998128999818939, 0.0618576518195183, 0.9354556640695821, 0.002422623439406722, 0.975463141596735, 0.9739089417098703, 0.02587092087698982, 0.41552632553009217, 0.9202679281548058, 0.950459521336377, 0.9988456172517917, 0.9987729751616911, 0.7523877497864756, 0.9634583145082173, 0.9977794034469538, 0.9673604340987049, 0.9928155883102108, 0.13131851874006956, 0.8679693321986176, 0.38749736157003556, 0.6068892501060116, 0.9781846815629067, 0.9848647514950214, 0.9898432537864342, 0.9926776766231802, 0.9943222383917506, 0.9975226923247802, 0.9172386272943911, 0.956492749999505, 0.9869011067255453, 0.9108936648604389, 0.9973800947361952, 0.9519326873047025, 0.9855769803472679, 0.976444892243569, 0.9827921172205305, 0.9725782902460357, 0.9915770270076145, 0.994478650415508, 0.987681308762507, 0.09710319096705744, 0.9023569555326885, 0.9934691777263386, 0.9980011698192544, 0.9893790402935367, 0.9989675015144768, 0.9631609912322826, 0.8710495091587432, 0.30762145929297635, 0.6591888413420921, 0.9972644661198384, 0.9710906482743304, 0.9945509316280589, 0.1646185845432497, 0.5732462766718837, 0.26035421527052965, 0.9436227315117731, 0.8963769753421743, 0.9787585595861779, 0.9564458956415661, 0.9980212855132649, 0.8572834323179427, 0.9988660577203444, 0.006417715183039888, 0.9927711717764011, 0.9882023594073357, 0.9712062857717466, 0.9988290165757467, 0.9960311406043892, 0.9783428513317005, 0.9707655141902222, 0.9947185500406286, 0.9496210573368306, 0.9349080933736765, 0.9705203068774436, 0.9957970748791287, 0.9646924504885309, 0.9821817810497181, 0.9977228824386003, 0.9695157956051681, 0.9886088595122225, 0.010665104957214408, 0.9990128394995231, 0.8850146085217331, 0.99673076767282, 0.9902786447651541, 0.9957370249581863, 0.43742296284134885, 0.2365654799039948, 0.3243476391136532, 0.9986749263248438, 0.9941281865468237, 0.9850066771420024, 0.9866886261506674, 0.8948109259297043, 0.9651773666740481, 0.03316330660209047, 0.0015643069151929467, 0.9875055592199272, 0.011432770584311748, 0.9289322066009517, 0.0468256545853038, 0.33634768059571407, 0.6165045756890636, 0.9779065129532555, 0.998415243940858, 0.9942687386376107, 0.9919211984351741, 0.9952747080175971, 0.9972444611759695, 0.9984250742966374, 0.9981702576040093, 0.9953593206757743, 0.9171146124307812, 0.08215921514490539, 0.9084725069924892, 0.9765948663132847, 0.9994256312110144, 0.5502649898056499, 0.4450672711663345, 0.9771144040245167, 0.022252435441754797, 0.020518599421665444, 0.9790303152623226, 0.9956310259538966, 0.9927883118057623, 0.9910312946319685, 0.9948827034696303, 0.871141972137914, 0.11512424219572336, 0.013519177065185862, 0.8160176145131623, 0.09827949371873854, 0.08537410565466175, 0.9401201984187891, 0.9907482481166296, 0.26802633007022947, 0.04036823885471057, 0.6909540417922553, 0.9915790788348794, 0.1718937302866715, 0.8198008675210486, 0.9555389874909136, 0.2045123604577367, 0.7773695345814938, 0.003338472631414082, 0.014714009005121324, 0.9917750097426781, 0.9981356015104353, 0.8466644067564458, 0.9949906376197204, 0.23987501197120223, 0.7523352648187707, 0.7373246052665535, 0.2601236931007107, 0.00215232370177893, 0.9839324722631797, 0.9900968427009547, 0.9933020552754156, 0.9584845379075309, 0.998876480632258, 0.9982348843558823, 0.973250030130689, 0.9400589354595447, 0.9382008302694144, 0.9802461539602958, 0.9633019500839352, 0.821568042592533, 0.8811991174512818, 0.8912682708814543, 0.9631652975067357, 0.9993370671515107, 0.97100561509482, 0.9916737534499174, 0.9444477564150777, 0.999454834126525, 0.6843499833818583, 0.3101730140507703, 0.9188858940255723, 0.9479193485556456, 0.9964676463900395, 0.998066629957297, 0.9882973605181624, 0.9753034260113055, 0.9859774121552425, 0.9981722388903983, 0.9719152336746131, 0.9980858545193277, 0.9964072551409637, 0.9993810657972936, 0.6976782593284426, 0.3018910533125706, 0.9729308090263917, 0.02677089490245575, 0.9924548132950802, 0.27993060765892813, 0.047834698431691296, 0.6718822692981946, 0.9891626303594958, 0.9873536498833773, 0.9806078510394742, 0.9638099595387564, 0.9899808286323378, 0.9989317662445365, 0.9612268533124493, 0.9991872306770496, 0.7994899866638662, 0.915494458376949, 0.9873459371980622, 0.9960993271801878, 0.0777935200990089, 0.9217861497938598, 0.8087770571931006, 0.19097028663409094, 0.14737289303315204, 0.8231916828420835, 0.029186821214375994, 0.9955638767447507, 0.9888869776198221, 0.9899030100629566, 0.9987518419396638, 0.9898849889712996, 0.9958509582636011, 0.11889359347275087, 0.8802372116750449, 0.9586638245465054, 0.9402116470052556, 0.3400907351948744, 0.4813960406631673, 0.1724403727748659, 0.9825267239478764, 0.9597374588882029, 0.9562479618147808, 0.9957046007476796, 0.07725512204646458, 0.9184775621079678, 0.9786599403548908, 0.9856702973077172, 0.9760454518434459], \"Term\": [\"0\", \"10\", \"10\", \"1080\", \"10x\", \"11\", \"144\", \"2014\", \"2020\", \"2021\", \"350\", \"3rd\", \"5000\", \"60fp\", \"67\", \"6th\", \"8\", \"8\", \"85\", \"9\", \"9\", \"95\", \">\", \"@\", \"absorb\", \"access\", \"accessori\", \"act\", \"actual\", \"actual\", \"ad\", \"add\", \"addict\", \"admit\", \"admit\", \"adult\", \"advanc\", \"alert\", \"alert\", \"alreadi\", \"alright\", \"also\", \"also\", \"also\", \"also\", \"amaz\", \"amazon\", \"ambiti\", \"ambush\", \"american\", \"anoth\", \"answer\", \"apocalyps\", \"apocalyps\", \"app\", \"appar\", \"ark\", \"around\", \"around\", \"artist\", \"artist\", \"ass\", \"assist\", \"aswel\", \"attract\", \"audienc\", \"audio\", \"audio\", \"averag\", \"averag\", \"aw\", \"awesom\", \"awhil\", \"babi\", \"bake\", \"ball\", \"bang\", \"banger\", \"bar\", \"base\", \"base\", \"base\", \"bastard\", \"batteri\", \"bb\", \"began\", \"benefit\", \"best\", \"better\", \"better\", \"better\", \"biggest\", \"bit\", \"bit\", \"bit\", \"block\", \"bloodborn\", \"board\", \"boi\", \"boomer\", \"bot\", \"bottl\", \"brain\", \"brain\", \"brainer\", \"breaker\", \"brother\", \"brrrrrr\", \"budget\", \"bug\", \"bug\", \"build\", \"build\", \"build\", \"bush\", \"busi\", \"buy\", \"buy\", \"c\", \"calm\", \"can\", \"cant\", \"captiv\", \"car\", \"card\", \"ceram\", \"charact\", \"charact\", \"cheap\", \"cheaper\", \"chines\", \"chore\", \"cinemat\", \"circuit\", \"clearli\", \"closest\", \"co\", \"coffe\", \"com\", \"combat\", \"combat\", \"combin\", \"come\", \"come\", \"complic\", \"comput\", \"comput\", \"con\", \"construct\", \"content\", \"cool\", \"copi\", \"corrupt\", \"cost\", \"count\", \"countless\", \"covid\", \"crash\", \"craze\", \"creativ\", \"cri\", \"cri\", \"cross\", \"cup\", \"cure\", \"customis\", \"dad\", \"daddi\", \"dang\", \"dark\", \"dark\", \"daryl\", \"data\", \"daughter\", \"daunt\", \"dc\", \"deaf\", \"dear\", \"deepli\", \"deliveri\", \"depress\", \"depress\", \"design\", \"desper\", \"dev\", \"develop\", \"differ\", \"digit\", \"dirti\", \"disgust\", \"dive\", \"divers\", \"divis\", \"dixon\", \"dlc\", \"do\", \"doesn\", \"don\", \"don\", \"dont\", \"don\\u00b4t\", \"doom\", \"download\", \"downsid\", \"dr\", \"dri\", \"drill\", \"drive\", \"driver\", \"drug\", \"ds\", \"ds\", \"dump\", \"e\", \"ear\", \"eargasm\", \"earli\", \"earth\", \"east\", \"easter\", \"eat\", \"end\", \"enjoy\", \"enjoy\", \"enjoy\", \"enter\", \"enthusiast\", \"entiti\", \"epic\", \"euro\", \"ever\", \"everybodi\", \"everyth\", \"everyth\", \"everyth\", \"everywher\", \"evil\", \"evolut\", \"ex\", \"exceed\", \"excus\", \"exot\", \"expans\", \"expens\", \"express\", \"ez\", \"faction\", \"factorio\", \"fan\", \"fan\", \"far\", \"farcri\", \"faster\", \"favorit\", \"feel\", \"feel\", \"feel\", \"fellow\", \"fi\", \"fiction\", \"fight\", \"filedetail\", \"find\", \"first\", \"first\", \"first\", \"fist\", \"fix\", \"flesh\", \"focu\", \"foe\", \"foot\", \"forest\", \"forgot\", \"fragil\", \"friend\", \"fsr\", \"fuck\", \"fun\", \"funni\", \"futur\", \"g\", \"ga\", \"game\", \"geforc\", \"gen\", \"get\", \"get\", \"get\", \"get\", \"gimmick\", \"girl\", \"girlfriend\", \"give\", \"give\", \"glitch\", \"glori\", \"go\", \"go\", \"go\", \"god\", \"gon\", \"good\", \"good\", \"goofi\", \"grandma\", \"graphic\", \"graphic\", \"graphic\", \"great\", \"greatest\", \"greatli\", \"grim\", \"grindi\", \"group\", \"grown\", \"gta\", \"gta\", \"guess\", \"guy\", \"h\", \"haha\", \"hang\", \"happi\", \"harm\", \"hate\", \"haul\", \"haunt\", \"healthi\", \"hear\", \"heck\", \"hello\", \"help\", \"herb\", \"hesit\", \"hey\", \"highli\", \"hilari\", \"hill\", \"histori\", \"hope\", \"horribl\", \"horrifi\", \"horror\", \"hot\", \"hour\", \"hour\", \"hour\", \"hr\", \"http\", \"http\", \"hurt\", \"id\", \"im\", \"imagin\", \"immedi\", \"implor\", \"improv\", \"inclus\", \"inconveni\", \"induc\", \"infin\", \"infrastructur\", \"insan\", \"intellectu\", \"introvert\", \"intuit\", \"irl\", \"isol\", \"it\", \"it\", \"itch\", \"item\", \"ive\", \"j\", \"japanes\", \"jerk\", \"joke\", \"justic\", \"keep\", \"keep\", \"kid\", \"kid\", \"kill\", \"killer\", \"kilomet\", \"kinda\", \"know\", \"know\", \"know\", \"kojima\", \"l\", \"laugh\", \"laugh\", \"lead\", \"leaderboard\", \"legend\", \"length\", \"let\", \"level\", \"level\", \"licens\", \"licens\", \"lie\", \"lie\", \"like\", \"like\", \"like\", \"limb\", \"linux\", \"listen\", \"local\", \"logist\", \"lol\", \"lone\", \"longer\", \"look\", \"look\", \"look\", \"look\", \"lord\", \"lot\", \"lot\", \"lot\", \"love\", \"love\", \"lover\", \"lurk\", \"macbook\", \"mach\", \"mad\", \"make\", \"make\", \"make\", \"mama\", \"man\", \"mani\", \"mani\", \"mani\", \"map\", \"master\", \"master\", \"master\", \"masterpiec\", \"masterpiec\", \"mean\", \"meh\", \"mehh\", \"men\", \"mile\", \"million\", \"minimum\", \"mmo\", \"mob\", \"mode\", \"mode\", \"mommi\", \"motion\", \"mouth\", \"ms\", \"much\", \"much\", \"much\", \"much\", \"multiplay\", \"multiplay\", \"murder\", \"na\", \"narr\", \"nasa\", \"nation\", \"nativ\", \"neat\", \"need\", \"need\", \"need\", \"need\", \"netflix\", \"network\", \"new\", \"new\", \"newest\", \"nice\", \"nonetheless\", \"nonsens\", \"north\", \"nostalg\", \"not\", \"notch\", \"nuanc\", \"nut\", \"nvidia\", \"object\", \"obvious\", \"odd\", \"oddli\", \"ok\", \"old\", \"omg\", \"one\", \"one\", \"one\", \"onlin\", \"op\", \"open\", \"opposit\", \"option\", \"out\", \"outstand\", \"outta\", \"overpr\", \"owner\", \"p\", \"paint\", \"paint\", \"panel\", \"part\", \"pass\", \"path\", \"paus\", \"pay\", \"peac\", \"peep\", \"perfect\", \"person\", \"perspect\", \"philosophi\", \"photo\", \"physic\", \"physic\", \"pipe\", \"piss\", \"pizza\", \"pl\", \"plagu\", \"planet\", \"platform\", \"platinum\", \"play\", \"playabl\", \"player\", \"player\", \"player\", \"playstat\", \"pleas\", \"pocket\", \"pod\", \"point\", \"polit\", \"poor\", \"potato\", \"pov\", \"prefer\", \"press\", \"press\", \"pretend\", \"pretti\", \"pretti\", \"pretti\", \"pride\", \"primal\", \"prime\", \"prison\", \"pro\", \"profit\", \"project\", \"prologu\", \"promis\", \"properti\", \"proud\", \"prove\", \"ps\", \"ps4\", \"ps5\", \"pump\", \"purchas\", \"pure\", \"puzzl\", \"pve\", \"q\", \"question\", \"race\", \"rage\", \"rais\", \"rais\", \"react\", \"realist\", \"realli\", \"realli\", \"realli\", \"rebuild\", \"recommend\", \"recommend\", \"reconsid\", \"redempt\", \"reinstal\", \"relax\", \"releas\", \"reluct\", \"remark\", \"replay\", \"resolut\", \"respons\", \"review\", \"review\", \"rich\", \"rich\", \"rid\", \"rip\", \"rise\", \"rock\", \"roll\", \"rpg\", \"rx\", \"sacrific\", \"sadli\", \"san\", \"sandbox\", \"scam\", \"scare\", \"scaveng\", \"sci\", \"scienc\", \"scratch\", \"screen\", \"script\", \"see\", \"see\", \"self\", \"seri\", \"session\", \"set\", \"sh\", \"sharedfil\", \"sharp\", \"sharp\", \"shit\", \"shitti\", \"shooter\", \"short\", \"short\", \"short\", \"shower\", \"shred\", \"shut\", \"silent\", \"sim\", \"simp\", \"simul\", \"sinc\", \"sinc\", \"size\", \"skeleton\", \"skill\", \"skin\", \"slaughter\", \"slave\", \"slowli\", \"sluggish\", \"snipe\", \"sniper\", \"social\", \"solar\", \"sold\", \"solo\", \"somebodi\", \"someth\", \"someth\", \"sometim\", \"soni\", \"soon\", \"sorri\", \"sort\", \"soul\", \"soul\", \"soul\", \"space\", \"spare\", \"sport\", \"stabl\", \"stain\", \"start\", \"start\", \"start\", \"steam\", \"steam\", \"steamcommun\", \"still\", \"still\", \"still\", \"stock\", \"strategi\", \"stress\", \"stuck\", \"stun\", \"suck\", \"super\", \"support\", \"surprisingli\", \"surviv\", \"surviv\", \"suspens\", \"swap\", \"system\", \"t\", \"t\", \"take\", \"take\", \"team\", \"team\", \"teen\", \"tend\", \"terrarium\", \"that\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"thorough\", \"thoroughli\", \"though\", \"though\", \"though\", \"thousand\", \"throughout\", \"throughout\", \"til\", \"time\", \"time\", \"time\", \"time\", \"tl\", \"ton\", \"torment\", \"train\", \"trash\", \"trash\", \"tri\", \"tri\", \"tri\", \"trial\", \"truck\", \"trust\", \"truth\", \"turn\", \"type\", \"ubisoft\", \"undeni\", \"underr\", \"underwhelm\", \"unexpect\", \"unforgett\", \"uniron\", \"unpredict\", \"unreal\", \"updat\", \"url\", \"usag\", \"usd\", \"use\", \"v\", \"v\", \"va\", \"van\", \"vehicl\", \"version\", \"virtual\", \"vision\", \"visit\", \"voic\", \"void\", \"vr\", \"w\", \"wait\", \"want\", \"want\", \"way\", \"way\", \"weak\", \"well\", \"well\", \"well\", \"wheel\", \"whilst\", \"wholesom\", \"wich\", \"wife\", \"win\", \"winter\", \"without\", \"witti\", \"woke\", \"wont\", \"word\", \"work\", \"work\", \"world\", \"world\", \"would\", \"would\", \"would\", \"wow\", \"www\", \"xd\", \"ye\", \"yea\", \"yeah\", \"year\", \"year\", \"yell\", \"youtu\", \"youtub\", \"youtub\", \"youtub\", \"z\", \"zen\", \"\\u2013\", \"\\u2018\", \"\\u2019\", \"\\u2019\", \"\\u201c\", \"\\u201d\", \"\\ud83d\\udc4d\"]}, \"R\": 25, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 3, 25, 7, 1, 11, 8, 4, 14, 5, 19, 21, 6, 22, 20, 16, 23, 24, 15, 9, 10, 13, 17, 12, 18]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el101225321843237923524736060\", ldavis_el101225321843237923524736060_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el101225321843237923524736060\", ldavis_el101225321843237923524736060_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el101225321843237923524736060\", ldavis_el101225321843237923524736060_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "1     -0.147657  0.511844       1        1  46.150330\n",
       "2     -0.508886  0.226886       2        1  24.125327\n",
       "24    -0.339347  0.405456       3        1  14.967552\n",
       "6      0.159099  0.451143       4        1   2.155133\n",
       "0     -0.232856 -0.382480       5        1   1.294641\n",
       "10     0.450959  0.088113       6        1   1.010560\n",
       "7      0.038743  0.298161       7        1   0.919349\n",
       "3      0.304285  0.251103       8        1   0.854175\n",
       "13    -0.393182 -0.102192       9        1   0.838558\n",
       "4      0.295410 -0.313346      10        1   0.761741\n",
       "18    -0.003692 -0.394500      11        1   0.596137\n",
       "20    -0.262529 -0.202796      12        1   0.580052\n",
       "5      0.310707 -0.156594      13        1   0.561754\n",
       "21    -0.003377  0.160343      14        1   0.538945\n",
       "19     0.138609 -0.309684      15        1   0.501105\n",
       "15    -0.239799  0.013712      16        1   0.493677\n",
       "22     0.067549 -0.237450      17        1   0.467838\n",
       "23     0.310315 -0.020703      18        1   0.463972\n",
       "14     0.068902  0.051011      19        1   0.451712\n",
       "8      0.175763 -0.103799      20        1   0.450335\n",
       "9     -0.097344 -0.250115      21        1   0.432031\n",
       "12    -0.135163  0.103358      22        1   0.421946\n",
       "16     0.204356  0.085917      23        1   0.399418\n",
       "11    -0.035745 -0.116338      24        1   0.345742\n",
       "17    -0.125119 -0.057051      25        1   0.217971, topic_info=        Term          Freq         Total Category  logprob  loglift\n",
       "57      game  41602.000000  41602.000000  Default  25.0000   25.000\n",
       "256     best   5303.000000   5303.000000  Default  24.0000   24.000\n",
       "144     play  16413.000000  16413.000000  Default  23.0000   23.000\n",
       "174      fun  15550.000000  15550.000000  Default  22.0000   22.000\n",
       "60      good  16229.000000  16229.000000  Default  21.0000   21.000\n",
       "...      ...           ...           ...      ...      ...      ...\n",
       "2828   choos      0.033596      1.511724  Topic25 -11.1737    2.322\n",
       "2861   broke      0.033596      1.511723  Topic25 -11.1737    2.322\n",
       "2879    ship      0.033596      1.511724  Topic25 -11.1737    2.322\n",
       "2909   clean      0.033596      1.511723  Topic25 -11.1737    2.322\n",
       "2939  specif      0.033596      1.511723  Topic25 -11.1737    2.322\n",
       "\n",
       "[803 rows x 6 columns], token_table=      Topic      Freq  Term\n",
       "term                       \n",
       "988      16  0.994860     0\n",
       "1         4  0.040047    10\n",
       "1         9  0.959548    10\n",
       "1184     24  0.824252  1080\n",
       "1150     14  0.950220   10x\n",
       "...     ...       ...   ...\n",
       "1266      4  0.077255     ’\n",
       "1266     12  0.918478     ’\n",
       "1267     12  0.978660     “\n",
       "1268     12  0.985670     ”\n",
       "2387     15  0.976045     👍\n",
       "\n",
       "[825 rows x 3 columns], R=25, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 3, 25, 7, 1, 11, 8, 4, 14, 5, 19, 21, 6, 22, 20, 16, 23, 24, 15, 9, 10, 13, 17, 12, 18])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_lda = lda_vis(pos_fd)\n",
    "positive_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "59fa385c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el101225321843242404857111369\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el101225321843242404857111369_data = {\"mdsDat\": {\"x\": [-0.22304030813224918, -0.5335300314577315, -0.4162661057697962, 0.38461160172472497, -0.17592881153094378, 0.3975601843716304, 0.09446053636607879, 0.24323756661250046, -0.3162287805563549, 0.25187981066421594, 0.01549040356639768, -0.23861480806607677, 0.29191615829114154, -0.04391217805085635, 0.13499820617181987, -0.18189567884176616, 0.0031893527897657735, 0.1924688516940398, 0.08811647525486135, 0.13559189116489337, -0.037970315754832465, -0.0709416435003792, 0.06544468262931524, -0.06924443348772173, 0.008607373847323385], \"y\": [0.5032324081877863, 0.0898891781686776, 0.3393720440055546, 0.17685944014369903, -0.3930908249405936, -0.024293061969823934, 0.3443084698468036, 0.2581736703997725, -0.10900353903895438, -0.2996323483717144, -0.3503227196261095, -0.2317412452335086, -0.1350097004512347, 0.19742303406501394, -0.22660689588214034, 0.03286888524020841, -0.22967664222587367, 0.042498136618690854, 0.1987600942415272, -0.07665120436229088, -0.12314635219865416, 0.08249729019179736, 0.005545510447843019, -0.050794611839003787, -0.0214590154174725], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [70.21736216094267, 13.017304959332792, 9.51265099867447, 0.7724320654025497, 0.7638164935141672, 0.568833579753197, 0.5227060126722969, 0.45964871115421, 0.4557416357700187, 0.4213834282754947, 0.38443163508196654, 0.35107402786005565, 0.3007293677512691, 0.2872978527744702, 0.28167356740297583, 0.26083507462542493, 0.24214329429812326, 0.23846919098715805, 0.19874088006044677, 0.1533241116541697, 0.14686266420127328, 0.14466795242703825, 0.12032270001958603, 0.09060787229832129, 0.08693976306586335]}, \"tinfo\": {\"Term\": [\"game\", \"shit\", \"fuck\", \"enemi\", \"play\", \"charact\", \"unplay\", \"kill\", \"point\", \"design\", \"level\", \"difficulti\", \"mechan\", \"mission\", \"interest\", \"stori\", \"get\", \"2k\", \"feel\", \"like\", \"fight\", \"build\", \"end\", \"combat\", \"limit\", \"game\", \"play\", \"get\", \"like\", \"time\", \"even\", \"good\", \"fun\", \"hour\", \"want\", \"player\", \"go\", \"tri\", \"would\", \"still\", \"bad\", \"buy\", \"work\", \"new\", \"everi\", \"2\", \"start\", \"recommend\", \"year\", \"peopl\", \"make\", \"realli\", \"much\", \"one\", \"thing\", \"way\", \"launcher\", \"resourc\", \"order\", \"useless\", \"specif\", \"food\", \"fire\", \"b\", \"explain\", \"rare\", \"ground\", \"unit\", \"import\", \"water\", \"outsid\", \"flaw\", \"tank\", \"process\", \"essenti\", \"tool\", \"materi\", \"equip\", \"per\", \"immedi\", \"size\", \"often\", \"exampl\", \"interact\", \"tier\", \"hand\", \"choos\", \"small\", \"result\", \"farm\", \"larg\", \"addit\", \"item\", \"core\", \"activ\", \"whatev\", \"sens\", \"effect\", \"simpli\", \"sort\", \"speed\", \"deal\", \"wall\", \"build\", \"less\", \"place\", \"sever\", \"move\", \"area\", \"exist\", \"part\", \"becom\", \"requir\", \"mean\", \"pick\", \"random\", \"enough\", \"enemi\", \"difficulti\", \"fight\", \"combat\", \"skill\", \"challeng\", \"boss\", \"strategi\", \"weapon\", \"gun\", \"cover\", \"quickli\", \"damag\", \"element\", \"rng\", \"zombi\", \"quest\", \"shoot\", \"unlock\", \"explor\", \"tediou\", \"tree\", \"pace\", \"rang\", \"gear\", \"design\", \"surviv\", \"repetit\", \"attack\", \"hit\", \"charact\", \"difficult\", \"interest\", \"limit\", \"level\", \"shot\", \"kill\", \"stori\", \"mission\", \"type\", \"mechan\", \"gener\", \"point\", \"end\", \"tactic\", \"accur\", \"welcom\", \"tile\", \"structur\", \"field\", \"plant\", \"navig\", \"color\", \"unnecessari\", \"defeat\", \"smaller\", \"dynam\", \"escap\", \"unrealist\", \"bonu\", \"repeatedli\", \"sent\", \"statu\", \"shown\", \"conflict\", \"intro\", \"shock\", \"injuri\", \"punch\", \"act\", \"creativ\", \"voic\", \"gorgeou\", \"flat\", \"text\", \"written\", \"fell\", \"terrain\", \"bind\", \"laptop\", \"angl\", \"overr\", \"strang\", \"soundtrack\", \"rain\", \"perspect\", \"explos\", \"innov\", \"gem\", \"closer\", \"express\", \"context\", \"hade\", \"knight\", \"unlik\", \"remain\", \"visual\", \"combin\", \"impress\", \"skip\", \"publish\", \"warn\", \"mmo\", \"suspect\", \"report\", \"pve\", \"deserv\", \"offici\", \"daili\", \"greedi\", \"f2p\", \"p2w\", \"studio\", \"unaccept\", \"amazon\", \"lvl\", \"incompet\", \"outdat\", \"2016\", \"diablo\", \"staff\", \"news\", \"prove\", \"guild\", \"wife\", \"ark\", \"pvp\", \"stress\", \"wasteland\", \"excus\", \"intend\", \"disappear\", \"excel\", \"trust\", \"rise\", \"sum\", \"club\", \"relic\", \"prior\", \"disast\", \"monet\", \"adapt\", \"entri\", \"redo\", \"arbitrari\", \"reflect\", \"fantasi\", \"march\", \"irl\", \"steel\", \"format\", \"150\", \"laughabl\", \"r\", \"bodi\", \"e\", \"stare\", \"fool\", \"camp\", \"sake\", \"g\", \"smash\", \"surround\", \"meter\", \"shift\", \"packag\", \"grass\", \"up\", \"protect\", \"mountain\", \"murder\", \"borderlin\", \"gift\", \"meanwhil\", \"breaker\", \"barren\", \"mindlessli\", \"waster\", \"potato\", \"unplay\", \"keyboard\", \"mous\", \"input\", \"camera\", \"port\", \"idk\", \"intuit\", \"laggi\", \"everytim\", \"glitchi\", \"tab\", \"display\", \"linux\", \"trial\", \"zoom\", \"induc\", \"cat\", \"sensit\", \"bat\", \"panel\", \"ear\", \"backup\", \"backpack\", \"hotfix\", \"optim\", \"fp\", \"frame\", \"xbox\", \"ram\", \"stutter\", \"gpu\", \"monitor\", \"hardwar\", \"spec\", \"resolut\", \"gb\", \"lowest\", \"ultra\", \"rtx\", \"medium\", \"@\", \"4k\", \"bet\", \"framer\", \"nvidia\", \"fund\", \"ti\", \"3080\", \"aswel\", \"cking\", \"elder\", \"it\", \"lmao\", \"your\", \"killer\", \"peac\", \"k\", \"paywal\", \"commit\", \"\\u201d\", \"\\u201c\", \"tip\", \"histor\", \"silver\", \"bandit\", \"starter\", \"brick\", \"afterward\", \"xd\", \"swear\", \"border\", \"sneak\", \"hone\", \"resist\", \"shell\", \"shit\", \"fuck\", \"tbh\", \"pl\", \"etern\", \"woke\", \"cod\", \"rainbow\", \"poo\", \"religion\", \"asham\", \"happend\", \"hatr\", \"commerc\", \"mw\", \"likeabl\", \"meeh\", \"dick\", \"retard\", \"cunt\", \"hirez\", \"nut\", \"poop\", \"cock\", \"warthund\", \"microsoft\", \"vehicl\", \"april\", \"oper\", \"internet\", \"recomend\", \"bro\", \"ur\", \"children\", \"adult\", \"usag\", \"languag\", \"rich\", \"clue\", \"girl\", \"lay\", \"english\", \"spare\", \"si\", \"que\", \"evil\", \"reviv\", \"la\", \"statement\", \"leak\", \"coffe\", \"ms\", \"teen\", \"nasa\", \"infin\", \"16gb\", \"de\", \"w\", \"tire\", \"road\", \"#\", \"infuri\", \"transfer\", \"steer\", \"netcod\", \"rez\", \"21\", \"bitch\", \"asshol\", \"pump\", \"incomplet\", \"mouth\", \"ck\", \"bite\", \"nich\", \"bruh\", \"american\", \"insta\", \"sour\", \"span\", \"darn\", \"26\", \"desk\", \"bia\", \"polit\", \"inventori\", \"litter\", \"unbalanc\", \"robot\", \"overhaul\", \"organ\", \"convinc\", \"bewar\", \"hint\", \"buyer\", \"iv\", \"roleplay\", \"transit\", \"dust\", \"unlimit\", \"swim\", \"perma\", \"proud\", \"unpleas\", \"incorpor\", \"altogeth\", \"torrent\", \"conduct\", \"pose\", \"vendor\", \"inform\", \"outfit\", \"expans\", \"respond\", \"pathet\", \"email\", \"l\", \"countless\", \"ps\", \"unpolish\", \"websit\", \"revert\", \"hardest\", \"jesu\", \"yesterday\", \"thru\", \"predecessor\", \"christ\", \"furri\", \"z\", \"videogam\", \"shouldnt\", \"barrel\", \"hoop\", \"costum\", \"loser\", \"amazingli\", \"cup\", \"contact\", \"file\", \"optimis\", \"softwar\", \"script\", \"sh\", \"corrupt\", \"bethesda\", \"chose\", \"verifi\", \"web\", \"theori\", \"insist\", \"keybind\", \"mp\", \"chip\", \"albeit\", \"rebind\", \"clich\", \"captiv\", \"behold\", \"sin\", \"borrow\", \"wherea\", \"burden\", \"bread\", \"child\", \"micro\", \"overwhelm\", \"gay\", \"un\", \"cooki\", \"transact\", \"crack\", \"femal\", \"women\", \"colour\", \"rubbish\", \"backward\", \"toy\", \"reskin\", \"assassin\", \"male\", \"tap\", \"programm\", \"aka\", \"creed\", \"hyper\", \"ladder\", \"substanc\", \"joe\", \"buf\", \"drown\", \"2k\", \"data\", \"amus\", \"720p\", \"id\", \"boy\", \"startup\", \"hill\", \"revers\", \"4x\", \"brother\", \"honor\", \"doll\", \"cach\", \"dumbest\", \"promin\", \"cum\", \"smarter\", \"ray\", \"trace\", \"bork\", \"shortag\", \"re\", \"tragic\", \"tilt\", \"decept\", \"hacker\", \"tester\", \"request\", \"cpu\", \"site\", \"swarm\", \"deni\", \"ryzen\", \"pound\", \"loyal\", \"i9\", \"cent\", \"32gb\", \"sand\", \"geforc\", \"remap\", \"limp\", \"03\", \"gimp\", \"aud\", \"dx12\", \"noodl\", \"weigh\", \"shader\", \"fkin\", \"contact\", \"ea\", \"\\u2019\", \"v\", \"gud\", \"gtx\", \"i\", \"git\", \"m\", \"isn\", \"t\", \"don\", \"copypast\", \"they\", \"missabl\", \"rat\", \"weren\", \"fuckeri\", \"stanc\", \"breathtak\", \"didn\", \"eagl\", \"adam\", \"cheapest\", \"mech\", \"choir\", \"civ\", \"com\", \"june\", \"modifi\", \"oper\", \"autom\", \"rocket\", \"chines\", \"sunk\", \"complic\", \"cancer\", \"nah\", \"2d\", \"heat\", \"\\u043d\\u0435\", \"\\u0432\", \"\\u043d\\u0430\", \"viru\", \"\\u0438\", \"\\u0438\\u0433\\u0440\\u0430\", \"infrastructur\", \"\\u0442\\u043e\", \"\\u0447\\u0442\\u043e\", \"\\u044d\\u0442\\u043e\", \"\\u0438\\u0437\", \"\\u044f\", \"tinker\", \"\\u0437\\u0430\", \"\\u0443\", \"\\u043d\\u043e\", \"\\u043d\\u0435\\u0442\", \"\\u043e\\u0442\", \"bob\", \"straw\", \"\\u0438\\u0433\\u0440\\u0443\", \"dog\", \"ball\", \"whenev\", \"anytim\", \"hello\", \"da\", \"unclear\", \"farmer\", \"tf\", \"rebel\", \"hive\", \"molotov\", \"preview\", \"clarifi\", \"restaur\", \"discern\", \"an\", \"albert\", \"nap\", \"prelud\", \"thr\", \"alla\", \"tran\", \"striker\", \"cock\", \"oper\", \"internet\", \"complic\", \"anger\", \"starv\", \"utterli\", \"librari\", \"refin\", \"mask\", \"disregard\", \"\\u0441\", \"tiger\", \"creepi\", \"anxieti\", \"\\u0435\\u0441\\u043b\\u0438\", \"puddl\", \"recreat\", \"recours\", \"\\u0434\\u043e\", \"cardboard\", \"undoubtedli\", \"mcdonald\", \"visitor\", \"10hr\", \"\\u0435\\u0441\\u0442\\u044c\", \"dinner\", \"tissu\", \"\\u0432\\u0430\\u0441\", \"demo\", \"imag\", \"somebodi\", \"jank\", \"editor\", \"elev\", \"ramp\", \"bell\", \"34\", \"whistl\", \"superb\", \"temporari\", \"famou\", \"arc\", \"yr\", \"geometri\", \"tendenc\", \"unfulfil\", \"shockingli\", \"devoid\", \"5600x\", \"dl\", \"humili\", \"farcri\", \"crow\", \"marvel\"], \"Freq\": [23950.0, 1170.0, 1093.0, 1869.0, 12210.0, 1725.0, 696.0, 1760.0, 2540.0, 1457.0, 1505.0, 1341.0, 1687.0, 1518.0, 1306.0, 1457.0, 8813.0, 430.0, 3540.0, 8486.0, 1140.0, 2361.0, 1951.0, 1079.0, 1176.0, 23950.011020322057, 12209.40014979022, 8812.617395837886, 8485.201170936065, 7389.333948708332, 6180.310810684049, 5751.264381965866, 5728.115132794762, 4378.410510306397, 4360.422899684478, 4220.68536056971, 4172.840340753795, 4054.358725377573, 3872.939834064259, 3685.4190952320205, 3424.516880534396, 3371.2687161803206, 3334.5672786152045, 3272.3385561141217, 3227.8613729799117, 3156.006717677739, 3053.7708806991504, 2979.638732209804, 2851.272694499987, 2838.596687116611, 6022.38695348808, 4345.500818669456, 4048.5055169359066, 5414.061559941175, 3690.7074310159287, 3983.5261397249465, 1105.3147832540776, 739.5241449925303, 503.1095643939385, 473.49510224143336, 450.50113496569753, 428.011716185735, 402.62047449700844, 401.8783395457528, 378.0436260308511, 363.93528514086546, 352.35091464808215, 351.35349862327087, 340.0364994768905, 375.69962438259347, 329.3556879689248, 326.4883412288267, 324.83241463995313, 317.39969901151676, 314.2230113059914, 310.35214043448724, 299.0148019228483, 282.4376210204402, 285.7275605786518, 269.1533707593699, 267.12122888338274, 750.4240926958522, 715.0431056416702, 397.3009118696529, 335.8938808320935, 594.6008150602551, 419.3650930843166, 771.0807016763792, 398.64809570972443, 417.3320696259343, 476.08345963856436, 379.0380824576217, 926.8281002558258, 490.4483410055273, 441.62045173837714, 455.1434534030947, 617.7692411949662, 604.3242910573014, 627.2223862475689, 424.2783721678682, 448.68331206697894, 526.4201771749468, 512.8867854320011, 1186.0157172849215, 744.8554515567922, 737.3950605381665, 569.8512708999125, 723.300321205134, 507.2887372230471, 520.6008202997344, 607.6466602536188, 569.7706061107599, 581.788967022622, 573.8741202793167, 536.8246126020452, 536.5198369740212, 529.9647681050033, 1868.7187404619688, 1341.0393970932635, 1139.42289269416, 1078.5069214646871, 875.4454930626486, 790.930323563582, 727.6123272376375, 691.464976369688, 686.7114525001595, 680.4228579359202, 679.6014600552709, 657.398638182781, 651.1645273771239, 631.4558560104649, 585.6019750610149, 581.1659203219401, 561.0034783949964, 558.8273492198753, 505.00876186278134, 463.4716603971319, 451.37815267613297, 437.25461876438607, 427.4728974147073, 426.07560068384345, 507.6189664557268, 1454.2058450031416, 976.7235296299061, 569.6603218067213, 835.0300483667935, 991.8171873578912, 1628.2112322067949, 940.9840246676259, 1221.9529022732704, 1086.02945351742, 1343.1979478935716, 791.4203092573161, 1232.5251443059565, 1028.3485474871302, 1029.5135979937006, 814.1034993357498, 910.2223289121453, 745.8647544961154, 928.5674337802382, 696.3498318667445, 471.6107660749765, 266.2954435126276, 259.5795568075472, 233.795357996518, 173.64058740864508, 141.08485676841173, 136.69188122528098, 132.88236491137886, 122.49574588381482, 117.05436119976024, 112.4471078371291, 108.09412115502253, 102.85575971661252, 102.5297955715643, 98.1318503792874, 89.01597437497395, 82.24515249564784, 75.52684448782556, 72.90294447022086, 68.33130069884001, 64.02546963494734, 62.774111979945054, 56.48310785739123, 55.5937723280841, 54.651041094793065, 284.50736020740044, 242.45628122361572, 224.3592908365272, 207.14705192339187, 152.1770022666663, 140.86351048907957, 111.2326791984807, 110.38340616552154, 99.69314075737265, 91.5738614784108, 90.05667594131462, 87.05938689522222, 84.38158856313423, 81.83842503972566, 80.77249039220631, 78.36311185020904, 69.17722545171567, 65.97988706670452, 63.49470393238968, 63.2324176782342, 63.041339559346014, 61.635939562119944, 59.00462678159888, 57.213674932425064, 54.361584417445115, 194.0599336881978, 155.42483517100928, 141.34916829831235, 121.19441355396643, 103.85470794041333, 101.17220969404244, 281.30246664030693, 275.4357160254403, 195.99246729573574, 192.5959170386309, 168.55544101086048, 158.7236306137949, 148.1148098700857, 148.04698629080107, 114.24779567739468, 106.13433671667364, 101.40599200220439, 100.36873811310615, 88.97609384593132, 88.93804351915642, 81.4930958896947, 78.07400381331448, 73.82605960865634, 73.7131799076902, 70.2330113883612, 64.66093892610687, 63.41862382411314, 61.797583044122895, 61.74260433939608, 61.612570622150315, 59.896580133185914, 68.7113780501608, 64.08361299611067, 294.10267835272913, 188.8384150285282, 143.23233076496265, 138.73666254424526, 128.0091741257688, 114.9928537114136, 111.25293829736371, 100.22469420506876, 78.88123141444638, 73.5427676331981, 63.19857176896101, 63.017841235753366, 60.543408722239256, 58.186614399543785, 57.671817094464075, 55.54983895757728, 54.7077159909497, 53.39435249585323, 52.89505519313184, 51.246770276767364, 50.70292500641974, 49.650381289995394, 48.92183060818446, 48.53693495419056, 48.006627532558504, 53.618609463089584, 241.50964374893093, 232.45368092968994, 221.56531144557928, 198.70387126472377, 129.04803049967646, 119.03144743516266, 96.64189025941974, 84.64224412503775, 72.89832431952524, 70.98035239435166, 69.25466550063598, 66.1278580311342, 64.74437881430167, 64.13485585431839, 63.91133490017584, 61.526767485845255, 56.55366083688572, 54.761508666910885, 53.34629546028941, 45.98644680044565, 41.55734949914169, 40.21538773307561, 36.159898571364934, 36.06977510765101, 36.05948913254146, 177.5656009598961, 695.3086331780714, 323.5241671424677, 249.86059848674807, 249.76530626443827, 156.45153955746983, 148.3989323049181, 143.99259737006918, 139.8188050876642, 132.11741043347743, 130.9088059326033, 126.66718209606108, 109.49739491194038, 85.32780051715918, 83.75757764128433, 61.653578410194115, 59.67698350133022, 57.67461224915598, 52.712965989378226, 51.9840766098654, 45.661936784151145, 41.16199836108205, 38.61764487837295, 29.35901693219645, 27.111212651309415, 26.68020846951721, 334.386615449708, 329.55535873124455, 325.3153709691783, 228.00321136043075, 143.31173638471466, 126.03429911790197, 111.82113629240307, 110.4885715055712, 97.51655491735477, 96.89886818531185, 95.64937262634382, 92.98876831980458, 76.90366148077094, 71.20557557574872, 61.64174557667382, 60.07258270194059, 51.49198317495488, 50.14534521504752, 45.75091946270159, 43.7567402381813, 38.076470024451055, 32.745896962196895, 32.67403490489635, 30.278317721012435, 27.236480043319997, 176.25727483179548, 93.64844237612263, 89.46406474154782, 82.63587187240768, 79.34839950148788, 72.48899414469454, 69.43873965539068, 67.52452003903336, 64.60303565787164, 63.17405269418195, 60.719483622244134, 59.98227084785003, 58.19302020017092, 54.8771657399659, 51.76181580751305, 51.283887607924385, 49.07307869188538, 49.002634874422135, 48.32229413872024, 46.62203553013527, 44.51153652734777, 43.816820640933344, 43.64206864708379, 40.227012629361376, 39.87816696033656, 42.48074615455261, 1169.6945415094299, 1092.7077081930004, 73.19782722849588, 63.03725558882094, 32.36194551268396, 28.751321277500825, 25.440917796778212, 25.12874785617245, 20.347130551435523, 19.81572972818286, 18.375837513048133, 8.996752157710336, 8.33033707940551, 3.634263193291745, 3.193329775526918, 2.2044779414951217, 0.5226106204028074, 0.03398681362583708, 0.03398681362583708, 0.033986810308859514, 0.033986810308859514, 0.033986810308859514, 0.033986810308859514, 0.033986810308859514, 0.033986810308859514, 0.03398681362583708, 0.03398681362583708, 0.033986810308859514, 0.033986810308859514, 0.033986810308859514, 0.033986810308859514, 159.31495625493585, 86.11117531940974, 78.16341641583834, 71.60795625447831, 65.8347698254359, 62.78568056099451, 58.67016914476719, 56.43296077117499, 53.34786924851136, 52.166806642670736, 51.646638893398936, 42.15792572964715, 38.101986500028886, 37.910572845092474, 32.19573079007064, 31.513104814489207, 30.111876790680636, 29.825264017509873, 29.54169749974227, 27.634375019293305, 27.41023521270958, 23.107656947250074, 21.94253117423734, 20.92912463164785, 19.54428005022245, 54.36823388068696, 43.3448393570883, 153.3048161829623, 141.52735033719102, 109.35821522916602, 95.3771803336551, 89.94597422775603, 79.25278167415661, 57.13974711945021, 55.795718316470435, 53.027803930286666, 48.61279967389705, 45.6296583400516, 44.45832642576342, 41.63681099015941, 41.623516351112684, 41.544198805673155, 41.11431472030266, 40.92542411070656, 39.068252259779335, 38.83418822894399, 34.61040385405537, 30.34292478424422, 23.453269257268282, 21.64445342463414, 21.342873863285536, 21.105167218092937, 30.20707097323698, 36.5919221345026, 205.40684131295936, 169.17635405587976, 158.14666076708113, 72.46474611717105, 68.13828143910729, 50.93840913171998, 45.31481278371448, 42.983387116439474, 41.49632362314944, 38.65122415665766, 35.69472886912868, 35.11862468450835, 34.436253404911746, 29.24813277200085, 28.91646334442609, 23.473906666136173, 22.792284801877905, 22.347707096855856, 22.268049696683935, 21.065794008575747, 20.85769090713278, 19.223744732675268, 18.056354571253227, 17.383718878188297, 17.366679898541186, 174.35734843020973, 22.023183003888814, 245.01848244448877, 75.97406719707381, 75.9740571029234, 74.27463117541518, 67.14380031357963, 66.64243900398284, 61.09833288033735, 55.47608793837464, 54.13132503256707, 48.745530516863454, 46.0798218531387, 36.24883856284475, 34.79235360030873, 32.56155645403733, 32.23047084415486, 31.959940042576875, 29.891146439759503, 26.845905291263566, 26.709826049597833, 26.351849623025394, 25.905622562963377, 20.829753794347056, 18.876489004731187, 18.866248489141107, 16.43561113837579, 66.49229356372196, 22.017174130652027, 266.9818608109673, 173.07602621048872, 118.56860252714426, 83.73257648162817, 78.83018184836429, 71.11298448640306, 69.96379291093406, 45.943294278125855, 40.98917288143223, 39.85982395479702, 32.96776452014265, 30.41368295217266, 27.304881043068622, 26.8557882521846, 22.543134946762706, 19.91165064945762, 17.917822874397988, 17.84910218243928, 17.462533648311442, 17.34447457314566, 17.06030769464634, 15.701432968779722, 14.343573802330983, 13.279857892192505, 13.231786908397364, 34.33082096125937, 119.21402157768662, 84.66366552120529, 82.65016847568987, 75.89572938248544, 55.211716913959926, 47.49030458567847, 41.50943946623314, 40.11355296946441, 39.86786443204469, 39.81576895664853, 39.35486934992173, 37.27995363077827, 35.536513254265145, 33.41570598945716, 33.03108541760848, 32.24632637455262, 32.109480004860515, 30.24044292818723, 28.397349767405434, 26.49677393304103, 24.343009012518515, 23.26864019355239, 21.999497051118826, 21.164089116505316, 17.69538756181316, 20.971035927876034, 429.5196741951669, 201.76643240349674, 104.25772906949453, 88.29639618797275, 72.04341509448447, 55.78651151713273, 39.49790504079032, 31.13150461335339, 28.578579064246224, 26.816272439118688, 23.081137172567544, 20.041133344141155, 12.90997019357408, 12.11602773363464, 11.515627164992084, 10.440638736234774, 9.875892392239466, 8.43290767957109, 6.41898813549829, 6.344773875547579, 6.174841395397081, 6.142298238833705, 6.138460838259917, 6.077750305845076, 6.02380653879128, 8.105931155219533, 119.71426576379478, 111.47900376670822, 94.20084123283135, 69.82462806324543, 61.5869333127222, 46.57932483028155, 32.22901072515859, 31.577991071644007, 22.859824550606504, 22.585108835543338, 18.83309249419255, 18.25013356822746, 16.016523251145035, 15.45537010293544, 12.849996492687966, 9.632486490790937, 8.524419218907461, 8.001372779865273, 6.275917187342898, 6.255287734877209, 6.174247748557911, 4.793722104624833, 4.758284748999674, 4.200968379304676, 4.071618211553268, 26.79410010933717, 217.09987947862126, 113.70943568248977, 101.53056680568218, 31.526782583286884, 30.364304050217974, 29.478256314916393, 21.657745581960558, 18.895827512289515, 15.78622045446908, 87.45593537569894, 38.37193662286525, 0.9426339934413067, 0.02646684965955889, 0.026466844109274624, 0.026466844109274624, 0.026466844109274624, 0.026466844109274624, 0.026466844109274624, 0.026466844109274624, 0.026466844109274624, 0.026466844109274624, 0.026466844109274624, 0.026466844109274624, 0.026466844109274624, 0.026466844109274624, 0.026466846884416757, 0.026466846884416757, 0.026466844109274624, 0.026466844109274624, 0.026466844109274624, 0.026466844109274624, 0.026466844109274624, 0.026466844109274624, 0.026466844109274624, 0.026466844109274624, 152.58192317630014, 63.22736112303401, 40.10472665966438, 32.34525522100454, 27.591887053144006, 23.11586020922199, 22.47583175655975, 21.905155430553577, 20.257854170987873, 17.28542202250875, 15.443547108223635, 14.45716531731348, 14.126139428894234, 12.71306780395806, 12.313728321233478, 12.138661439252829, 12.12472103195728, 11.93330076455081, 11.898964813569444, 11.645744873097945, 11.218553968207287, 10.30935598558194, 9.926810074838848, 9.86987624775404, 9.648596775624718, 125.44688188025644, 121.15796372205334, 72.5832851171441, 41.78781454774587, 20.20197617138912, 18.339231149946993, 18.28189442738817, 17.559981855592174, 10.546797575644948, 6.705459507851322, 6.149119032831726, 5.868427057488594, 4.437707383961304, 4.217894787223753, 3.944696142028652, 2.3244774014185063, 1.5579668847256254, 1.3662667901490004, 1.136862276821852, 1.0691970515850118, 0.4207191079640757, 0.282549847138952, 0.024720606529858125, 0.024720606529858125, 0.024720606529858125, 0.024720606529858125, 0.024720606529858125, 0.024720606529858125, 82.85684624988298, 46.74262060989075, 45.65275817698919, 39.493754694150155, 21.386895631741112, 11.547705494624783, 8.595525310626474, 8.186607305814212, 7.955769593082279, 7.397145624349655, 6.504913788354898, 6.295111369077195, 5.914720223724324, 5.378704198740016, 5.238138278460282, 4.896519449819942, 4.852652675368236, 4.4236776812980665, 3.985258895913409, 3.983808971952195, 3.6336120109524463, 3.469790765518497, 3.353229677640159, 2.453166805837351, 2.2018427808366234, 44.08308975182213, 29.525786674400237, 20.36957192322918, 18.885890585839356, 15.350630050096333, 14.11733073637551, 12.902156896253468, 11.859681835116731, 10.911087626213542, 10.356994362804729, 10.0125186814439, 9.06071388584404, 8.51570649026269, 8.201563577425656, 6.730320424677436, 5.608150560542861, 5.074306619674167, 4.8600279251871985, 4.738815870745571, 4.067621567430099, 3.5778775624306918, 2.835122376998563, 2.6942428505139135, 2.5152143708371955, 2.423988540062398, 4.934302497461082], \"Total\": [23950.0, 1170.0, 1093.0, 1869.0, 12210.0, 1725.0, 696.0, 1760.0, 2540.0, 1457.0, 1505.0, 1341.0, 1687.0, 1518.0, 1306.0, 1457.0, 8813.0, 430.0, 3540.0, 8486.0, 1140.0, 2361.0, 1951.0, 1079.0, 1176.0, 23950.868736126104, 12210.257865594267, 8813.475111641932, 8486.058886748537, 7390.191664512378, 6181.168526488095, 5752.122097769912, 5728.97285048902, 4379.268226110443, 4361.280615488524, 4221.543076373756, 4173.69805688453, 4055.2164411816216, 3873.797549868307, 3686.276811036069, 3425.3745963384445, 3372.126431984369, 3335.424994419253, 3273.19627191817, 3228.71908878396, 3156.8644334817873, 3054.628596503199, 2980.4964480138524, 2852.1304103040356, 2839.4544029206595, 6031.138825151662, 4350.502822065079, 4051.882427513658, 5435.953788060103, 3732.4282067158088, 4139.833068888566, 1106.1673664180425, 740.3767282463715, 503.962240193058, 474.37532470971104, 451.3537181605577, 428.86429934970016, 403.4730576817055, 402.7314805834641, 378.8985673563108, 364.7878683100136, 353.2035611582598, 352.20608178723603, 340.8890826408557, 376.64186820690446, 330.20827115880485, 327.34176348909915, 325.6849978039183, 318.252282325788, 315.07559446995657, 311.2047235984524, 299.8673867102049, 283.29020418440535, 286.62160456947356, 270.0063989017122, 267.9738120473479, 753.6883091973467, 718.5667572201693, 399.14979329583963, 337.69953419572283, 605.6186628654496, 423.81263696364596, 798.1952922479319, 404.14012770429736, 424.4583129109434, 495.4904239609253, 385.80874009761897, 1049.5882350206637, 517.1257514430486, 459.65904169072354, 480.2002591892582, 694.6002370577795, 692.97057672119, 744.484476858389, 444.38309377571915, 481.1143167259704, 604.7527797761561, 596.5557540911311, 2361.220864903441, 1104.9614837841928, 1120.1721269331697, 749.6737192400207, 1289.6335022146116, 658.4361930204784, 711.188882129414, 1243.432128888644, 1119.5826242692158, 1263.1494276316926, 1193.8313835915362, 934.6393369008406, 1049.2662913962577, 1766.898827049568, 1869.5810735012467, 1341.9017301325414, 1140.2852262408921, 1079.3692547126607, 876.3078261019264, 791.7926566028598, 728.4746602769153, 692.3273108705622, 687.5737880184265, 681.285190975198, 680.4637930945487, 658.2609717144406, 652.0268604164017, 632.3181890497427, 586.4643081002927, 582.0282533612179, 561.8658130098959, 559.6896822591531, 505.87109490205944, 464.33399343641, 452.24048571541107, 438.11695180366416, 428.33523046435135, 426.93793372312155, 508.67156561285645, 1457.8728734362126, 980.4856912998137, 570.9814714214198, 840.6510615593796, 1014.0780695894929, 1725.0048887975088, 980.8240844113508, 1306.4313987070157, 1176.8134594472117, 1505.0894450752482, 879.4261921495009, 1760.8667463558825, 1457.752813923717, 1518.6602386176544, 1162.750570476241, 1687.7420234783551, 993.8954753521886, 2540.030662704661, 1951.463600802446, 472.4706773073307, 267.1553547449818, 260.4394680399014, 234.655269228872, 174.50049864099907, 141.94476800076572, 137.55179245763497, 133.74227614373285, 123.35565711616883, 117.91427243211425, 113.3070190694831, 108.95403238737654, 103.71567094896653, 103.38970680391832, 98.99176161164141, 89.87588560732796, 83.10506372800185, 76.38675572017956, 73.76285570257487, 69.19121193119402, 64.88538086730135, 63.63402321229907, 57.343019089745255, 56.45368356043812, 55.51095232714708, 285.3687186588793, 243.31763967509437, 225.22064928800586, 208.00841037487052, 153.03836071814496, 141.72486894055822, 112.09403764995938, 111.24476461700021, 100.55449920885133, 92.43521992988948, 90.9180343927933, 87.9207453467009, 85.24294701461291, 82.69978349120434, 81.63384884368499, 79.22447030168772, 70.03858390319435, 66.8412455181832, 64.35606238386836, 64.0937761297129, 63.90269801082471, 62.49729801359864, 59.865985233077566, 58.07503338390376, 55.22294286892381, 352.1635885272683, 359.72989985007723, 514.6868251151717, 476.1345004025445, 345.3711594467761, 416.1338253540233, 282.1662753762964, 276.2995247614298, 196.85627603172503, 193.45972577462018, 169.41924974684977, 159.5874393497842, 148.978618606075, 148.91079502679037, 115.11160441338399, 106.99814545266295, 102.2698007381937, 101.23254684909546, 89.83990258192063, 89.80185225514573, 82.35690462568401, 78.93781254930379, 74.68986834464565, 74.57698864367951, 71.09682012435051, 65.52474766209617, 64.28243256010246, 62.661391780112226, 62.606413075385404, 62.47637935813964, 60.760388869175245, 82.8249407909996, 433.4954122035259, 294.9663838821705, 189.70212055796935, 144.0960362944038, 139.6003680736864, 128.87287965520994, 115.85655924085476, 112.11664382680488, 101.08839973450992, 79.74493694388754, 74.40647316263926, 64.06227729840218, 63.88154676519454, 61.407114251680426, 59.05031992898496, 58.53552262390525, 56.413544487018456, 55.57142152039087, 54.2580580252944, 53.758760722573015, 52.11047580620854, 51.56663053586091, 50.51408681943657, 49.78553613762563, 49.40064048363173, 48.87033306199967, 58.17653065674047, 242.37778894777458, 233.3218261285336, 222.43345664442293, 199.57201646356742, 129.9161756985201, 119.89959263400631, 97.51003545826339, 85.5103893238814, 73.7664695183689, 71.8484975931953, 70.12281070669623, 66.99600322997784, 65.61252401314532, 65.00300105316204, 64.77948009901948, 62.39491268468892, 57.42180603572939, 55.62965386575455, 54.214440659133075, 46.85459199928931, 42.42549469798536, 41.08353293191928, 37.0280437702086, 36.937920306494675, 36.92763433138512, 193.87886253112993, 696.1767132952399, 324.39224725963646, 250.72867860391668, 250.63338638160687, 157.31961967463843, 149.2670124220867, 144.86067748723778, 140.6868852048328, 132.98549055064603, 131.7768860497719, 127.53526221322967, 110.36547502910896, 86.19588063432776, 84.62565775845292, 62.52165852736273, 60.54506361849883, 58.54269236632459, 53.581046106546836, 52.852156727034014, 46.530016901319755, 42.03007847825066, 39.48572499554156, 30.22709704936505, 27.979292768478015, 27.54828858668581, 335.25553747959196, 330.4242807611285, 326.18429365243924, 228.87213339031456, 144.18065841459847, 126.90322114778574, 112.69005832228684, 111.35749353545498, 98.38547694723854, 97.76779021519563, 96.5182946562276, 93.85769034968835, 77.77258351065471, 72.0744976056325, 62.51066760655763, 60.941504731824395, 52.36090520483869, 51.01426724493133, 46.619841492585394, 44.62566226806511, 38.94539205433486, 33.6148189920807, 33.542956934780165, 31.147239750896226, 28.105402073203788, 177.12465693595456, 94.51582448028168, 90.33144684570686, 83.50325397656673, 80.21578160564692, 73.35637624885359, 70.30612175954973, 68.39190214319241, 65.47041776203069, 64.04143479834103, 61.586865726403204, 60.84965295200911, 59.06040230433, 55.74454784412498, 52.62919791167212, 52.15126971208346, 49.94046079604445, 49.870016978581205, 49.189676242879315, 47.48941763429434, 45.37891863150685, 44.68420274509242, 44.509450751242866, 41.094394733520446, 40.745549064495634, 77.0031559828657, 1170.5687107911249, 1093.5818774746954, 74.07199651019094, 63.911424870516015, 33.236114794379034, 29.62549055919589, 26.315087078473276, 26.002917137867513, 21.221299833130587, 20.689899009877923, 19.250006794743197, 9.870921439405409, 9.204506361100583, 4.508432474986816, 4.0674990572219905, 3.0786472231901936, 1.3967799020978786, 0.9081561190016718, 0.9081561340156832, 0.9081560920039308, 0.9081561010302912, 0.9081561066475626, 0.9081561066475626, 0.908156108921201, 0.9081561110575428, 0.9081562017192505, 0.9081563068986451, 0.9081561499166452, 0.9081561861118169, 0.9081562343837575, 0.9081562340311878, 160.1904363184882, 86.98665538296208, 79.03889647939069, 72.48343631803066, 66.71024988898824, 63.66116062454687, 59.54564920831956, 57.30844083472736, 54.223349312063725, 53.0422867062231, 52.5221189569513, 43.03340579319951, 38.97746656358125, 38.78605290864484, 33.07121085362301, 32.38858487804157, 30.98735685423299, 30.700744081062226, 30.417177563294622, 28.509855082845657, 28.285715276261932, 23.983137010802427, 22.818011237789694, 21.8046046952002, 20.419760113774803, 78.08738147977141, 65.92870881489169, 154.17401669522516, 142.3965508494539, 110.2274157414289, 96.24638084591798, 90.8151747400189, 80.12198218641949, 58.00894763171309, 56.66491882873331, 53.897004442549544, 49.48200018615993, 46.49885885231448, 45.327526938026296, 42.50601150242229, 42.49271686337556, 42.41339931793603, 41.98351523256554, 41.79462462296944, 39.937452772042214, 39.703388741206865, 35.47960436631825, 31.21212529650709, 24.32246976953115, 22.513653936897008, 22.212074375548404, 21.974367730355805, 39.46533036465144, 78.96966685681157, 206.2766902888086, 170.046203031729, 159.01650974293037, 73.3345950930203, 69.00813041495654, 51.80825810756924, 46.18466175956374, 43.85323609228873, 42.3661725989987, 39.52107313250692, 36.56457784497794, 35.988473660357606, 35.306102380761004, 30.117981747850095, 29.786312320275336, 24.343755641985418, 23.66213377772715, 23.2175560727051, 23.13789867253318, 21.93564298442499, 21.727539882982025, 20.093593708524512, 18.92620354710247, 18.25356785403754, 18.23652887439043, 210.2186429103214, 42.816394615209994, 245.88962259113114, 76.84520734371621, 76.8451972495658, 75.14577132205758, 68.01494046022204, 67.51357915062525, 61.969473026979756, 56.34722808501704, 55.00246517920947, 49.61667066350586, 46.9509619997811, 37.11997870948716, 35.66349374695113, 33.43269660067973, 33.10161099079726, 32.83108018921928, 30.7622865864019, 27.717045437905963, 27.58096619624023, 27.22298976966779, 26.776762709605773, 21.700893940989452, 19.747629151373584, 19.737388635783503, 17.306751285018187, 83.3529467640892, 49.65660327480529, 267.85705706360994, 173.95122246313124, 119.44379877978683, 84.60777273427074, 79.70537810100686, 71.98818073904563, 70.83898916357663, 46.818490530768415, 41.86436913407479, 40.73502020743958, 33.84296077278521, 31.28887920481521, 28.180077295711172, 27.73098450482715, 23.418331199405255, 20.78684690210017, 18.793019127040537, 18.72429843508183, 18.337729900953992, 18.21967082578821, 17.93550394728889, 16.576629221422273, 15.218770054973536, 14.155054144835058, 14.106983161039917, 51.51006987594717, 120.08655823853718, 85.53620218205585, 83.52270513654042, 76.768266043336, 56.084253574810475, 48.36284124652902, 42.38197612708369, 40.98608963031496, 40.74040109289524, 40.68830561749908, 40.22740601077228, 38.15249029162882, 36.409049915115695, 34.28824265030771, 33.90362207845903, 33.11886303540317, 32.982016665711065, 31.112979589037774, 29.26988642825598, 27.369310593891576, 25.21554567336906, 24.141176854402936, 22.872033711969372, 22.036625777355862, 18.567924222663706, 22.959165265270066, 430.4042617319144, 202.65101994024423, 105.14231660624202, 89.18098372472022, 72.92800263123195, 56.67109905388021, 40.3824925775378, 32.01609215010087, 29.463166600993702, 27.700859975866166, 23.965724709315023, 20.925720880888633, 13.79455773032156, 13.00061527038212, 12.400214701739564, 11.325226272982254, 10.760479928986946, 9.31749521631857, 7.3035756722457705, 7.22936141229506, 7.059428932144562, 7.026885775581186, 7.023048375007398, 6.962337842592557, 6.9083940755387605, 13.525369752448631, 120.59661074065544, 112.36134874356887, 95.083186209692, 70.70697304010608, 62.46927828958286, 47.46166980714221, 33.111355702019246, 32.460336048504665, 23.742169527467155, 23.46745381240399, 19.7154374710532, 19.132478545088112, 16.898868228005686, 16.33771507979609, 13.732341469548619, 10.51483146765159, 9.406764195768114, 8.883717756725925, 7.158262164203551, 7.1376327117378615, 7.056592725418564, 5.676067081485486, 5.640629725860326, 5.083313356165329, 4.953963188413921, 49.65660327480529, 217.98156872597409, 114.5911249298426, 102.41225605303501, 32.408471830639705, 31.24599329757079, 30.35994556226921, 22.539434829313375, 19.77751675964233, 16.6679097018219, 102.68954991066963, 54.17678369085255, 1.8243232407941274, 0.9081561042766658, 0.9081560914620953, 0.9081560914620953, 0.9081560914620953, 0.9081560914620953, 0.9081560914620953, 0.9081560914620953, 0.9081560914620953, 0.9081560914620953, 0.9081560914620953, 0.9081560914620953, 0.9081560914620953, 0.9081560914620953, 0.9081562108899837, 0.9081562379608483, 0.9081560968364838, 0.9081561450847039, 0.9081561861118169, 0.9081561666697714, 0.908156168920739, 0.9081561169827087, 0.9081561589323512, 0.9081562224671031, 153.46637437262, 64.11181231935387, 40.989177855984245, 33.22970641732441, 28.476338249463872, 24.000311405541854, 23.360282952879615, 22.789606626873443, 21.14230536730774, 18.169873218828617, 16.3279983045435, 15.341616513633348, 15.010590625214103, 13.597519000277927, 13.198179517553346, 13.023112635572698, 13.009172228277148, 12.817751960870677, 12.783416009889311, 12.530196069417812, 12.103005164527154, 11.193807181901807, 10.811261271158717, 10.754327444073908, 10.533047971944587, 126.33031736468718, 122.04139920648407, 73.46672060157483, 42.6712500321766, 21.08541165581985, 19.222666634377724, 19.1653299118189, 18.443417340022904, 11.430233060075683, 7.588894992282055, 7.03255451726246, 6.751862541919328, 5.321142868392037, 5.101330271654486, 4.828131626459386, 3.20791288584924, 2.441402369156359, 2.249702274579734, 2.0202977612525856, 1.9526325360157453, 1.3041545923948092, 1.1659853315696855, 0.9081560909605916, 0.908156099986952, 0.908156108921201, 0.9081561861118169, 0.9081562343837575, 0.9081562224671031, 83.74652999658454, 47.63230435659232, 46.54244192369076, 40.38343844085173, 22.27657937844268, 12.43738924132635, 9.485209057328042, 9.07629105251578, 8.845453339783848, 8.286829371051223, 7.394597535056467, 7.184795115778764, 6.804403970425893, 6.268387945441585, 6.1278220251618505, 5.786203196521511, 5.742336422069805, 5.313361427999635, 4.874942642614978, 4.873492718653764, 4.523295757654015, 4.359474512220066, 4.242913424341728, 3.3428505525389194, 3.091526527538192, 44.97112639179172, 30.413823314369825, 21.25760856319877, 19.773927225808944, 16.238666690065923, 15.005367376345099, 13.790193536223056, 12.74771847508632, 11.79912426618313, 11.245031002774317, 10.900555321413488, 9.948750525813628, 9.403743130232279, 9.089600217395244, 7.618357064647025, 6.49618720051245, 5.962343259643756, 5.748064565156787, 5.62685251071516, 4.955658207399688, 4.46591420240028, 3.7231590169681517, 3.582279490483502, 3.403251010806784, 3.3120251800319864, 11.767684288138405], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\"], \"logprob\": [25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.4163, -4.0901, -4.4161, -4.454, -4.5922, -4.7709, -4.8429, -4.8469, -5.1156, -5.1197, -5.1523, -5.1637, -5.1925, -5.2383, -5.2879, -5.3613, -5.377, -5.3879, -5.4068, -5.4205, -5.443, -5.4759, -5.5005, -5.5445, -5.549, -4.7968, -5.1231, -5.1939, -4.9033, -5.2865, -5.2101, -4.8068, -5.2087, -5.5939, -5.6546, -5.7044, -5.7556, -5.8167, -5.8186, -5.8797, -5.9177, -5.9501, -5.9529, -5.9857, -5.8859, -6.0176, -6.0263, -6.0314, -6.0546, -6.0646, -6.077, -6.1142, -6.1713, -6.1597, -6.2194, -6.227, -5.1941, -5.2424, -5.83, -5.9979, -5.4268, -5.776, -5.1669, -5.8266, -5.7808, -5.6491, -5.8771, -4.9829, -5.6194, -5.7243, -5.6941, -5.3886, -5.4106, -5.3734, -5.7643, -5.7084, -5.5486, -5.5747, -4.7364, -5.2015, -5.2116, -5.4693, -5.2309, -5.5856, -5.5597, -5.4051, -5.4695, -5.4486, -5.4623, -5.529, -5.5296, -5.5419, -3.968, -4.2999, -4.4628, -4.5177, -4.7263, -4.8278, -4.9113, -4.9622, -4.9691, -4.9783, -4.9795, -5.0128, -5.0223, -5.053, -5.1284, -5.136, -5.1713, -5.1752, -5.2765, -5.3623, -5.3888, -5.4205, -5.4432, -5.4464, -5.2713, -4.2188, -4.6169, -5.156, -4.7736, -4.6015, -4.1058, -4.6541, -4.3929, -4.5108, -4.2982, -4.8272, -4.3842, -4.5653, -4.5642, -4.799, -4.6874, -4.8865, -4.6674, -4.9552, -2.8341, -3.4056, -3.4312, -3.5358, -3.8332, -4.0409, -4.0725, -4.1008, -4.1821, -4.2276, -4.2677, -4.3072, -4.3569, -4.3601, -4.4039, -4.5014, -4.5805, -4.6657, -4.7011, -4.7659, -4.8309, -4.8507, -4.9563, -4.9722, -4.9893, -3.3282, -3.4882, -3.5658, -3.6456, -3.954, -4.0312, -4.2674, -4.275, -4.3769, -4.4619, -4.4786, -4.5124, -4.5437, -4.5743, -4.5874, -4.6177, -4.7423, -4.7897, -4.828, -4.8322, -4.8352, -4.8578, -4.9014, -4.9322, -4.9833, -3.7108, -3.9328, -4.0278, -4.1816, -4.336, -4.3622, -3.0448, -3.0659, -3.4062, -3.4237, -3.557, -3.6171, -3.6863, -3.6867, -3.9459, -4.0196, -4.0651, -4.0754, -4.1959, -4.1963, -4.2837, -4.3266, -4.3826, -4.3841, -4.4324, -4.5151, -4.5345, -4.5604, -4.5613, -4.5634, -4.5916, -4.4544, -4.5241, -2.9158, -3.3588, -3.6352, -3.6671, -3.7476, -3.8548, -3.8879, -3.9923, -4.2318, -4.3018, -4.4534, -4.4563, -4.4963, -4.536, -4.5449, -4.5824, -4.5977, -4.622, -4.6314, -4.663, -4.6737, -4.6947, -4.7095, -4.7174, -4.7284, -4.6178, -2.9842, -3.0224, -3.0704, -3.1793, -3.611, -3.6918, -3.9001, -4.0327, -4.1821, -4.2087, -4.2334, -4.2796, -4.3007, -4.3102, -4.3136, -4.3517, -4.436, -4.4682, -4.4943, -4.6428, -4.7441, -4.7769, -4.8832, -4.8857, -4.886, -3.2918, -1.9182, -2.6833, -2.9417, -2.9421, -3.4099, -3.4627, -3.4928, -3.5223, -3.5789, -3.5881, -3.621, -3.7667, -4.0161, -4.0347, -4.3411, -4.3737, -4.4078, -4.4977, -4.5117, -4.6413, -4.7451, -4.8089, -5.083, -5.1627, -5.1787, -2.5719, -2.5865, -2.5994, -2.9549, -3.4192, -3.5477, -3.6673, -3.6793, -3.8042, -3.8106, -3.8235, -3.8517, -4.0417, -4.1187, -4.2629, -4.2887, -4.4428, -4.4693, -4.561, -4.6056, -4.7446, -4.8954, -4.8976, -4.9738, -5.0797, -3.1205, -3.7529, -3.7986, -3.878, -3.9186, -4.009, -4.052, -4.08, -4.1242, -4.1466, -4.1862, -4.1984, -4.2287, -4.2873, -4.3458, -4.3551, -4.3991, -4.4006, -4.4146, -4.4504, -4.4967, -4.5124, -4.5164, -4.5979, -4.6066, -4.5434, -1.1372, -1.2053, -3.9085, -4.0579, -4.7247, -4.843, -4.9653, -4.9777, -5.1887, -5.2152, -5.2906, -6.0048, -6.0818, -6.9113, -7.0406, -7.4112, -8.8506, -11.5835, -11.5835, -11.5835, -11.5835, -11.5835, -11.5835, -11.5835, -11.5835, -11.5835, -11.5835, -11.5835, -11.5835, -11.5835, -11.5835, -2.976, -3.5912, -3.6881, -3.7757, -3.8597, -3.9072, -3.975, -4.0138, -4.0701, -4.0924, -4.1025, -4.3055, -4.4066, -4.4117, -4.5751, -4.5965, -4.642, -4.6515, -4.6611, -4.7278, -4.736, -4.9067, -4.9585, -5.0057, -5.0742, -4.0511, -4.2777, -2.9688, -3.0487, -3.3066, -3.4434, -3.502, -3.6286, -3.9557, -3.9795, -4.0304, -4.1173, -4.1806, -4.2066, -4.2722, -4.2725, -4.2744, -4.2848, -4.2894, -4.3359, -4.3419, -4.457, -4.5886, -4.8462, -4.9264, -4.9405, -4.9517, -4.5931, -4.4014, -2.6564, -2.8505, -2.9179, -3.6983, -3.7599, -4.0508, -4.1678, -4.2206, -4.2558, -4.3268, -4.4064, -4.4227, -4.4423, -4.6056, -4.617, -4.8255, -4.855, -4.8747, -4.8783, -4.9338, -4.9437, -5.0253, -5.0879, -5.1259, -5.1269, -2.8203, -4.8893, -2.4032, -3.5742, -3.5742, -3.5968, -3.6977, -3.7052, -3.7921, -3.8886, -3.9132, -4.018, -4.0742, -4.3142, -4.3552, -4.4214, -4.4317, -4.4401, -4.507, -4.6145, -4.6195, -4.633, -4.6501, -4.8682, -4.9666, -4.9672, -5.1051, -3.7075, -4.8127, -2.243, -2.6765, -3.0547, -3.4026, -3.4629, -3.5659, -3.5822, -4.0028, -4.1169, -4.1448, -4.3347, -4.4153, -4.5231, -4.5397, -4.7148, -4.8389, -4.9444, -4.9483, -4.9702, -4.9769, -4.9935, -5.0765, -5.1669, -5.244, -5.2476, -4.2942, -3.034, -3.3762, -3.4003, -3.4856, -3.8037, -3.9544, -4.089, -4.1232, -4.1293, -4.1307, -4.1423, -4.1965, -4.2444, -4.3059, -4.3175, -4.3415, -4.3458, -4.4057, -4.4686, -4.5379, -4.6227, -4.6678, -4.7239, -4.7626, -4.9416, -4.7718, -1.57, -2.3256, -2.9858, -3.152, -3.3554, -3.6111, -3.9564, -4.1945, -4.28, -4.3437, -4.4937, -4.6349, -5.0747, -5.1382, -5.189, -5.287, -5.3426, -5.5005, -5.7734, -5.785, -5.8122, -5.8175, -5.8181, -5.828, -5.837, -5.5401, -2.5881, -2.6594, -2.8278, -3.1272, -3.2528, -3.5321, -3.9004, -3.9208, -4.2439, -4.2559, -4.4376, -4.4691, -4.5996, -4.6353, -4.8199, -5.1081, -5.2303, -5.2936, -5.5365, -5.5398, -5.5528, -5.8059, -5.8133, -5.9379, -5.9692, -4.0851, -1.9498, -2.5965, -2.7098, -3.8793, -3.9169, -3.9465, -4.2548, -4.3912, -4.571, -2.859, -3.6828, -7.3893, -10.962, -10.962, -10.962, -10.962, -10.962, -10.962, -10.962, -10.962, -10.962, -10.962, -10.962, -10.962, -10.962, -10.962, -10.962, -10.962, -10.962, -10.962, -10.962, -10.962, -10.962, -10.962, -10.962, -2.2874, -3.1684, -3.6236, -3.8387, -3.9976, -4.1746, -4.2027, -4.2284, -4.3066, -4.4653, -4.5779, -4.6439, -4.6671, -4.7725, -4.8044, -4.8187, -4.8199, -4.8358, -4.8387, -4.8602, -4.8976, -4.9821, -5.0199, -5.0256, -5.0483, -2.299, -2.3338, -2.8461, -3.3983, -4.1251, -4.2218, -4.2249, -4.2652, -4.775, -5.2279, -5.3145, -5.3613, -5.6407, -5.6915, -5.7585, -6.2874, -6.6875, -6.8188, -7.0026, -7.0639, -7.9966, -8.3948, -10.831, -10.831, -10.831, -10.831, -10.831, -10.831, -2.4301, -3.0026, -3.0262, -3.1711, -3.7844, -4.4007, -4.696, -4.7447, -4.7733, -4.8461, -4.9747, -5.0074, -5.0698, -5.1648, -5.1913, -5.2587, -5.2677, -5.3602, -5.4646, -5.465, -5.557, -5.6031, -5.6373, -5.9498, -6.0579, -3.0198, -3.4206, -3.7919, -3.8675, -4.0747, -4.1585, -4.2485, -4.3327, -4.4161, -4.4682, -4.5021, -4.6019, -4.664, -4.7016, -4.8993, -5.0817, -5.1817, -5.2248, -5.2501, -5.4028, -5.5311, -5.7638, -5.8148, -5.8835, -5.9205, -5.2097], \"loglift\": [25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.3535, 0.3535, 0.3535, 0.3535, 0.3535, 0.3534, 0.3534, 0.3534, 0.3534, 0.3534, 0.3534, 0.3534, 0.3534, 0.3534, 0.3533, 0.3533, 0.3533, 0.3533, 0.3533, 0.3533, 0.3533, 0.3533, 0.3533, 0.3533, 0.3533, 0.3521, 0.3524, 0.3527, 0.3495, 0.3423, 0.3151, 2.0381, 2.0377, 2.0372, 2.037, 2.037, 2.0369, 2.0368, 2.0368, 2.0366, 2.0366, 2.0365, 2.0365, 2.0364, 2.0364, 2.0363, 2.0363, 2.0363, 2.0362, 2.0362, 2.0361, 2.036, 2.0359, 2.0358, 2.0357, 2.0357, 2.0346, 2.034, 2.0342, 2.0335, 2.0205, 2.0283, 2.0043, 2.0252, 2.022, 1.9989, 2.0212, 1.9145, 1.9859, 1.9989, 1.9853, 1.9217, 1.902, 1.8675, 1.9926, 1.9691, 1.9002, 1.8878, 1.3503, 1.6445, 1.6208, 1.7646, 1.4606, 1.7781, 1.7269, 1.3229, 1.3634, 1.2636, 1.3064, 1.4844, 1.3681, 0.8347, 2.3521, 2.3519, 2.3518, 2.3517, 2.3516, 2.3515, 2.3514, 2.3513, 2.3513, 2.3513, 2.3513, 2.3512, 2.3512, 2.3512, 2.3511, 2.3511, 2.351, 2.351, 2.3508, 2.3507, 2.3506, 2.3506, 2.3505, 2.3505, 2.3505, 2.35, 2.3487, 2.3502, 2.3458, 2.3304, 2.2948, 2.3111, 2.2857, 2.2723, 2.2387, 2.2471, 1.9958, 2.0036, 1.9638, 1.9961, 1.7351, 2.0655, 1.3463, 1.3221, 4.8616, 4.8602, 4.8601, 4.8597, 4.8584, 4.8573, 4.8571, 4.8569, 4.8564, 4.8561, 4.8558, 4.8555, 4.8551, 4.855, 4.8547, 4.8538, 4.853, 4.8521, 4.8517, 4.8509, 4.85, 4.8498, 4.8483, 4.848, 4.8478, 4.8716, 4.8711, 4.8708, 4.8704, 4.869, 4.8685, 4.8669, 4.8668, 4.866, 4.8652, 4.8651, 4.8648, 4.8644, 4.8641, 4.864, 4.8637, 4.8622, 4.8616, 4.8611, 4.8611, 4.861, 4.8607, 4.8601, 4.8597, 4.8589, 4.2787, 4.0354, 3.5823, 3.5063, 3.673, 3.4604, 5.1663, 5.1662, 5.1649, 5.1649, 5.1642, 5.1639, 5.1635, 5.1635, 5.1618, 5.1612, 5.1609, 5.1608, 5.1597, 5.1597, 5.1588, 5.1583, 5.1577, 5.1577, 5.1571, 5.1561, 5.1558, 5.1555, 5.1554, 5.1554, 5.155, 4.9825, 3.2576, 5.251, 5.2493, 5.2479, 5.2477, 5.2472, 5.2464, 5.2462, 5.2453, 5.243, 5.2422, 5.2403, 5.2403, 5.2397, 5.2392, 5.239, 5.2385, 5.2382, 5.2379, 5.2377, 5.2372, 5.237, 5.2367, 5.2364, 5.2363, 5.2361, 5.1723, 5.3789, 5.3787, 5.3786, 5.3781, 5.3758, 5.3752, 5.3735, 5.3723, 5.3706, 5.3703, 5.37, 5.3694, 5.3691, 5.369, 5.369, 5.3685, 5.3672, 5.3667, 5.3663, 5.3638, 5.3618, 5.3611, 5.3587, 5.3587, 5.3587, 5.2946, 5.3898, 5.3883, 5.3875, 5.3875, 5.3855, 5.3852, 5.385, 5.3848, 5.3845, 5.3844, 5.3842, 5.3831, 5.3809, 5.3807, 5.377, 5.3766, 5.3761, 5.3747, 5.3744, 5.3722, 5.3701, 5.3688, 5.3619, 5.3595, 5.359, 5.4668, 5.4667, 5.4667, 5.4656, 5.4633, 5.4625, 5.4616, 5.4615, 5.4605, 5.4605, 5.4603, 5.4601, 5.4581, 5.4573, 5.4554, 5.455, 5.4526, 5.4522, 5.4506, 5.4497, 5.4468, 5.4432, 5.4431, 5.4411, 5.438, 5.5563, 5.5519, 5.5515, 5.5507, 5.5503, 5.5493, 5.5487, 5.5484, 5.5478, 5.5475, 5.547, 5.5468, 5.5464, 5.5455, 5.5445, 5.5444, 5.5436, 5.5436, 5.5434, 5.5427, 5.5419, 5.5416, 5.5415, 5.5398, 5.5396, 4.9664, 5.6512, 5.6511, 5.6401, 5.6382, 5.6253, 5.622, 5.6181, 5.6177, 5.6099, 5.6088, 5.6055, 5.5592, 5.5521, 5.4364, 5.41, 5.3179, 4.6688, 2.3665, 2.3665, 2.3665, 2.3665, 2.3665, 2.3665, 2.3665, 2.3665, 2.3665, 2.3665, 2.3665, 2.3665, 2.3665, 2.3665, 5.8012, 5.7966, 5.7956, 5.7946, 5.7935, 5.7929, 5.7919, 5.7913, 5.7904, 5.7901, 5.7899, 5.7862, 5.784, 5.7839, 5.7799, 5.7793, 5.7781, 5.7778, 5.7775, 5.7755, 5.7753, 5.7695, 5.7676, 5.7657, 5.7629, 5.4447, 5.3873, 5.8468, 5.8463, 5.8445, 5.8433, 5.8428, 5.8415, 5.8373, 5.8369, 5.8361, 5.8347, 5.8335, 5.833, 5.8317, 5.8317, 5.8317, 5.8315, 5.8314, 5.8304, 5.8303, 5.8276, 5.8242, 5.816, 5.813, 5.8125, 5.812, 5.5851, 5.0832, 5.868, 5.867, 5.8667, 5.8602, 5.8595, 5.8552, 5.8532, 5.8521, 5.8514, 5.8499, 5.8481, 5.8477, 5.8472, 5.8429, 5.8425, 5.8358, 5.8347, 5.834, 5.8339, 5.8317, 5.8313, 5.8279, 5.8251, 5.8234, 5.8233, 5.6851, 5.2074, 5.9455, 5.9376, 5.9376, 5.9374, 5.9361, 5.936, 5.9349, 5.9335, 5.9331, 5.9313, 5.9303, 5.9253, 5.9243, 5.9226, 5.9224, 5.9221, 5.9203, 5.9171, 5.9169, 5.9165, 5.916, 5.9081, 5.9039, 5.9039, 5.8974, 5.723, 5.1357, 6.0201, 6.0184, 6.016, 6.013, 6.0124, 6.0112, 6.011, 6.0045, 6.0023, 6.0017, 5.9972, 5.995, 5.9918, 5.9913, 5.9853, 5.9804, 5.9757, 5.9755, 5.9745, 5.9742, 5.9734, 5.9692, 5.9642, 5.9596, 5.9593, 5.6177, 6.0314, 6.0284, 6.0282, 6.0273, 6.023, 6.0205, 6.0179, 6.0172, 6.017, 6.017, 6.0168, 6.0155, 6.0144, 6.0129, 6.0126, 6.012, 6.0119, 6.0102, 6.0084, 6.0063, 6.0035, 6.0019, 5.9998, 5.9983, 5.9906, 5.9481, 6.2189, 6.2165, 6.2125, 6.211, 6.2087, 6.2052, 6.1988, 6.1929, 6.1904, 6.1885, 6.1833, 6.1777, 6.1546, 6.1505, 6.1469, 6.1396, 6.1351, 6.1212, 6.0918, 6.0904, 6.087, 6.0864, 6.0863, 6.085, 6.0839, 5.709, 6.473, 6.4725, 6.471, 6.4678, 6.4661, 6.4616, 6.4534, 6.4528, 6.4425, 6.442, 6.4346, 6.4332, 6.4267, 6.4249, 6.414, 6.3927, 6.3819, 6.3758, 6.3488, 6.3484, 6.3468, 6.3114, 6.3103, 6.2897, 6.2842, 5.8634, 6.5194, 6.5157, 6.5148, 6.4958, 6.4948, 6.494, 6.4835, 6.4778, 6.4691, 6.3629, 6.1785, 5.8631, 2.9879, 2.9879, 2.9879, 2.9879, 2.9879, 2.9879, 2.9879, 2.9879, 2.9879, 2.9879, 2.9879, 2.9879, 2.9879, 2.9879, 2.9879, 2.9879, 2.9879, 2.9879, 2.9879, 2.9879, 2.9879, 2.9879, 2.9879, 6.5327, 6.5246, 6.5167, 6.5115, 6.5069, 6.5009, 6.4999, 6.4989, 6.4958, 6.4886, 6.4828, 6.4791, 6.4778, 6.4712, 6.4691, 6.4682, 6.4681, 6.467, 6.4668, 6.4653, 6.4626, 6.4562, 6.4531, 6.4527, 6.4508, 6.7157, 6.7155, 6.7107, 6.7018, 6.6799, 6.6757, 6.6756, 6.6737, 6.6423, 6.599, 6.5885, 6.5825, 6.5412, 6.5326, 6.5207, 6.4006, 6.2736, 6.224, 6.1478, 6.1205, 5.5914, 5.3053, 3.119, 3.119, 3.119, 3.119, 3.119, 3.119, 6.9957, 6.9875, 6.9871, 6.9841, 6.9656, 6.9322, 6.9079, 6.9032, 6.9004, 6.8928, 6.8782, 6.8742, 6.8663, 6.8533, 6.8495, 6.8394, 6.838, 6.8231, 6.8049, 6.8048, 6.7874, 6.7781, 6.7711, 6.6969, 6.667, 7.0278, 7.0181, 7.005, 7.0018, 6.9915, 6.9867, 6.9811, 6.9755, 6.9695, 6.9654, 6.9627, 6.9542, 6.9485, 6.9449, 6.9238, 6.9007, 6.8864, 6.8799, 6.8759, 6.8502, 6.826, 6.7752, 6.7628, 6.7453, 6.7356, 6.1786]}, \"token.table\": {\"Topic\": [14, 20, 24, 7, 13, 1, 6, 14, 14, 22, 19, 10, 20, 25, 10, 19, 25, 19, 10, 4, 5, 1, 2, 7, 1, 2, 13, 11, 18, 17, 23, 15, 16, 6, 14, 19, 23, 24, 5, 24, 23, 7, 25, 2, 3, 6, 13, 12, 18, 14, 10, 2, 3, 20, 2, 9, 9, 18, 1, 23, 11, 16, 8, 9, 1, 2, 3, 17, 25, 10, 17, 15, 8, 14, 5, 14, 14, 22, 8, 4, 11, 8, 19, 17, 3, 19, 17, 8, 11, 13, 19, 14, 18, 1, 2, 3, 17, 1, 15, 19, 9, 8, 22, 17, 24, 9, 20, 3, 1, 3, 5, 17, 13, 17, 1, 2, 17, 16, 14, 11, 23, 17, 5, 7, 13, 12, 13, 4, 18, 3, 2, 5, 12, 11, 15, 4, 16, 20, 5, 15, 18, 21, 1, 2, 3, 17, 16, 16, 3, 20, 18, 5, 18, 24, 25, 19, 13, 16, 23, 6, 3, 14, 19, 2, 13, 1, 2, 3, 4, 19, 4, 25, 20, 6, 1, 3, 14, 25, 6, 1, 3, 13, 3, 24, 7, 7, 23, 9, 24, 25, 23, 19, 13, 21, 4, 18, 19, 15, 20, 4, 8, 21, 9, 25, 2, 3, 5, 11, 3, 25, 16, 1, 2, 3, 3, 13, 1, 2, 3, 7, 2, 4, 2, 12, 1, 1, 9, 13, 1, 2, 7, 7, 1, 2, 16, 2, 3, 5, 5, 6, 25, 7, 25, 1, 2, 23, 1, 3, 5, 18, 4, 3, 17, 2, 20, 5, 2, 2, 8, 7, 10, 10, 10, 12, 1, 10, 16, 8, 1, 18, 10, 3, 20, 5, 1, 2, 3, 25, 1, 8, 20, 13, 21, 9, 1, 1, 5, 10, 8, 6, 2, 21, 21, 6, 3, 20, 5, 1, 2, 12, 16, 10, 12, 22, 23, 19, 15, 11, 1, 3, 23, 11, 19, 16, 9, 1, 25, 18, 21, 20, 19, 9, 25, 2, 2, 1, 5, 6, 14, 15, 9, 13, 8, 15, 22, 14, 4, 5, 9, 17, 14, 7, 2, 3, 1, 3, 4, 9, 15, 7, 21, 11, 1, 2, 3, 15, 25, 16, 18, 11, 17, 9, 1, 3, 11, 5, 16, 13, 18, 9, 13, 5, 2, 3, 5, 7, 2, 13, 13, 1, 2, 1, 2, 3, 24, 1, 12, 2, 3, 20, 9, 15, 11, 16, 10, 20, 6, 21, 1, 3, 18, 7, 11, 25, 24, 2, 24, 1, 2, 8, 1, 3, 10, 12, 8, 18, 8, 1, 3, 6, 23, 7, 10, 8, 9, 14, 1, 2, 3, 17, 13, 1, 3, 8, 12, 22, 23, 13, 4, 14, 1, 6, 14, 20, 10, 6, 1, 2, 1, 3, 10, 17, 2, 15, 6, 4, 15, 2, 15, 5, 18, 6, 3, 8, 9, 1, 2, 3, 16, 11, 11, 1, 2, 15, 5, 1, 2, 3, 12, 1, 2, 3, 4, 1, 1, 1, 2, 3, 4, 14, 12, 9, 15, 8, 13, 20, 16, 23, 23, 7, 2, 18, 19, 8, 15, 6, 16, 6, 24, 14, 4, 6, 1, 6, 13, 3, 3, 8, 5, 12, 10, 25, 1, 2, 3, 3, 2, 19, 19, 1, 3, 23, 17, 1, 24, 24, 7, 24, 7, 7, 12, 1, 2, 5, 20, 4, 1, 3, 6, 20, 1, 2, 11, 18, 10, 2, 16, 23, 1, 2, 19, 16, 13, 14, 13, 7, 3, 14, 15, 15, 10, 18, 20, 8, 20, 17, 1, 2, 3, 9, 4, 1, 2, 17, 20, 5, 11, 8, 12, 4, 25, 3, 19, 1, 3, 16, 4, 13, 11, 1, 2, 17, 20, 2, 3, 1, 5, 1, 2, 3, 4, 19, 8, 11, 17, 25, 1, 2, 5, 14, 14, 13, 10, 2, 1, 2, 6, 8, 1, 11, 19, 24, 13, 4, 7, 14, 1, 1, 3, 5, 3, 22, 7, 4, 6, 10, 18, 7, 25, 8, 1, 3, 13, 6, 20, 11, 15, 13, 21, 9, 4, 2, 18, 12, 3, 13, 25, 25, 5, 20, 5, 23, 17, 1, 2, 3, 16, 10, 1, 2, 24, 4, 19, 1, 22, 11, 14, 24, 2, 15, 18, 19, 19, 18, 14, 15, 3, 1, 9, 7, 1, 3, 10, 18, 6, 15, 23, 24, 25, 2, 1, 2, 5, 15, 3, 4, 9, 15, 16, 4, 8, 13, 13, 2, 24, 21, 15, 17, 16, 22, 24, 1, 3, 5, 5, 8, 13, 1, 2, 3, 1, 6, 7, 8, 2, 1, 3, 3, 17, 16, 20, 4, 1, 2, 23, 17, 25, 6, 12, 18, 1, 1, 5, 10, 11, 1, 16, 11, 25, 16, 3, 9, 22, 24, 24, 24, 24, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 24, 22, 22, 22, 22, 22, 21, 11, 11], \"Freq\": [0.9888646963809061, 0.9005238819010367, 0.8843109569458222, 0.9821909733887936, 0.9794434356017904, 0.9997261733913503, 0.9845728666565939, 0.9833570631275864, 0.9454317343326256, 0.9758673409000852, 0.9990607394771426, 0.9631672096766387, 0.946808968749988, 0.9322725781884119, 0.9801179689583387, 0.9746989813140539, 0.8956732751045986, 0.9867574490054334, 0.9740091352600809, 0.9956753449838777, 0.998707921945292, 0.03698393473882378, 0.9615823032094184, 0.9908513224124431, 0.015551747216721567, 0.9823520325229123, 0.9933303890849006, 0.9758145136592248, 0.9566145761662375, 0.9621468852007241, 0.4445032621869085, 0.9665153125066006, 0.9244947094057223, 0.9835240939196148, 0.982283911688454, 0.9891355199018489, 0.8192013021971105, 0.9910858396566999, 0.9895275529902517, 0.9466370504701912, 0.9842692672075357, 0.9768134343343451, 0.880126717200387, 0.7700062745248141, 0.22781250725586216, 0.8330823945182714, 0.15695755259039895, 0.9350646050117474, 0.9733473291919109, 0.9892715893545063, 0.9606694090223424, 0.005947770993978366, 0.9932777559943872, 0.8406148428081736, 0.9981837015015456, 0.9649993737661123, 0.9594040722018052, 0.9697925277532488, 0.9998906407670435, 0.9914668365550111, 0.9779244164439449, 0.970991164315501, 0.9722360766183461, 0.988609140150458, 0.411760588282538, 0.509118297876457, 0.07860071967215476, 0.9330574719241425, 0.9413449177946914, 0.986704341483358, 0.9881563927791334, 0.9805433722042062, 0.20270956624666928, 0.7601608734250097, 0.9952916222818576, 0.9902590803858664, 0.976573775989995, 0.9249614590923887, 0.9943347514869637, 0.9902544981736843, 0.9846880395517952, 0.9775993140504994, 0.8499271056727359, 0.9652143259211539, 0.9993484189597825, 0.9881580017842576, 0.9215294192668254, 0.9736261013940832, 0.9825543075520734, 0.9925686180408336, 0.9597039221209254, 0.976526976384972, 0.9694136934288806, 0.43833256574341045, 0.5022825342721592, 0.0592913615498333, 0.9183998780212002, 0.9996659579624048, 0.9868153091197737, 0.9230332373067209, 0.9916118556772028, 0.9924971168438218, 0.9969610647640138, 0.9270504087376487, 0.8707257172852593, 0.989155752849778, 0.9408085814695005, 0.9989989088731125, 0.05565201619047066, 0.9437654412300649, 0.31061887585346226, 0.6600651111886073, 0.9868558832971361, 0.9821365922343827, 0.009438132918021305, 0.9886444231627317, 0.9825177932588297, 0.9746861758909724, 0.9902530963189925, 0.9936504778306432, 0.7841092003444627, 0.9613177263974395, 0.9858738670052429, 0.9945371263364307, 0.977168444723513, 0.9500253571439228, 0.9821165319373217, 0.9890101747430022, 0.9830834534136251, 0.9996578976925196, 0.7434873963149345, 0.25412987275171495, 0.8872263302583226, 0.9837381095283017, 0.9510623699677864, 0.9863546941473293, 0.4430427888562876, 0.5437343317781711, 0.9855346031689614, 0.974349454679758, 0.980667415438378, 0.5481484737127508, 0.04447660928858811, 0.9475451544090511, 0.003867531242485923, 0.9862730141406442, 0.9621408146951361, 0.9923929503207135, 0.999318416204866, 0.9900013674789179, 0.9909872978565624, 0.9945846931736892, 0.9499691236578979, 0.844713905230562, 0.6038601433521362, 0.9293265789253191, 0.19195482128885277, 0.7918136378165177, 0.936394535803315, 0.9903432462865165, 0.9984251255910747, 0.9771847813626026, 0.9967874825380292, 0.2945418269142264, 0.6915329849290534, 0.11905691450752845, 0.8697769032077773, 0.009921409542294039, 0.3696756607407942, 0.5914810571852708, 0.9884648005020624, 0.9784055577498505, 0.9664358139841591, 0.9934311472664232, 0.0020577925926620663, 0.9973434765768814, 0.95565889575017, 0.8071581680163659, 0.9919916110962801, 0.03874293117792473, 0.959397322063873, 0.0010195508204717035, 0.9993280207393038, 0.7070613279047613, 0.9932268165532945, 0.9933702429003284, 0.6234583266965912, 0.9861260117591802, 0.9488457181707365, 0.8057673567869696, 0.989469531998033, 0.9424006375662876, 0.2768713640439432, 0.7014074555779893, 0.0435555904775285, 0.9146674000280985, 0.9677251796548797, 0.962879924783476, 0.8502687109017062, 0.9930996835635506, 0.9980512974488552, 0.9954970104504202, 0.987698719078948, 0.9237211580416462, 0.8716098782402034, 0.037519630520273656, 0.0894698881637295, 0.9945424537836064, 0.9979153073997069, 0.9329994827097676, 0.9847526839913976, 0.6236344861874784, 0.018960128174968532, 0.3566553840480567, 0.9996891958795033, 0.9900590652601193, 0.6785900707184994, 0.29996058171876955, 0.020940644384140516, 0.992669411383757, 0.9954456449063608, 0.9962307001734959, 0.9965862336250892, 0.9628080838561766, 0.9998109537892248, 0.9997772835715383, 0.9941045347703962, 0.9676089618138172, 0.004174977439264976, 0.9950362896914859, 0.9926067263997194, 0.9923937096217936, 0.2671582821023712, 0.7325761314491337, 0.9963820246590462, 0.9976284751811536, 0.9971270821105784, 0.987414275247843, 0.9920428877822777, 0.9875838152707042, 0.9570656998345396, 0.9786899699335265, 0.8815100591974295, 0.014135663780152833, 0.9824286327206219, 0.9759579620279658, 0.8461750437316373, 0.15364460073097821, 0.9888105779963151, 0.9759408706902938, 0.993344115362106, 0.9988728905616632, 0.9968003192710118, 0.9988275358844935, 0.8074343405205348, 0.9932150297920576, 0.9959010317693733, 0.9979846787176953, 0.9929479474469279, 0.9918899739009562, 0.9987159516239207, 0.9963692499133598, 0.9859797650888231, 0.9994679159497055, 0.999830187624482, 0.9817098824115178, 0.9752200934653908, 0.9940312595005475, 0.9999637284085319, 0.9937417599719032, 0.9908617999602075, 0.9986797657697903, 0.946670313203864, 0.9829347528611934, 0.20022226173178223, 0.04829481690012837, 0.7505819459894951, 0.9236187035260762, 0.9999460925870994, 0.9817607631861937, 0.8381922682301728, 0.9774386988707916, 0.9760670649730835, 0.9958030257362489, 0.9998327485900954, 0.9998049245563916, 0.9951520692213687, 0.9938764933432431, 0.9845699269739601, 0.9906713761398366, 0.9965924433085755, 0.9601231016820432, 0.9873961403433553, 0.9923750485698148, 0.9981135785832093, 0.995052839901625, 0.9814888890928866, 0.016512040683630158, 0.9824664206759944, 0.9117689827893241, 0.9797456333315272, 0.9960819730797741, 0.8691394938688967, 0.9629937622114141, 0.9485230986457757, 0.9682630801617784, 0.9677532211387206, 0.9866435755079239, 0.02070846479157268, 0.9782284320590522, 0.8531750426210135, 0.9733687589118388, 0.955761577526627, 0.9677020705738957, 0.9800971815377744, 0.9997104022761426, 0.837455594397267, 0.9517937985909686, 0.9552059288288275, 0.9637118135418691, 0.9872750850462134, 0.9940585844125048, 0.9863935780091712, 0.9962726849963339, 0.9973918711800096, 0.6978000142977764, 0.3011253173733143, 0.9907635619136139, 0.9880955308546544, 0.9573459968741592, 0.9907299725313494, 0.9630993220722173, 0.16649332102734052, 0.82770965310735, 0.9186674153332093, 0.9870501016769313, 0.9919636145628581, 0.978928754593782, 0.9974728571051484, 0.958807114937602, 0.9864822515672258, 0.9956993804388143, 0.9946140688735212, 0.0025053251105126477, 0.06429729114221795, 0.9353724973308373, 0.9900364116506698, 0.9951176315842608, 0.9938107874087901, 0.9898229018516324, 0.9599284065146518, 0.9852604282096679, 0.01048030040064563, 0.8832034973998635, 0.10575575858833317, 0.9845594321539396, 0.960861228173288, 0.9698281424606283, 0.9529589607851368, 0.9942697581013044, 0.9581237026666789, 0.998790824185997, 0.29985233186593885, 0.7002233431642094, 0.9815097702720179, 0.9778544422772513, 0.985077683618416, 0.9681367836928592, 0.9527290296871006, 0.9925894881722399, 0.9896143799757879, 0.9899026150430494, 0.9606643781223463, 0.03834584702589197, 0.0687562485222991, 0.9282093550510379, 0.9989446746907543, 0.9803498911727572, 0.9862848036302342, 0.3248982025785393, 0.6742316460195313, 0.10099067566871457, 0.005979711059331784, 0.8923057725202873, 0.9657424307026753, 0.999875220433576, 0.649635978079858, 0.07647771129527697, 0.9228310496296755, 0.9567583297186181, 0.9926067604669173, 0.9938475366513548, 0.9939732411301246, 0.962640010317949, 0.9900661200158167, 0.980080761375275, 0.9881196030265212, 0.960686836012248, 0.9984847264477564, 0.0013264493210863586, 0.9662167437871542, 0.9890116819739296, 0.5098709187879796, 0.4248924323233163, 0.9648327126505767, 0.9971074323229315, 0.8205224744663563, 0.5184986829025998, 0.4808049175865789, 0.9899707781602942, 0.4603784163640367, 0.539181929074998, 0.9845506812480669, 0.7159324088913799, 0.9839879392257589, 0.9909518745938337, 0.974608199413713, 0.32133586406673637, 0.6782293852228247, 0.9956502477391829, 0.8886436835389723, 0.9822131373674504, 0.9878095897064819, 0.9926542534125985, 0.9970937564542914, 0.9884046749714835, 0.4257023402829057, 0.560624393487324, 0.012406625582015466, 0.9736401531398965, 0.9545454211178837, 0.9992886201499616, 0.0007403966066806334, 0.988681326918301, 0.7375539509157089, 0.9826582297531115, 0.4949765421607949, 0.9641506339327699, 0.9944499513158044, 0.9826070343816837, 0.999634524843978, 0.9894449873945803, 0.9809873965817909, 0.8808916329247202, 0.9757251884121261, 0.9938836198770781, 0.0026536168540678765, 0.9951063202754536, 0.9959613733088895, 0.003863167498981655, 0.9962549836192687, 0.9945316712946191, 0.9980906502187755, 0.9843990487792302, 0.9922631812550611, 0.4671107920164592, 0.5138218712181052, 0.9963408816061311, 0.9853911356691669, 0.9854187700197667, 0.9937312837327686, 0.9878245990300651, 0.9968827442400574, 0.9906645259825304, 0.9754918735451875, 0.43186916883027654, 0.48896918929014555, 0.07961833838770462, 0.9890013002787812, 0.9928148043328432, 0.9814223608576117, 0.9998399682276313, 0.9978312710571583, 0.9720171568656075, 0.9851712606778307, 0.3798256567898589, 0.5745531766088852, 0.04493711995823683, 0.9857392497137631, 0.17675854919019318, 0.6579345997634969, 0.16426046995452295, 0.9959884749753085, 0.9998969828804507, 0.9998713559558837, 0.622433430908479, 0.011810881041906623, 0.3657436162643751, 0.5318497807032001, 0.46853433061948574, 0.9424493389785719, 0.9915117720819391, 0.9313247763910296, 0.9180990525535998, 0.0773678976871011, 0.968740450336329, 0.9667203209202258, 0.5121291290374854, 0.7517182114692468, 0.9862002908534011, 0.996065126959542, 0.9642278044809981, 0.8829845655142673, 0.9936707550712571, 0.947558818469422, 0.9903138824667177, 0.9843556354504148, 0.995866708823579, 0.8817818615822798, 0.9707125663431551, 0.9907954681783183, 0.996319012622938, 0.8512200812560264, 0.14763708726391786, 0.9797336194405685, 0.9984590395253669, 0.9980843893704401, 0.9984413219156151, 0.9845442917191504, 0.9614305913236563, 0.9918112566027862, 0.942698879885374, 0.32498899735569664, 0.5117861923167422, 0.1629710221343816, 0.9978031145770012, 0.9978402014473133, 0.8215154150864113, 0.8543298692561946, 0.9989649881291327, 0.0009194339513383642, 0.9224004294589707, 0.9578024626229698, 0.9998334344554636, 0.8159505905147332, 0.7976532472971833, 0.9897173492281244, 0.9426941023235353, 0.9858858219130335, 0.9834180528198507, 0.9666552741727474, 0.4030802000624104, 0.16401194347367046, 0.43087883454947323, 0.9510375920683612, 0.9867028111353279, 0.0017513703159413696, 0.9982810800865807, 0.997525371246324, 0.9886080152246561, 0.5391286138464412, 0.46075308848550484, 0.9817023188639448, 0.9624290266653212, 0.9946300889580196, 0.9994911668181903, 0.9890011703666081, 0.8284778273398732, 0.01237194640483317, 0.987281323105687, 0.9842798091845946, 0.9875713010312998, 0.9880024125936723, 0.9882657763837447, 0.9908364554661145, 0.9892331886015764, 0.9992082926550182, 0.9972151653457313, 0.9818012891279014, 0.972533604239892, 0.991830712642972, 0.9694883132548094, 0.9858185063821645, 0.9947694054682024, 0.9181210424308129, 0.9928165851123444, 0.08638062125367367, 0.8897203989128388, 0.023034832334312978, 0.9838765950189099, 0.9949368746383689, 0.23877054164505152, 0.7603307750708344, 0.9911501818595859, 0.7868883383214168, 0.44154034423687105, 0.5454321899396642, 0.9851333933076746, 0.99951415855739, 0.9765792050878356, 0.8885962428335465, 0.99876774169506, 0.853863317495544, 0.09892814289207587, 0.899450126754391, 0.9550751118809712, 0.9827837683725125, 0.9749222653558877, 0.9880446988242514, 0.15581251672231275, 0.8421935171111214, 0.9478406656407166, 0.9924878547914789, 0.9963660178585815, 0.9985075722674486, 0.7545649521109379, 0.24271038268536538, 0.026309350861815246, 0.965929024498074, 0.006264131157575059, 0.9912437165796254, 0.8585998505251582, 0.98960951332803, 0.988554099350942, 0.9962844552474002, 0.9408396029374657, 0.042755901982151755, 0.954131707391176, 0.992235465402364, 0.9611649227666423, 0.9456276528632872, 0.9759859631337192, 0.9921467979024006, 0.9992163171669456, 0.06651225891127727, 0.9332501328488592, 0.9800500306377886, 0.9971337842163265, 0.9997942150793984, 0.9811683596616125, 0.965765050909542, 0.9867253040739187, 0.9771750131132985, 0.9896579966256887, 0.9842215993124165, 0.9859965747750852, 0.9996536312649538, 0.294288576158049, 0.7051950030080988, 0.9915382669498914, 0.9980828275156541, 0.9298582409735372, 0.9967237490948917, 0.9971318211415043, 0.9906511187369692, 0.9928825987266794, 0.9618733636478937, 0.990658504822548, 0.9173844547493465, 0.9881904615737481, 0.001019902696054969, 0.9964449340457047, 0.002039805392109938, 0.9976236616030576, 0.9902727862500798, 0.9916498972885669, 0.9448008079875787, 0.13633324921745885, 0.8472137629942085, 0.9876276976223877, 0.9990037957275717, 0.9978967474445025, 0.9702256937268486, 0.9855276412045482, 0.9972570219726155, 0.9590071553041788, 0.904636213024747, 0.8385964682447258, 0.9944855852973855, 0.987884190081451, 0.9948853793552475, 0.9623600798151325, 0.9750919909624728, 0.988900467893457, 0.0005358441982625072, 0.01044896186611889, 0.9870576817105762, 0.9838130867282848, 0.002961212257463237, 0.9949673185076476, 0.9044194449614825, 0.9972075239093273, 0.8685086482319817, 0.9998387505268502, 0.9224260997879958, 0.9820454608679112, 0.9923851196174905, 0.5982917778005318, 0.9961288389696589, 0.9455750064230389, 0.9887651582211193, 0.8299488236672923, 0.8617794964350347, 0.9718204883873146, 0.9910238047512153, 0.9630063277255797, 0.9974505624603982, 0.9997000305164311, 0.991656354939234, 0.9900403384485016, 0.299290328326793, 0.7000641587873836, 0.9850918474449618, 0.9899924007284168, 0.9910708717581069, 0.9936075207249003, 0.939195937811628, 0.7528191059846484, 0.8698580093043227, 0.996575636113051, 0.2328463324187494, 0.21580879590030433, 0.5508803474297242, 0.9736015552438796, 0.998278029895683, 0.9922462954377248, 0.9983097491301165, 0.9508210020003256, 0.976090605859363, 0.9899813722324466, 0.9879671757502839, 0.9886573937275976, 0.9893532119851123, 0.997100766759838, 0.9883452199482758, 0.9959745437809561, 0.9321949432971924, 0.9793531073809673, 0.978935973739766, 0.9653523362732228, 0.8207665899837326, 0.07771716323037642, 0.6469953838928837, 0.2739530003870769, 0.9945802070464466, 0.3336937791663642, 0.6522196592797119, 0.1273979833046587, 0.8599363873064462, 0.011734024778060669, 0.9997063670968623, 0.9952966811558874, 0.9962988259914849, 0.9748796707890731, 0.9982958129164975, 0.9623576443070437, 0.037441123209736894, 0.9991654888123642, 0.9819560613031107, 0.981774177285632, 0.8864258501274669, 0.9983125904717558, 0.04997914836722534, 0.9475213544619805, 0.9936471834083087, 0.9199166522280663, 0.8892816745043078, 0.9874854509109141, 0.9788867442398609, 0.981826367118797, 0.9998725816290386, 0.9997941167915876, 0.990240001405108, 0.9961894295413097, 0.9896941748567388, 0.9996036610738585, 0.981395716536946, 0.984843610804369, 0.9188332786977772, 0.9741298025609423, 0.9982333274110324, 0.9909973896148935, 0.9583208988983837, 0.6469295935793301, 0.8641245096621992, 0.8350968821397848, 0.6881563343450419, 0.9362016082564973, 0.9459706334071742, 0.9356146735456399, 0.9493928088655447, 0.9092163039637559, 0.9417694145390506, 0.9832724894159157, 0.9088651826936367, 0.9576865304836011, 0.8933511036502434, 0.8814173051207462, 0.9125505117116492, 0.9387162234817942, 0.9326748260313915, 0.9560567629825916, 0.9214387017756369, 0.9948414423001388, 0.9860368480214798, 0.9904709272101891], \"Term\": [\"#\", \"03\", \"10hr\", \"150\", \"16gb\", \"2\", \"2016\", \"21\", \"26\", \"2d\", \"2k\", \"3080\", \"32gb\", \"34\", \"4k\", \"4x\", \"5600x\", \"720p\", \"@\", \"accur\", \"act\", \"activ\", \"activ\", \"adapt\", \"addit\", \"addit\", \"adult\", \"afterward\", \"aka\", \"albeit\", \"albert\", \"altogeth\", \"amazingli\", \"amazon\", \"american\", \"amus\", \"an\", \"anger\", \"angl\", \"anxieti\", \"anytim\", \"arbitrari\", \"arc\", \"area\", \"area\", \"ark\", \"ark\", \"asham\", \"assassin\", \"asshol\", \"aswel\", \"attack\", \"attack\", \"aud\", \"b\", \"backpack\", \"backup\", \"backward\", \"bad\", \"ball\", \"bandit\", \"barrel\", \"barren\", \"bat\", \"becom\", \"becom\", \"becom\", \"behold\", \"bell\", \"bet\", \"bethesda\", \"bewar\", \"bia\", \"bia\", \"bind\", \"bitch\", \"bite\", \"bob\", \"bodi\", \"bonu\", \"border\", \"borderlin\", \"bork\", \"borrow\", \"boss\", \"boy\", \"bread\", \"breaker\", \"brick\", \"bro\", \"brother\", \"bruh\", \"buf\", \"build\", \"build\", \"build\", \"burden\", \"buy\", \"buyer\", \"cach\", \"camera\", \"camp\", \"cancer\", \"captiv\", \"cardboard\", \"cat\", \"cent\", \"challeng\", \"charact\", \"charact\", \"child\", \"child\", \"children\", \"chip\", \"choos\", \"choos\", \"chose\", \"christ\", \"ck\", \"cking\", \"clarifi\", \"clich\", \"closer\", \"club\", \"clue\", \"cod\", \"coffe\", \"color\", \"colour\", \"combat\", \"combin\", \"combin\", \"commerc\", \"commit\", \"conduct\", \"conflict\", \"contact\", \"contact\", \"context\", \"convinc\", \"cooki\", \"copypast\", \"core\", \"core\", \"core\", \"corrupt\", \"costum\", \"countless\", \"cover\", \"cpu\", \"crack\", \"creativ\", \"creed\", \"creepi\", \"crow\", \"cum\", \"cup\", \"cup\", \"da\", \"daili\", \"damag\", \"darn\", \"data\", \"de\", \"de\", \"deal\", \"deal\", \"deal\", \"decept\", \"decept\", \"defeat\", \"demo\", \"deni\", \"deserv\", \"design\", \"design\", \"desk\", \"devoid\", \"diablo\", \"difficult\", \"difficult\", \"difficult\", \"difficulti\", \"dinner\", \"disappear\", \"disast\", \"discern\", \"display\", \"disregard\", \"dl\", \"dog\", \"doll\", \"don\", \"don\", \"drown\", \"drown\", \"dumbest\", \"dust\", \"dx12\", \"dynam\", \"e\", \"ea\", \"ear\", \"editor\", \"effect\", \"effect\", \"effect\", \"elder\", \"element\", \"elev\", \"email\", \"end\", \"end\", \"end\", \"enemi\", \"english\", \"enough\", \"enough\", \"enough\", \"entri\", \"equip\", \"escap\", \"essenti\", \"etern\", \"even\", \"everi\", \"everytim\", \"evil\", \"exampl\", \"exampl\", \"excel\", \"excus\", \"exist\", \"exist\", \"expans\", \"explain\", \"explor\", \"explos\", \"express\", \"f2p\", \"famou\", \"fantasi\", \"farcri\", \"farm\", \"farm\", \"farmer\", \"feel\", \"feel\", \"fell\", \"femal\", \"field\", \"fight\", \"file\", \"fire\", \"fkin\", \"flat\", \"flaw\", \"food\", \"fool\", \"format\", \"fp\", \"frame\", \"framer\", \"fuck\", \"fun\", \"fund\", \"furri\", \"g\", \"game\", \"gay\", \"gb\", \"gear\", \"geforc\", \"gem\", \"gener\", \"gener\", \"gener\", \"geometri\", \"get\", \"gift\", \"gimp\", \"girl\", \"git\", \"glitchi\", \"go\", \"good\", \"gorgeou\", \"gpu\", \"grass\", \"greedi\", \"ground\", \"gtx\", \"gud\", \"guild\", \"gun\", \"hacker\", \"hade\", \"hand\", \"hand\", \"happend\", \"hardest\", \"hardwar\", \"hatr\", \"heat\", \"hello\", \"hill\", \"hint\", \"histor\", \"hit\", \"hit\", \"hive\", \"hone\", \"honor\", \"hoop\", \"hotfix\", \"hour\", \"humili\", \"hyper\", \"i\", \"i9\", \"id\", \"idk\", \"imag\", \"immedi\", \"import\", \"impress\", \"impress\", \"incompet\", \"incomplet\", \"incorpor\", \"induc\", \"infin\", \"inform\", \"inform\", \"infrastructur\", \"infuri\", \"injuri\", \"innov\", \"input\", \"insist\", \"insta\", \"intend\", \"interact\", \"interact\", \"interest\", \"interest\", \"intro\", \"intuit\", \"inventori\", \"irl\", \"isn\", \"it\", \"item\", \"item\", \"item\", \"iv\", \"jank\", \"jesu\", \"joe\", \"k\", \"keybind\", \"keyboard\", \"kill\", \"kill\", \"killer\", \"knight\", \"l\", \"la\", \"ladder\", \"laggi\", \"languag\", \"laptop\", \"larg\", \"larg\", \"laughabl\", \"laughabl\", \"launcher\", \"lay\", \"leak\", \"less\", \"less\", \"level\", \"level\", \"level\", \"librari\", \"like\", \"likeabl\", \"limit\", \"limit\", \"limp\", \"linux\", \"litter\", \"lmao\", \"loser\", \"lowest\", \"loyal\", \"lvl\", \"m\", \"make\", \"make\", \"male\", \"march\", \"marvel\", \"marvel\", \"mask\", \"materi\", \"mcdonald\", \"mean\", \"mean\", \"meanwhil\", \"mechan\", \"mechan\", \"medium\", \"meeh\", \"meter\", \"micro\", \"mindlessli\", \"mission\", \"mission\", \"mmo\", \"molotov\", \"monet\", \"monitor\", \"mountain\", \"mous\", \"mouth\", \"move\", \"move\", \"move\", \"mp\", \"ms\", \"much\", \"much\", \"murder\", \"mw\", \"nah\", \"nap\", \"nasa\", \"navig\", \"netcod\", \"new\", \"news\", \"nich\", \"noodl\", \"nvidia\", \"offici\", \"often\", \"often\", \"one\", \"one\", \"optim\", \"optimis\", \"order\", \"organ\", \"outdat\", \"outfit\", \"outfit\", \"outsid\", \"overhaul\", \"overr\", \"overwhelm\", \"p2w\", \"pace\", \"packag\", \"panel\", \"part\", \"part\", \"part\", \"pathet\", \"paywal\", \"peac\", \"peopl\", \"per\", \"perma\", \"perspect\", \"pick\", \"pick\", \"pick\", \"pl\", \"place\", \"place\", \"place\", \"plant\", \"play\", \"player\", \"point\", \"point\", \"point\", \"polit\", \"polit\", \"poo\", \"port\", \"pose\", \"potato\", \"potato\", \"pound\", \"predecessor\", \"prelud\", \"preview\", \"prior\", \"process\", \"programm\", \"promin\", \"protect\", \"proud\", \"prove\", \"ps\", \"publish\", \"puddl\", \"pump\", \"punch\", \"pve\", \"pvp\", \"pvp\", \"que\", \"quest\", \"quickli\", \"r\", \"rain\", \"rainbow\", \"ram\", \"ramp\", \"random\", \"random\", \"random\", \"rang\", \"rare\", \"ray\", \"re\", \"realli\", \"realli\", \"rebel\", \"rebind\", \"recommend\", \"recours\", \"recreat\", \"redo\", \"refin\", \"reflect\", \"relic\", \"religion\", \"remain\", \"remain\", \"remain\", \"remap\", \"repeatedli\", \"repetit\", \"repetit\", \"report\", \"request\", \"requir\", \"requir\", \"resist\", \"reskin\", \"resolut\", \"resourc\", \"respond\", \"restaur\", \"result\", \"result\", \"revers\", \"revert\", \"reviv\", \"rez\", \"rich\", \"rise\", \"rng\", \"road\", \"robot\", \"roleplay\", \"rtx\", \"rubbish\", \"ryzen\", \"sake\", \"sand\", \"script\", \"sens\", \"sens\", \"sens\", \"sensit\", \"sent\", \"sever\", \"sever\", \"sh\", \"shader\", \"shell\", \"shell\", \"shift\", \"shit\", \"shock\", \"shockingli\", \"shoot\", \"shortag\", \"shot\", \"shot\", \"shouldnt\", \"shown\", \"si\", \"silver\", \"simpli\", \"simpli\", \"sin\", \"site\", \"size\", \"skill\", \"skip\", \"skip\", \"small\", \"small\", \"small\", \"smaller\", \"smarter\", \"smash\", \"sneak\", \"softwar\", \"somebodi\", \"sort\", \"sort\", \"soundtrack\", \"sour\", \"span\", \"spare\", \"spec\", \"specif\", \"speed\", \"speed\", \"staff\", \"stare\", \"start\", \"starter\", \"startup\", \"starv\", \"statement\", \"statu\", \"steel\", \"steer\", \"still\", \"stori\", \"stori\", \"strang\", \"strategi\", \"straw\", \"stress\", \"structur\", \"studio\", \"stutter\", \"substanc\", \"sum\", \"superb\", \"surround\", \"surviv\", \"surviv\", \"surviv\", \"suspect\", \"swarm\", \"swear\", \"swim\", \"t\", \"t\", \"tab\", \"tactic\", \"tank\", \"tap\", \"tbh\", \"tediou\", \"teen\", \"temporari\", \"tendenc\", \"terrain\", \"tester\", \"text\", \"tf\", \"theori\", \"thing\", \"thing\", \"thing\", \"thru\", \"ti\", \"tier\", \"tier\", \"tiger\", \"tile\", \"tilt\", \"time\", \"tinker\", \"tip\", \"tire\", \"tissu\", \"tool\", \"torrent\", \"toy\", \"trace\", \"tragic\", \"transact\", \"transfer\", \"transit\", \"tree\", \"tri\", \"trial\", \"trust\", \"type\", \"type\", \"ultra\", \"un\", \"unaccept\", \"unbalanc\", \"unclear\", \"undoubtedli\", \"unfulfil\", \"unit\", \"unlik\", \"unlik\", \"unlik\", \"unlimit\", \"unlock\", \"unnecessari\", \"unplay\", \"unpleas\", \"unpolish\", \"unrealist\", \"up\", \"ur\", \"usag\", \"useless\", \"utterli\", \"v\", \"vendor\", \"verifi\", \"videogam\", \"viru\", \"visitor\", \"visual\", \"visual\", \"visual\", \"voic\", \"w\", \"w\", \"wall\", \"wall\", \"wall\", \"want\", \"warn\", \"wasteland\", \"waster\", \"water\", \"way\", \"way\", \"weapon\", \"web\", \"websit\", \"weigh\", \"welcom\", \"whatev\", \"whatev\", \"whenev\", \"wherea\", \"whistl\", \"wife\", \"woke\", \"women\", \"work\", \"would\", \"written\", \"xbox\", \"xd\", \"year\", \"yesterday\", \"your\", \"yr\", \"z\", \"zombi\", \"zoom\", \"\\u0432\", \"\\u0432\\u0430\\u0441\", \"\\u0434\\u043e\", \"\\u0435\\u0441\\u043b\\u0438\", \"\\u0435\\u0441\\u0442\\u044c\", \"\\u0437\\u0430\", \"\\u0438\", \"\\u0438\\u0433\\u0440\\u0430\", \"\\u0438\\u0433\\u0440\\u0443\", \"\\u0438\\u0437\", \"\\u043d\\u0430\", \"\\u043d\\u0435\", \"\\u043d\\u0435\\u0442\", \"\\u043d\\u043e\", \"\\u043e\\u0442\", \"\\u0441\", \"\\u0442\\u043e\", \"\\u0443\", \"\\u0447\\u0442\\u043e\", \"\\u044d\\u0442\\u043e\", \"\\u044f\", \"\\u2019\", \"\\u201c\", \"\\u201d\"]}, \"R\": 25, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [5, 10, 18, 20, 19, 14, 4, 1, 12, 6, 11, 15, 23, 17, 22, 13, 8, 2, 9, 21, 7, 25, 3, 16, 24]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el101225321843242404857111369\", ldavis_el101225321843242404857111369_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el101225321843242404857111369\", ldavis_el101225321843242404857111369_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el101225321843242404857111369\", ldavis_el101225321843242404857111369_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "4     -0.223040  0.503232       1        1  70.217362\n",
       "9     -0.533530  0.089889       2        1  13.017305\n",
       "17    -0.416266  0.339372       3        1   9.512651\n",
       "19     0.384612  0.176859       4        1   0.772432\n",
       "18    -0.175929 -0.393091       5        1   0.763816\n",
       "13     0.397560 -0.024293       6        1   0.568834\n",
       "3      0.094461  0.344308       7        1   0.522706\n",
       "0      0.243238  0.258174       8        1   0.459649\n",
       "11    -0.316229 -0.109004       9        1   0.455742\n",
       "5      0.251880 -0.299632      10        1   0.421383\n",
       "10     0.015490 -0.350323      11        1   0.384432\n",
       "14    -0.238615 -0.231741      12        1   0.351074\n",
       "22     0.291916 -0.135010      13        1   0.300729\n",
       "16    -0.043912  0.197423      14        1   0.287298\n",
       "21     0.134998 -0.226607      15        1   0.281674\n",
       "12    -0.181896  0.032869      16        1   0.260835\n",
       "7      0.003189 -0.229677      17        1   0.242143\n",
       "1      0.192469  0.042498      18        1   0.238469\n",
       "8      0.088116  0.198760      19        1   0.198741\n",
       "20     0.135592 -0.076651      20        1   0.153324\n",
       "6     -0.037970 -0.123146      21        1   0.146863\n",
       "24    -0.070942  0.082497      22        1   0.144668\n",
       "2      0.065445  0.005546      23        1   0.120323\n",
       "15    -0.069244 -0.050795      24        1   0.090608\n",
       "23     0.008607 -0.021459      25        1   0.086940, topic_info=        Term          Freq         Total Category  logprob  loglift\n",
       "90      game  23950.000000  23950.000000  Default  25.0000  25.0000\n",
       "656     shit   1170.000000   1170.000000  Default  24.0000  24.0000\n",
       "974     fuck   1093.000000   1093.000000  Default  23.0000  23.0000\n",
       "69     enemi   1869.000000   1869.000000  Default  22.0000  22.0000\n",
       "176     play  12210.000000  12210.000000  Default  21.0000  21.0000\n",
       "...      ...           ...           ...      ...      ...      ...\n",
       "1846      dl      2.835122      3.723159  Topic25  -5.7638   6.7752\n",
       "2802  humili      2.694243      3.582279  Topic25  -5.8148   6.7628\n",
       "1848  farcri      2.515214      3.403251  Topic25  -5.8835   6.7453\n",
       "4364    crow      2.423989      3.312025  Topic25  -5.9205   6.7356\n",
       "2807  marvel      4.934302     11.767684  Topic25  -5.2097   6.1786\n",
       "\n",
       "[754 rows x 6 columns], token_table=      Topic      Freq  Term\n",
       "term                       \n",
       "3240     14  0.988865     #\n",
       "3573     20  0.900524    03\n",
       "1861     24  0.884311  10hr\n",
       "1862      7  0.982191   150\n",
       "3918     13  0.979443  16gb\n",
       "...     ...       ...   ...\n",
       "1815     22  0.956057   это\n",
       "1818     22  0.921439     я\n",
       "844      21  0.994841     ’\n",
       "1065     11  0.986037     “\n",
       "1066     11  0.990471     ”\n",
       "\n",
       "[796 rows x 3 columns], R=25, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[5, 10, 18, 20, 19, 14, 4, 1, 12, 6, 11, 15, 23, 17, 22, 13, 8, 2, 9, 21, 7, 25, 3, 16, 24])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_lda = lda_vis(neg_fd)\n",
    "negative_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "10e8d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(positive_lda, 'positive_lda.html')\n",
    "pyLDAvis.save_html(negative_lda, 'negative_lda.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4ab4a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_fd = FrequencyDistribution(df_big['review_stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "12f571b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el101225321902014408831930137\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el101225321902014408831930137_data = {\"mdsDat\": {\"x\": [-0.13542893743146953, -0.5373174224574682, -0.39287816820611066, 0.4591577805851548, -0.21764806471291448, 0.16677937240447965, 0.08047575040476661, 0.28339782462309543, -0.36407995156526407, 0.3301810306942853, -0.011291059147228446, -0.2887672411540755, 0.38295200750371317, -0.09852286968979555, 0.1840027386072752, -0.268451051942487, 0.08734970554238307, 0.34687993640487214, 0.07575972616654227, 0.2072384291969679, -0.08243045022433927, -0.08410756413369135, 0.1377936943454447, -0.051780151970501155, -0.20926506384363508], \"y\": [0.5375371702441729, 0.10351252158361407, 0.3849395172465274, 0.2743671252792186, -0.4087809368474129, 0.4164572794263133, 0.30620802630050437, 0.18572737409708828, -0.12323481843259596, -0.27457330865695345, -0.4323183521565877, -0.2552740254552374, -0.11521066002400432, 0.26333355654468527, -0.37579536527930685, 0.11181224472886475, -0.28138733953257544, 0.022552787283399635, 0.1154210477221951, -0.1281553885266689, -0.23653897503401783, 0.0803561776272944, -0.024758696495069482, -0.09163264429679745, -0.054564317346650536], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [61.85306666774524, 11.78931768687404, 10.910449835955205, 4.4643745164570054, 1.0182592668805621, 1.0169245319466427, 0.6973250226788873, 0.6403594678545999, 0.5981705984472028, 0.5882870740695193, 0.5781302206865432, 0.5533492778034437, 0.5236505415163749, 0.5216261118632861, 0.518786430983714, 0.5066884035333553, 0.4881450483800034, 0.4520512256370022, 0.387689290509604, 0.3690669041047953, 0.3413220117369101, 0.31896775825432133, 0.31379669386272024, 0.2757881982208212, 0.2744072139981924]}, \"tinfo\": {\"Term\": [\"game\", \"play\", \"fun\", \"good\", \"like\", \"friend\", \"great\", \"nice\", \"recommend\", \"crash\", \"love\", \"realli\", \"10\", \"bad\", \"build\", \"best\", \"dont\", \"spend\", \"charact\", \"ye\", \"run\", \"look\", \"also\", \"wast\", \"much\", \"make\", \"even\", \"would\", \"want\", \"go\", \"tri\", \"still\", \"thing\", \"way\", \"player\", \"feel\", \"work\", \"better\", \"enjoy\", \"new\", \"everi\", \"peopl\", \"mani\", \"first\", \"start\", \"well\", \"updat\", \"year\", \"see\", \"say\", \"hour\", \"lot\", \"much\", \"use\", \"get\", \"time\", \"need\", \"one\", \"also\", \"like\", \"enemi\", \"combat\", \"item\", \"skill\", \"explor\", \"weapon\", \"limit\", \"due\", \"resourc\", \"reward\", \"quest\", \"campaign\", \"upgrad\", \"attack\", \"abil\", \"area\", \"craft\", \"power\", \"boss\", \"unlock\", \"certain\", \"stay\", \"uniqu\", \"damag\", \"action\", \"build\", \"fight\", \"design\", \"balanc\", \"style\", \"level\", \"place\", \"difficulti\", \"challeng\", \"system\", \"map\", \"differ\", \"gener\", \"interest\", \"requir\", \"base\", \"mechan\", \"game\", \"fun\", \"good\", \"great\", \"best\", \"ever\", \"amaz\", \"fuck\", \"15\", \"wow\", \"play\", \"love\", \"10\", \"hesit\", \"goofi\", \"pleas\", \"recommend\", \"graphic\", \"underr\", \"defiantli\", \"pc\", \"brrrrrr\", \"hate\", \"priceless\", \"buy\", \"surviv\", \"like\", \"worth\", \"definit\", \"realli\", \"zombi\", \"stori\", \"overal\", \"get\", \"high\", \"longer\", \"man\", \"object\", \"path\", \"lol\", \"act\", \"complex\", \"combin\", \"gave\", \"immers\", \"voic\", \"weird\", \"lead\", \"sadli\", \"npc\", \"promis\", \"sorri\", \"ball\", \"thousand\", \"pve\", \"pure\", \"hero\", \"situat\", \"size\", \"human\", \"music\", \"charact\", \"stori\", \"cut\", \"line\", \"kid\", \"main\", \"crash\", \"ok\", \"stuck\", \"rate\", \"went\", \"lock\", \"glitch\", \"window\", \"funni\", \"polish\", \"biggest\", \"smooth\", \"pre\", \"doubt\", \"googl\", \"ugli\", \"70\", \"depress\", \"stabl\", \"textur\", \"killer\", \"clip\", \"resolv\", \"announc\", \"lover\", \"w\", \"cup\", \"adult\", \"dri\", \"potato\", \"spare\", \"usag\", \"ark\", \"audienc\", \"coffe\", \"nasa\", \"teen\", \"boi\", \"infin\", \"ms\", \"eargasm\", \"leaderboard\", \"do\", \"mehh\", \"grandma\", \"terrarium\", \"deaf\", \"throat\", \"\\u2018\", \"don\", \"paint\", \"rich\", \"audio\", \"averag\", \"signific\", \"dark\", \"master\", \"soul\", \"press\", \"brain\", \"beauti\", \"short\", \"kid\", \"launch\", \"kick\", \"third\", \"block\", \"count\", \"aliv\", \"anti\", \"boy\", \"exact\", \"automat\", \"effici\", \"brutal\", \"six\", \"mass\", \"wild\", \"protect\", \"men\", \"\\u201d\", \"\\u201c\", \"sent\", \"weight\", \"foot\", \"format\", \"adapt\", \"greatli\", \"\\u2019\", \"v\", \"it\", \"t\", \"don\", \"sim\", \"car\", \"vr\", \"alot\", \"complic\", \"race\", \"app\", \"angri\", \"c\", \"mob\", \"ill\", \"paus\", \"14\", \"aggress\", \"trial\", \"cargo\", \"95\", \"sniper\", \"customis\", \"underwhelm\", \"silver\", \"titan\", \"swap\", \"girlfriend\", \"street\", \"drive\", \"earli\", \"access\", \"strategi\", \"war\", \"hang\", \"factorio\", \"tend\", \"greatest\", \"filter\", \"fi\", \"aswel\", \"sci\", \"outstand\", \"revisit\", \"videogam\", \"newli\", \"ez\", \"wich\", \"til\", \"infrastructur\", \"34\", \"yellow\", \"dual\", \"fav\", \"leap\", \"star\", \"ye\", \"suck\", \"kinda\", \"imagin\", \"shitti\", \"expens\", \"season\", \"11\", \"benefit\", \"nonsens\", \"ubisoft\", \"profit\", \"ps\", \"hello\", \"z\", \"evolut\", \"obtus\", \"filler\", \"redempt\", \"summar\", \"noon\", \"zen\", \"haunt\", \"thou\", \"torment\", \"dont\", \"cant\", \"addict\", \"trash\", \"ton\", \"ive\", \"scare\", \"polici\", \"bite\", \"swamp\", \"woke\", \"ca\", \"panic\", \"don\\u00b4t\", \"rep\", \"pong\", \"junki\", \"derang\", \"heartfelt\", \"idk\", \"didnt\", \"doesnt\", \"retard\", \"isnt\", \"your\", \"server\", \"rng\", \"mod\", \"crap\", \"toxic\", \"terribl\", \"20\", \"multipl\", \"buggi\", \"friend\", \"multiplay\", \"ass\", \"hot\", \"truck\", \"nobodi\", \"euro\", \"chines\", \"buddi\", \"digit\", \"grim\", \"enthusiast\", \"breaker\", \"pod\", \"panel\", \"suspens\", \"goo\", \"funki\", \"scrape\", \"milki\", \"craze\", \"norman\", \"server\", \"singleplay\", \"shark\", \"lag\", \"mod\", \"terribl\", \"spend\", \"month\", \"pass\", \"skin\", \"earn\", \"practic\", \"currenc\", \"children\", \"300\", \"wife\", \"own\", \"forgot\", \"yea\", \"obtain\", \"dad\", \"ps5\", \"purpl\", \"oblig\", \"defi\", \"53\", \"vibrat\", \"exchang\", \"loneli\", \"recipi\", \"dualsens\", \"download\", \"cost\", \"realist\", \"account\", \"expans\", \"sandbox\", \"<\", \"consol\", \"puzzl\", \"corrupt\", \"logist\", \"gen\", \"pretend\", \"immens\", \"monoton\", \"weather\", \"divis\", \"exercis\", \"remark\", \"backup\", \"photo\", \"os\", \"mr\", \"san\", \"breathtak\", \"wast\", \"refund\", \"unplay\", \"fp\", \"meh\", \"rip\", \"text\", \"request\", \"overpr\", \"stutter\", \"perspect\", \"gta\", \"cheaper\", \"4k\", \"downgrad\", \"amus\", \"braindead\", \"1440p\", \"144\", \"microwav\", \"dlss\", \"\\u2705\", \"126\", \"exhilar\", \"\\ud83d\\udd32\", \"float\", \"shot\", \"shoot\", \"gun\", \"fire\", \"harder\", \"instantli\", \"cross\", \"wheel\", \"bullet\", \"bodi\", \"teleport\", \"ammo\", \"thrown\", \"feet\", \"explos\", \"rifl\", \"gap\", \"horrend\", \"gunplay\", \"unreward\", \"backpack\", \"shower\", \"grenad\", \"jerk\", \"55\", \"screen\", \"playabl\", \"cheap\", \"faster\", \"prefer\", \"board\", \"dedic\", \"session\", \"hurt\", \"reinstal\", \"minimum\", \"babi\", \"resolut\", \"render\", \"applic\", \"email\", \"newest\", \"unreal\", \"sooner\", \"nativ\", \"banger\", \"slaughter\", \"notif\", \"wasteland\", \"dang\", \"nice\", \"highli\", \"relax\", \"grindi\", \"stress\", \"hype\", \"scam\", \"goog\", \"enviro\", \"thingi\", \"dinner\", \"todd\", \"howard\", \"enthral\", \"refreshingli\", \"drifter\", \"chill\", \"provok\", \"cozi\", \"bunker\", \"environment\", \"vastli\", \"humor\", \"poe\", \"uninterest\", \"recomend\", \"setup\", \"20\", \"toxic\", \"mod\", \"ship\", \"pack\", \"pvp\", \"casual\", \"server\", \"terribl\", \"multipl\", \"buggi\", \"low\", \"space\", \"awesom\", \"planet\", \"shooter\", \">\", \"rock\", \"laptop\", \"bang\", \"nut\", \"dc\", \"omg\", \"forgotten\", \"healthi\", \"birth\", \"ost\", \"8th\", \"boomer\", \"10\\u20ac\", \"scanner\", \"50fp\", \"ly\", \"brother\", \"powerhous\", \"crafter\", \"terraform\", \"ship\", \"mod\", \"im\", \"gon\", \"platform\", \"epic\", \"hr\", \"gay\", \"languag\", \"murder\", \"scienc\", \"american\", \"fellow\", \"trailer\", \"beaten\", \"aint\", \"mother\", \"whale\", \"spice\", \"demon\", \"van\", \"undeni\", \"ambiti\", \"og\", \"36\", \"mach\", \"squid\", \"na\", \"solo\", \"bot\", \"mmo\", \"social\", \"internet\", \"popular\", \"met\", \"tip\", \"linux\", \"network\", \"establish\", \"fighter\", \"void\", \"conveni\", \"media\", \"carrier\", \"proton\", \"viru\", \"fashion\", \"east\", \"smoother\", \"predatori\", \"versa\", \"vice\", \"inclus\", \"pro\", \"con\", \"com\", \"faction\", \"strateg\", \"share\", \"www\", \"nation\", \"smart\", \"induc\", \"twitch\", \"intel\", \"nvidia\", \"amd\", \"url\", \"geforc\", \"steamcommun\", \"orang\", \"http\", \"quot\", \"youtu\", \"|\", \"filedetail\", \"sharedfil\", \"juic\", \"v\", \"youtub\", \"yeah\", \"wan\", \"ram\", \"neat\", \"clean\", \"gpu\", \"ring\", \"driver\", \"surprisingli\", \"ultra\", \"rtx\", \"@\", \"elden\", \"ti\", \"cure\", \"3080\", \"christma\", \"q\", \"pan\", \"weav\", \"3070\", \"sway\", \"moss\", \"unforgett\", \"poignant\", \"na\", \"op\", \"e\", \"art\", \"co\", \"g\", \"l\", \"xd\", \"k\", \"extens\", \"urg\", \"egg\", \"artist\", \"humour\", \"mama\", \"bb\", \"easter\", \"sam\", \"jaw\", \"vehicular\", \"recal\", \"sem\", \"ser\", \"ce\", \"intimid\", \"ir\", \"f\", \"de\", \"terribl\", \"pack\", \"low\", \"multipl\", \"ship\", \"mod\", \"20\", \"shit\", \"eat\", \"p\", \"everytim\", \"virtual\", \"h\", \"smoke\", \"hill\", \"drug\", \"everybodi\", \"moron\", \"barrier\", \"luckili\", \"lockdown\", \"pandem\", \"primal\", \"lama\", \"dog\", \"battal\", \"bulli\", \"carnivor\", \"shall\", \"jack\", \"nois\", \"endur\", \"holi\", \"r\", \"20\", \"compani\", \"refus\", \"server\", \"forev\", \"mod\", \"launcher\", \"terribl\", \"delet\"], \"Freq\": [65417.0, 29125.0, 19853.0, 19789.0, 20623.0, 4897.0, 10564.0, 3523.0, 8392.0, 3704.0, 7986.0, 11186.0, 6894.0, 6815.0, 6864.0, 6427.0, 2724.0, 2642.0, 4728.0, 2679.0, 6060.0, 7055.0, 7249.0, 2580.0, 10043.0, 13069.000242359813, 12398.57546527782, 10081.342275200283, 9888.56636415452, 9882.852534106922, 8637.097261321049, 8626.057334199786, 8599.84535426364, 8183.598620518311, 7903.9483671417975, 7687.305952927594, 7437.481596983941, 7348.4919558731835, 7283.370048829195, 7156.953097649012, 6944.094464901055, 6929.772962832163, 6671.627734178092, 6636.273256896291, 6476.262628495292, 5972.24607388418, 5759.294508312312, 5680.737345996991, 5611.75657268863, 5609.269542612979, 10875.096295001795, 7918.103697936147, 9942.219991458423, 6869.958073937726, 18531.55224308602, 16597.75311452334, 7604.36371531185, 12140.17913336444, 7121.2047329016295, 9389.858058591288, 3324.302181978182, 2821.0568588004267, 2543.9000011674325, 2265.721409698464, 2018.1762989865638, 1966.26705866225, 1788.8241504585092, 1784.5396720859333, 1698.0350636823691, 1756.5031079374533, 1667.6855803554836, 1646.7654920845273, 1635.8667241205135, 1632.1168084448993, 1591.9349845136837, 1554.4163549886828, 1548.448068638888, 1630.9559498868277, 1501.7368845418653, 1494.3866181321832, 1392.5784076037735, 1366.4817300736067, 1317.7935907139345, 1305.7905056050452, 1283.1034395187864, 6693.073598906068, 2884.5866499007807, 3047.433896702995, 1997.765318046047, 1719.054275385174, 3335.7920540523537, 2054.8167770194746, 2003.185316916009, 2386.9403342994965, 2802.427753516449, 2735.605891420367, 2563.109301118773, 1881.3476040274027, 1986.1311597929719, 1820.7138594187295, 1888.4056615983461, 1783.923464526763, 65416.59393791226, 19852.383307157994, 19787.932846555068, 10563.479996609325, 6425.762162876651, 5058.017172856021, 3566.6733780825916, 2941.3790651498452, 813.5875690077214, 605.7231089814524, 29049.107086814958, 7899.444829209294, 6802.201574948921, 86.1640740527298, 55.05904001855282, 1513.1139549043867, 8040.402962981514, 3828.5523352672376, 20.34205449673728, 17.75438229076961, 2170.422021421754, 13.589121969083772, 1808.7801172374745, 5.5911718374734765, 4769.380548699999, 1920.1873109765463, 11232.082526507986, 2868.5639838936136, 1168.670058178324, 4669.610870921903, 996.4499553513227, 1176.2047304228768, 961.0555054426962, 1084.4188415102647, 2156.066786942652, 1389.7884268946668, 1374.5387615341253, 1254.1779204960214, 1076.3162424874024, 1031.198473523989, 957.5778461548248, 953.2137168795608, 939.2579560580232, 898.1055904998485, 890.5315758226202, 845.2729912388883, 836.6705338986891, 806.3820268161023, 795.0560088093487, 768.9135222651917, 763.3827111438553, 669.7058526788309, 660.5317372257355, 647.8562435944832, 646.9925596494182, 636.133232149626, 628.6330457250696, 616.1462094879793, 611.4785492557318, 803.0052060509969, 1150.660137973295, 3850.7217425685285, 3395.871119574638, 756.7007656424123, 1135.1529177336974, 855.3092803117833, 1028.075920459421, 3703.0932547645584, 1668.2030230488494, 1250.0940144297545, 1164.9752582740994, 965.1205263590535, 866.3531776932952, 864.6950374747557, 857.6548514767272, 734.1723218315097, 653.2258264511004, 620.4250132919282, 473.8073059561008, 398.957517464183, 382.07207832519236, 380.9220713143581, 296.235520182915, 292.1730482782765, 282.9029607525441, 282.1558371535616, 256.1509712959564, 236.66107590716837, 198.73637684512852, 188.1637775023254, 166.92703743189662, 155.59921913134247, 420.58806664457427, 340.98550241153185, 337.4390202162099, 319.12033857323127, 309.6631876334719, 306.0615591638712, 305.7756080242167, 298.992211544634, 279.6935451926837, 217.98931645104415, 211.6486686645515, 210.86470113453896, 207.0179196496676, 191.96778297964607, 187.73821552120316, 185.73336007157147, 178.83287135603405, 173.26692247804215, 171.58521407710143, 164.74467796250008, 164.3500532474779, 161.962280282703, 72.13270794309628, 199.38668714773343, 186.44050732372637, 213.59926827515886, 189.82460650810788, 198.89650254018125, 203.1036876942747, 186.4802328783719, 197.8281421322094, 195.12002496607843, 193.7412062117909, 189.80594784209504, 191.1227599232119, 191.89903733057272, 191.32217694586978, 190.19112180112157, 1406.7255569217036, 652.5081650534529, 642.8005793971784, 599.4253463956957, 476.1098779218939, 387.17114473039135, 378.5879029396102, 355.37967040750664, 323.44340326424526, 322.2455944580701, 269.99986536431135, 249.9137401639714, 248.25135650246085, 226.8286661790063, 211.7246962362155, 183.47212552264978, 177.49534877692, 171.5075398007705, 170.0273614039279, 165.06721509914058, 157.43540951218006, 147.70174893646072, 145.79457465280677, 145.1787541630756, 143.50844672348998, 579.6435442307074, 365.95201640445504, 262.2222678115777, 385.4399310526772, 158.10987428587822, 1516.9887802688766, 1016.1844811720304, 923.2426975929972, 693.2414829663724, 553.0908636989683, 529.9200777362191, 495.3105751455494, 292.6299531232175, 285.6589650887061, 284.6171934210377, 263.8011809853643, 252.02282767870733, 203.05289450829642, 191.20297435533212, 184.18749916618057, 164.45555369071366, 164.41908212480703, 163.55416314464813, 161.9898107730318, 136.3954624946801, 132.42124694559868, 127.5620798162168, 125.20296079509004, 123.46799999427803, 123.32252151894956, 240.8865625340949, 2271.2120176860453, 1957.51708692238, 1450.9952803922677, 1407.2696319129682, 360.03870334635695, 291.1536607426717, 277.4231389830903, 173.69415477650497, 126.95649700486561, 116.2824095587837, 116.12227949859077, 108.22355359011813, 103.81182282455163, 94.03345546279226, 93.89015739078792, 84.54202945554897, 73.94960215018394, 63.84104688999821, 62.840936338617844, 62.771477603766094, 48.82418447720369, 45.980789686062025, 45.11848657404669, 43.314538476413524, 38.12117969922141, 527.2117245037103, 2678.593736352944, 2150.1644631820163, 1130.603037292294, 638.7083292859365, 507.252348594206, 479.4645510712944, 446.78982498663896, 409.9424922732092, 264.11449383905915, 261.86965263926044, 234.09487343923894, 196.0658469525352, 160.5414310600622, 94.92563594568725, 83.87221348475175, 74.27239448004475, 63.74864917813765, 52.779974594361704, 37.69983384160783, 37.209691643370675, 31.770818971036384, 29.29770054278979, 27.082668609770643, 23.755325179204284, 12.190180630362258, 2723.507008896412, 2097.6304147060237, 1754.3490150694472, 1133.0297814221926, 1117.3446929779093, 612.6005146560456, 135.29058374186062, 135.00343045044366, 94.71280590318517, 88.93710614275365, 64.763335426891, 52.82812911081705, 35.55590292633778, 34.95816065879634, 34.004344398197944, 15.065604603263225, 10.824855184616279, 3.5812580192197494, 1.7966581688315186, 0.047490965340030715, 0.047490965340030715, 0.04749097095770643, 0.04749095972235501, 0.047490965340030715, 0.04749095972235501, 0.04749099904608498, 0.04749097095770643, 0.047491004663760696, 0.04749097095770643, 0.04749097095770643, 0.04749097657538214, 0.04749097657538214, 0.04749097095770643, 0.04749097095770643, 4896.163169015528, 1758.254714675962, 906.0304525050359, 256.3589990307679, 237.26299729353724, 207.31153569479744, 175.46440611515376, 132.6603755103506, 130.19532880520586, 111.81283989892684, 107.40290224267447, 83.22519043626156, 80.05213682524389, 74.59583114333076, 72.68133840458273, 40.459739848484986, 37.54295952025647, 22.52544012602033, 12.353573898585093, 10.215611490426324, 7.646146980885527, 0.5970871631544115, 0.052989850997139876, 0.05298979722834028, 0.05298979185146032, 0.05298980260522024, 0.052989834866499996, 0.0529898079821002, 2641.375947957802, 1432.3990707525936, 938.5587204281693, 563.6648190710921, 497.17639294544836, 494.9835575629909, 382.8796786916897, 243.2385151409628, 214.35385823155713, 211.32181219738004, 203.26271387054703, 176.8055486970412, 152.72694944100496, 149.79771536095623, 111.69744548037124, 37.106432279914884, 34.3675373489786, 30.295413865818386, 21.576581618915707, 16.329185989135592, 11.954019425493913, 34.14694130661544, 3.652902099925052, 2.781983453737116, 1.0767871998193854, 1296.5156278727927, 1134.1097166521117, 1101.0826338602299, 980.682065717879, 804.6924188887211, 683.8974729775306, 422.8565938886252, 417.00981267067453, 270.1335745333785, 177.21534038131418, 130.9477755522919, 125.69217690733537, 120.81702533618325, 116.2962033807812, 101.23450614954503, 85.14351532894595, 78.06608903757827, 70.35923304515552, 69.49357840999325, 67.09038299845375, 61.177353124977316, 61.126514392536464, 40.16561417738205, 38.44849771092267, 25.16638967743983, 2579.4089523927273, 1558.843862675307, 1185.9093333127591, 849.9684716284289, 449.48643811273763, 356.6498062925486, 251.22309389083117, 235.52182345027467, 227.23566872734142, 216.4123199568916, 167.4572229607622, 148.3006521825005, 107.1507233475124, 101.6479083632727, 76.77117064890697, 64.94370603566404, 62.37842544537223, 40.23244456803128, 22.056262414428826, 14.832106538474358, 11.20216479015132, 6.9753685217991315, 4.214632294332543, 4.10240162339605, 3.37699929188409, 12.234307812506657, 1257.316852220539, 999.1329663006056, 898.0853327194403, 769.3102871441042, 619.3851543803307, 419.4768704032908, 342.6309284551546, 313.31205190673194, 308.73970704737553, 291.68782863518055, 222.8831628636886, 206.0896786343416, 198.01387730035444, 194.09303345333592, 186.49328402603015, 133.98659992635254, 130.2034432941467, 123.23928346028656, 104.72017661055204, 84.38960816132347, 71.59818861497561, 68.01126777413383, 66.67302467909762, 63.25672305186585, 52.0302339033598, 1531.6640852276792, 766.5565711666692, 561.1985389013176, 508.22231360672635, 506.58798608704296, 501.24576343568435, 346.482019304758, 326.4958956778174, 313.26602217739077, 281.5629825742781, 234.3287412606341, 219.3807838694191, 193.38283347835372, 174.49725807632962, 149.49317387962228, 142.6267543023193, 108.79604835946202, 92.06156038402257, 89.5658012103749, 84.39800785611229, 83.09850182133698, 82.05198432433575, 75.68218698781246, 73.09970373587204, 63.96721872126777, 3522.3972008482433, 1172.6031312545397, 921.0042153705061, 512.4007937237899, 421.7931151275038, 353.7435820598264, 202.1336430827569, 31.6159483124406, 30.237075617206237, 25.42305106134077, 11.239036144665054, 9.063867200462495, 5.813391613378171, 4.83234431175956, 3.7363165479337126, 1.7080150283408975, 0.052263345704909155, 0.0522633369197701, 0.0522633369197701, 0.0522633369197701, 0.0522633369197701, 0.0522633369197701, 0.0522633369197701, 0.0522633369197701, 0.0522633369197701, 0.05226334131233963, 0.05226334131233963, 0.052263354490048215, 0.052263345704909155, 0.0522633676677568, 0.052263350097478685, 0.052263345704909155, 0.052263345704909155, 0.052263345704909155, 0.052263354490048215, 0.052263345704909155, 0.052263345704909155, 0.052263345704909155, 0.052263345704909155, 1562.783000363569, 1484.0239652810478, 797.1056695779612, 475.49707985757493, 413.4400319713831, 407.5448807553441, 230.588133878531, 113.86712556659168, 111.46251182479382, 82.33315084903225, 75.88233861888307, 52.36880546928868, 48.496838364148736, 31.653821961362535, 19.115473368304603, 16.711422832944077, 12.531540931686054, 11.295515932929776, 10.544543798780587, 4.4734826707558835, 2.134319255124604, 94.06304360433819, 1.7406465144264711, 0.047856728842342953, 0.047856732609509296, 0.047856751445341024, 0.047856751445341024, 1300.9497556151123, 703.3220745186735, 473.29636719305023, 420.0789162977966, 419.2960151033947, 216.4889258591099, 175.53581063568578, 163.20105527970372, 131.5882430315946, 98.78799097799454, 97.37334707057833, 96.55520665584231, 89.42180575748202, 76.51880879290603, 65.98321467268002, 62.02200461519657, 44.110943768477, 40.03381812222242, 37.94069803805986, 35.552477025715476, 31.182986301544695, 30.311491298205606, 18.943734153526403, 12.06349672021578, 10.909310348439991, 781.3616561756087, 680.7786350443769, 622.1439787758977, 596.5533043847386, 412.6326910266855, 378.34925349032284, 282.88723121682085, 227.63566752592985, 183.2321732954334, 181.05962779008786, 170.73837928650596, 129.66323063885935, 111.21265794299504, 102.16719781245762, 87.60417117535228, 81.8256530504395, 78.84615314448475, 70.7816966692931, 64.61742279683904, 59.070037517702694, 58.739864233926205, 54.27997434506224, 53.989811876733214, 48.09841373543583, 45.414958543150476, 40.822857378965736, 792.874581414409, 715.6084648599987, 456.20507704887734, 279.09724210315613, 255.25039904547347, 243.57309832780197, 172.9716545544473, 172.138180081051, 158.12908557824338, 125.86503011293178, 91.44778253535469, 69.9625922481009, 64.21504226276053, 53.91714541667677, 51.966964570807335, 44.97346527091334, 44.84143257032174, 44.36087048304555, 336.95486587548555, 31.712552897671408, 29.066487579512533, 21.738880579708013, 18.734464591915078, 18.734464591915078, 18.625049882255574, 80.92765079682064, 62.34883663924039, 717.3067549516471, 578.495942627697, 300.2966612705047, 291.8589166936019, 258.37743325049576, 224.07555038855418, 190.56787681054828, 173.66723413662416, 158.4008458889798, 152.73819401800486, 122.72321285071365, 114.00831744712433, 111.5183937207706, 98.6874569661154, 68.71240420191178, 56.091194600477785, 55.896379712907674, 45.25518240236945, 39.34389285586507, 37.61212794878885, 35.89395456021751, 18.317876175508413, 5.029518317457046, 4.1749472982546845, 2.372795427905978, 500.80054071851134, 754.9013393457373, 654.4266108646759, 627.7757643355498, 529.9036243131231, 220.1593531041711, 197.757799941924, 180.91371668474852, 156.26102285584233, 122.88859735392735, 78.02442837509003, 69.72966006613362, 57.46616498679988, 24.278752302540468, 23.442463961640755, 19.366519435593396, 16.22101340050809, 10.118989038107001, 8.700821788455512, 0.04604966623461783, 0.04604966623461783, 0.04604966623461783, 0.04604966623461783, 0.04604966623461783, 0.04604966623461783, 0.04604966623461783, 0.04604967695392367, 0.04604967159427075, 0.04604967695392367, 0.04604967159427075, 0.04604967159427075, 0.04604967159427075, 0.04604967159427075, 0.04604967695392367, 0.04604967159427075, 2141.888382428573, 501.1156532324025, 293.64100272937185, 196.69356286201506, 184.89991137721142, 116.7676628365395, 102.03902659389611, 96.32218890207703, 90.04392610466297, 84.59091616429424, 73.93434028120755, 59.00881480422677, 51.79372403297408, 10.268572213061518, 10.080617534223506, 4.164961579581955, 2.1328711981612773, 0.03966928520432248, 0.03966927453869258, 0.03966927453869258, 0.03966927453869258, 0.03966927453869258, 0.03966927453869258, 0.03966927453869258, 0.03966927453869258, 0.03966927720510006, 0.03966927720510006, 0.03966928520432248, 0.03966927987150753, 0.03966927720510006, 0.03966928787072995, 0.03966927720510006, 0.03966928787072995, 0.03966927987150753, 0.03966927987150753, 0.03966927720510006], \"Total\": [65417.0, 29125.0, 19853.0, 19789.0, 20623.0, 4897.0, 10564.0, 3523.0, 8392.0, 3704.0, 7986.0, 11186.0, 6894.0, 6815.0, 6864.0, 6427.0, 2724.0, 2642.0, 4728.0, 2679.0, 6060.0, 7055.0, 7249.0, 2580.0, 10043.0, 13070.296989240287, 12399.872212158294, 10082.639022080757, 9889.863111034994, 9884.149280987396, 8638.394008201523, 8627.35408108026, 8601.142101144114, 8184.895367398783, 7905.245114022269, 7688.602699808066, 7438.778343864413, 7349.788702753655, 7284.666795709667, 7158.249844529484, 6945.391211781527, 6931.069709712635, 6672.924481058564, 6637.570003776763, 6477.559375375764, 5973.542822633354, 5760.591255192784, 5682.034092877463, 5613.053319569101, 5610.566289493451, 10923.413088649551, 7952.51798724972, 10043.292077790704, 6900.734805025243, 19675.13928089548, 17644.11384194183, 7719.634786213453, 13074.712581848133, 7249.841661952374, 20623.178538297794, 3325.593693115041, 2822.3483701063237, 2545.1915123042913, 2267.012920872887, 2019.4678101234233, 1967.5585697991096, 1790.1156615953687, 1785.8311832227928, 1699.3265748192287, 1757.8496990230121, 1668.977091492343, 1648.0570032213868, 1637.158235257373, 1633.4083195817589, 1593.2264956505433, 1555.7078661255423, 1549.7395797757474, 1632.317992196684, 1503.0283956787248, 1495.6781292690428, 1393.869918740633, 1367.7732412104663, 1319.085101850794, 1307.0820167419047, 1284.3949507213829, 6864.576565492448, 2914.70029689695, 3086.471657563383, 2066.373622634191, 1786.7395889373315, 3972.932339682856, 2222.614585770435, 2160.9260571091245, 2709.2231622824615, 4055.827595541087, 4278.821822266229, 4051.0786683594933, 2242.763354445741, 3318.372809371731, 2710.955742814774, 5206.171095498373, 4100.140469331291, 65417.896275314306, 19853.68564456006, 19789.235183957135, 10564.782334011381, 6427.064500278705, 5059.319648813022, 3567.975715484647, 2942.6814025519006, 814.889906409777, 607.025446383508, 29125.804877474933, 7986.563317293797, 6894.01891019732, 87.46641145478549, 56.36137742060848, 1557.0185801874964, 8392.141752619957, 4023.30867639383, 21.644391898792914, 19.056719692825244, 2338.911589468354, 14.891459371139415, 2033.8572971591439, 6.8935092395291235, 6427.608752017053, 3133.3979085316764, 20623.178538297794, 4871.347012542381, 1916.8932028895429, 11186.97594137842, 1812.4152049952515, 4764.814514717725, 2763.563818193682, 19675.13928089548, 2157.3528098952347, 1391.0744498472502, 1375.8247844867087, 1255.4639434486048, 1077.6022654399858, 1032.4844964765723, 958.8638691074079, 954.499739832144, 940.5439790106063, 899.3916134524317, 891.8175987752034, 846.5590141914714, 837.9565568512722, 807.6680497686855, 796.3420317619318, 770.1995452177748, 764.6687340964385, 670.991875631414, 661.8177601783186, 649.1422665470664, 648.2785826020014, 637.4192551022091, 629.9190686776527, 617.4322324405624, 612.7645722083149, 807.6753899725759, 1185.5426292650268, 4728.010010329663, 4764.814514717725, 792.9026453071789, 1657.5444486330655, 1046.7352632152574, 1793.8264563141001, 3704.391367791158, 1669.5011360754493, 1251.3921274563543, 1166.2733713006992, 966.4186393856533, 867.6512907198949, 865.9931505013554, 858.9529645033269, 735.4704348581095, 654.5239394777001, 621.7231263185279, 475.10541898270054, 400.2556304907828, 383.37019135179213, 382.22018434095787, 297.5336332095148, 293.47116130487626, 284.2010737791439, 283.45395018016137, 257.4490843225562, 237.95918893376805, 200.0344898717282, 189.46189052892507, 168.2251504584963, 156.89733215794215, 421.8980358783624, 342.29547164532, 338.748989449998, 320.4303078070194, 310.97315686726006, 307.37152839765935, 307.08557725800483, 300.30218077842216, 281.00351442647184, 219.29928568483226, 212.95863789833962, 212.17467036832707, 208.3278888834557, 193.27775221343418, 189.04818475499127, 187.04332930535958, 180.14284058982216, 174.57689171183026, 172.89518331088954, 166.0546471962882, 165.660022481266, 163.27224951649112, 73.44267717688443, 214.5470850841136, 345.79788805496213, 469.63481549899257, 402.24320762891267, 557.6770642964028, 707.8635054202939, 427.16607678361294, 925.673558617448, 795.9584585048227, 877.062900197503, 609.6547830047705, 772.5468106609633, 1601.7183448307617, 1273.7565926386, 1046.7352632152574, 1408.0242252172923, 653.8068333490412, 644.0992476927667, 600.724014691284, 477.4085462174823, 388.46981302597976, 379.88657123519863, 356.67833870309505, 324.7420715598337, 323.54426275365853, 271.29853365989976, 251.21240845955973, 249.55002479804918, 228.12733447459462, 213.02336453180382, 184.7707938182381, 178.79401707250832, 172.80620809635883, 171.32602969951623, 166.3658833947289, 158.7340778077684, 149.00041723204905, 147.0932429483951, 146.47742245866394, 144.8071150190783, 612.2903172735543, 448.12876525054696, 302.12789434482306, 569.2191364265541, 345.79788805496213, 1518.291266248504, 1017.4869671516575, 924.5451835726243, 694.5439689459995, 554.3933496785954, 531.2225637158463, 496.61306112517656, 293.9324391028446, 286.96145106833325, 285.91967940066485, 265.1036669649914, 253.32531365833444, 204.35538048792353, 192.50546033495922, 185.48998514580768, 165.75803967034076, 165.72156810443414, 164.85664912427524, 163.2922967526589, 137.6979484743072, 133.7237329252258, 128.8645657958439, 126.5054467747172, 124.7704859739052, 124.62500751266323, 989.6608366878262, 2272.51679081233, 1958.8218600486653, 1452.3000535185529, 1408.5744050392534, 361.343476472642, 292.4584338689568, 278.7279121093754, 174.99892790279, 128.26127013115067, 117.58718268506877, 117.42705262487584, 109.5283267164032, 105.1165959508367, 95.33822858907733, 95.19493051707299, 85.84680258183404, 75.254375276469, 65.14582001628328, 64.14570946490292, 64.07625073005117, 50.128957603488765, 47.2855628123471, 46.42325970033177, 44.6193116026986, 39.425952825506485, 557.21108508195, 2679.9024398048005, 2151.473166633873, 1131.9117407441515, 640.017032737794, 508.56105204606337, 480.77325452315176, 448.0985284384963, 411.25119572506657, 265.4231972909165, 263.1783560911178, 235.40357689109624, 197.3745504043925, 161.8501345119195, 96.23433939754463, 85.18091693660914, 75.58109793190214, 65.05735262999502, 54.08867804621908, 39.0085372934652, 38.51839509522805, 33.07952242289376, 30.60640399464714, 28.39137206162799, 25.064028631061632, 13.498884082219616, 2724.8206490319435, 2098.944054841555, 1755.6626552049795, 1134.343421557725, 1118.6583331134416, 613.9141547915775, 136.6042238773925, 136.31707058597556, 96.02644603871713, 90.2507462782856, 66.07697556242296, 54.14176924634901, 36.869543061869734, 36.27180079432829, 35.3179845337299, 16.37924473879516, 12.138495320148218, 4.894898154751692, 3.110298304363459, 1.3611312885287885, 1.3611313093636688, 1.3611314832632182, 1.36113119426696, 1.3611313602212787, 1.3611312049935496, 1.3611323751788205, 1.3611315468584748, 1.3611328328068282, 1.36113157122146, 1.3611315739265524, 1.3611319915134357, 1.3611319983347108, 1.3611317791098543, 1.3611316386727075, 4897.471310313072, 1759.5628559735067, 907.3385938025804, 257.66714032831226, 238.57113859108156, 208.61967699234177, 176.7725474126981, 133.96851680789493, 131.50347010275019, 113.12098119647122, 108.71104354021885, 84.53333173380594, 81.36027812278827, 75.90397244087514, 73.98947970212711, 41.76788114602937, 38.85110081780085, 23.833581423564684, 13.661715196129455, 11.523752787970688, 8.95428827842989, 1.9052284606987757, 1.3611323751788205, 1.3611311986505852, 1.361131117995174, 1.3611315229397591, 1.3611328328068282, 1.3611319915134357, 2642.6815323400238, 1433.7046551348155, 939.8643048103912, 564.970403453314, 498.4819773276703, 496.2891419452128, 384.18526307391164, 244.54409952318468, 215.65944261377902, 212.62739657960194, 204.56829825276893, 178.11113307926308, 154.03253382322686, 151.10329974317813, 113.00302986259317, 38.41201666213681, 35.673121731200524, 31.600998248040284, 22.882166001137605, 17.63477037135749, 13.259603807715822, 44.570642411911635, 4.958486482146962, 4.087567835959025, 2.3823715820412947, 1297.822645384586, 1135.416734163905, 1102.389651372023, 981.9890832296722, 805.9994364005144, 685.2044904893238, 424.16361140041835, 418.3168301824677, 271.4405920451717, 178.5223578931073, 132.25479306408502, 126.99919441912854, 122.12404284797643, 117.60322089257437, 102.5415236613382, 86.45053284073913, 79.37310654937144, 71.6662505569487, 70.80059592178642, 68.39740051024692, 62.48437063677049, 62.43353190432964, 41.47263168917522, 39.75551522271585, 26.47340718923299, 2580.7200207894716, 1560.1549310720513, 1187.2204017095034, 851.2795400251732, 450.79750650948193, 357.9608746892929, 252.53416228757547, 236.83289184701897, 228.54673712408572, 217.7233883536359, 168.7682913575065, 149.6117205792448, 108.46179174425673, 102.95897676001704, 78.0822390456513, 66.25477443240837, 63.689493842116555, 41.54351296477561, 23.367330811173133, 16.14317493521867, 12.513233186895635, 8.286436918543446, 5.525700691076859, 5.413470020140366, 4.688067688628406, 203.6874487592902, 1258.623061041512, 1000.4391751215788, 899.3915415404135, 770.6164959650774, 620.691363201304, 420.78307922426393, 343.93713727612777, 314.6182607277051, 310.0459158683487, 292.9940374561537, 224.1893716846617, 207.3958874553147, 199.32008612132753, 195.39924227430902, 187.79949284700325, 135.29280874732564, 131.5096521151198, 124.54549228125968, 106.02638543152517, 85.69581698229659, 72.90439743594874, 69.31747659510695, 67.97923350007075, 64.56293187283897, 53.33644272433294, 1532.9716408070242, 767.8641267460144, 562.5060944806629, 509.52986918607155, 507.89554166638817, 502.55331901502956, 347.7895748841032, 327.8034512571626, 314.573577756736, 282.87053815362333, 235.63629683997925, 220.68833944876425, 194.69038905769887, 175.80481365567476, 150.80072945896742, 143.93430988166446, 110.1036039388072, 93.36911596336775, 90.87335678972008, 85.70556343545746, 84.40605740068216, 83.35953990368093, 76.98974256715763, 74.40725931521722, 65.27477430061295, 3523.706068599735, 1173.9119990060315, 922.3130831219979, 513.7096614752817, 423.10198287899556, 355.0524498113181, 203.44251083424862, 32.92481606393235, 31.545943368697976, 26.73191881283251, 12.547903896156797, 10.372734951954238, 7.122259364869915, 6.141212063251305, 5.045184299425457, 3.0168827798326414, 1.3611312768529884, 1.3611310884115144, 1.3611310884115144, 1.3611311009938203, 1.3611311103848291, 1.3611311170108635, 1.3611311170108635, 1.3611311170108635, 1.3611311219580622, 1.3611312399391315, 1.3611313119702284, 1.3611319983347108, 1.3611315739265524, 1.3611328328068282, 1.3611318465701383, 1.3611316071355426, 1.3611316132180455, 1.3611316147811339, 1.3611323751788205, 1.3611319915134357, 1.3611317791098543, 1.3611316386727075, 1.361131767893533, 1564.0962747300473, 1485.337239647526, 798.4189439444393, 476.810354224053, 414.7533063378612, 408.85815512182216, 231.9014082450091, 115.18039993306978, 112.77578619127192, 83.64642521551035, 77.19561298536117, 53.682079835766785, 49.81011273062684, 32.967096327840636, 20.4287477347827, 18.024697199422175, 13.844815298164153, 12.608790299407875, 11.857818165258688, 5.7867570372339845, 3.4475936216027048, 154.85839907986133, 3.0539208809045717, 1.3611311346793995, 1.3611312466025334, 1.3611318465701383, 1.3611328328068282, 1302.262624081297, 704.6349429848582, 474.60923565923486, 421.3917847639812, 420.60888356957935, 217.8017943252945, 176.84867910187037, 164.51392374588832, 132.9011114977792, 100.10085944417916, 98.68621553676296, 97.86807512202694, 90.73467422366664, 77.83167725909065, 67.29608313886465, 63.3348730813812, 45.423812234661625, 41.346686588407046, 39.25356650424448, 36.8653454919001, 32.49585476772932, 31.62435976439022, 20.256602619711018, 13.376365186400399, 12.222178814624609, 1283.4246736450618, 682.0933794223706, 623.4587231538915, 597.8680487627323, 413.9474354046792, 379.6639978683165, 284.20197559481454, 228.9504119039235, 184.54691767342706, 182.37437216808152, 172.05312366449962, 130.977975016853, 112.52740232098873, 103.4819421904513, 88.91891555334597, 83.14039742843319, 80.16089752247844, 72.09644104728679, 65.93216717483273, 60.38478189569638, 60.05460861191989, 55.59471872305593, 55.3045562547269, 49.41315811342951, 46.72970292114416, 42.13760175695942, 794.1861422521108, 716.9200256977005, 457.51663788657913, 280.4088029408579, 256.56195988317523, 244.88465916550373, 174.28321539214906, 173.44974091875275, 159.44064641594514, 127.17659095063354, 92.75934337305645, 71.27415308580267, 65.5266031004623, 55.22870625437854, 53.278525408509104, 46.28502610861511, 46.15299340802351, 45.67243132074732, 348.60736459849716, 33.02411373537318, 30.37804841721429, 23.050441417409772, 20.046025429616837, 20.046025429616837, 19.936610719957333, 448.12876525054696, 863.2826553006537, 718.6174943204232, 579.8066819964731, 301.60740063928085, 293.169656062378, 259.6881726192719, 225.38628975733027, 191.87861617932438, 174.97797350540026, 159.7115852577559, 154.04893338678096, 124.03395221948976, 115.31905681590044, 112.82913308954672, 99.99819633489152, 70.02314357068789, 57.401933969253896, 57.207119081683786, 46.56592177114556, 40.65463222464118, 38.922867317564965, 37.20469392899362, 19.628615544284518, 6.340257686233154, 5.485686667030793, 3.6835347966820864, 1283.4246736450618, 756.2164207688812, 655.7416922878199, 629.0908457586937, 531.2187057362671, 221.47443452731508, 199.07288136506799, 182.22879810789252, 157.57610427898632, 124.20367877707133, 79.339509798234, 71.0447414892776, 58.78124640994387, 25.593833725684448, 24.757545384784734, 20.681600858737376, 17.53609482365207, 11.43407046125098, 10.015903211599491, 1.361131089378598, 1.361131089378598, 1.361131089378598, 1.361131089378598, 1.361131089378598, 1.361131089378598, 1.361131089378598, 1.3611314184243692, 1.3611312713573736, 1.3611319915134357, 1.3611316071355426, 1.361131767893533, 1.3611317791098543, 1.3611318465701383, 1.3611328328068282, 1.3611319983347108, 2143.209844243386, 502.43711504721557, 294.9624645441849, 198.01502467682812, 186.2213731920245, 118.08912465135256, 103.36048840870917, 97.6436507168901, 91.36538791947603, 85.9123779791073, 75.25580209602062, 60.33027661903984, 53.11518584778715, 11.590034027874585, 11.402079349036574, 5.486423394395022, 3.454333012974345, 1.3611313269170051, 1.36113108935176, 1.361131098742769, 1.3611311125285832, 1.3611311179511092, 1.3611311179511092, 1.3611311228745864, 1.3611311228745864, 1.3611312168829102, 1.3611313010892137, 1.3611319983347108, 1.3611315539091489, 1.361131349020294, 1.3611323751788205, 1.3611313708717871, 1.3611328328068282, 1.3611317748362468, 1.3611319915134357, 1.3611315130832993], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\"], \"logprob\": [25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.6164, -4.6691, -4.876, -4.8953, -4.8958, -5.0306, -5.0319, -5.0349, -5.0845, -5.1193, -5.1471, -5.1801, -5.1922, -5.2011, -5.2186, -5.2488, -5.2508, -5.2888, -5.2941, -5.3185, -5.3995, -5.4358, -5.4496, -5.4618, -5.4622, -4.8002, -5.1175, -4.8899, -5.2595, -4.2672, -4.3774, -5.1579, -4.6901, -5.2236, -4.947, -4.3278, -4.492, -4.5954, -4.7112, -4.8269, -4.8529, -4.9475, -4.9499, -4.9996, -4.9658, -5.0176, -5.0303, -5.0369, -5.0392, -5.0641, -5.088, -5.0918, -5.0399, -5.1225, -5.1274, -5.1979, -5.2168, -5.2531, -5.2623, -5.2798, -3.628, -4.4697, -4.4148, -4.8371, -4.9873, -4.3244, -4.8089, -4.8343, -4.6591, -4.4986, -4.5227, -4.5879, -4.8971, -4.8429, -4.9299, -4.8933, -4.9503, -1.2708, -2.4633, -2.4665, -3.0942, -3.5913, -3.8306, -4.18, -4.3727, -5.6579, -5.9529, -2.0826, -3.3848, -3.5344, -7.9031, -8.351, -5.0374, -3.3671, -4.1091, -9.3467, -9.4827, -4.6767, -9.7501, -4.859, -10.6382, -3.8894, -4.7992, -3.0328, -4.3978, -5.2957, -3.9105, -5.4552, -5.2893, -5.4913, -5.3706, -3.7897, -4.2289, -4.2399, -4.3315, -4.4845, -4.5273, -4.6014, -4.6059, -4.6207, -4.6655, -4.674, -4.7261, -4.7363, -4.7732, -4.7874, -4.8208, -4.828, -4.9589, -4.9727, -4.9921, -4.9934, -5.0104, -5.0222, -5.0423, -5.0499, -4.7774, -4.4177, -3.2098, -3.3355, -4.8368, -4.4312, -4.7143, -4.5303, -1.7708, -2.5682, -2.8568, -2.9273, -3.1155, -3.2234, -3.2254, -3.2335, -3.389, -3.5058, -3.5573, -3.8269, -3.9989, -4.0421, -4.0451, -4.2966, -4.3104, -4.3426, -4.3453, -4.442, -4.5211, -4.6958, -4.7504, -4.8702, -4.9405, -3.9448, -4.1546, -4.165, -4.2209, -4.2509, -4.2626, -4.2636, -4.286, -4.3527, -4.602, -4.6315, -4.6352, -4.6536, -4.7291, -4.7514, -4.7621, -4.8, -4.8316, -4.8413, -4.882, -4.8844, -4.8991, -5.7079, -4.6912, -4.7583, -4.6223, -4.7403, -4.6936, -4.6727, -4.7581, -4.699, -4.7128, -4.7199, -4.7404, -4.7335, -4.7295, -4.7325, -4.7384, -2.3601, -3.1283, -3.1433, -3.2132, -3.4435, -3.6503, -3.6727, -3.736, -3.8301, -3.8338, -4.0107, -4.088, -4.0947, -4.1849, -4.2539, -4.3971, -4.4302, -4.4645, -4.4732, -4.5028, -4.5501, -4.6139, -4.6269, -4.6312, -4.6427, -3.2467, -3.7066, -4.0399, -3.6548, -4.5458, -2.1994, -2.6001, -2.696, -2.9825, -3.2084, -3.2512, -3.3187, -3.845, -3.8691, -3.8728, -3.9487, -3.9944, -4.2105, -4.2706, -4.308, -4.4213, -4.4215, -4.4268, -4.4364, -4.6084, -4.6379, -4.6753, -4.694, -4.7079, -4.7091, -4.0396, -1.7277, -1.8763, -2.1758, -2.2064, -3.5696, -3.7819, -3.8302, -4.2985, -4.6119, -4.6997, -4.7011, -4.7716, -4.8132, -4.9121, -4.9136, -5.0185, -5.1524, -5.2994, -5.3152, -5.3163, -5.5675, -5.6275, -5.6465, -5.6873, -5.815, -3.1882, -1.5461, -1.7658, -2.4086, -2.9797, -3.2101, -3.2664, -3.337, -3.4231, -3.8627, -3.8713, -3.9834, -4.1607, -4.3606, -4.886, -5.0098, -5.1314, -5.2842, -5.473, -5.8094, -5.8225, -5.9806, -6.0616, -6.1402, -6.2713, -6.9385, -1.512, -1.7731, -1.9518, -2.389, -2.403, -3.004, -4.5143, -4.5164, -4.8708, -4.9338, -5.2509, -5.4546, -5.8506, -5.8675, -5.8952, -6.7093, -7.0398, -8.146, -8.8358, -12.4689, -12.4689, -12.4689, -12.4689, -12.4689, -12.4689, -12.4689, -12.4689, -12.4689, -12.4689, -12.4689, -12.4689, -12.4689, -12.4689, -12.4689, -0.8817, -1.9058, -2.5688, -3.8313, -3.9087, -4.0437, -4.2104, -4.4901, -4.5088, -4.6611, -4.7013, -4.9563, -4.9952, -5.0658, -5.0918, -5.6776, -5.7524, -6.2632, -6.8639, -7.054, -7.3437, -9.8936, -12.3155, -12.3155, -12.3155, -12.3155, -12.3155, -12.3155, -1.4437, -2.0556, -2.4784, -2.9883, -3.1138, -3.1182, -3.375, -3.8287, -3.9551, -3.9693, -4.0082, -4.1477, -4.2941, -4.3134, -4.6069, -5.7089, -5.7856, -5.9117, -6.2511, -6.5298, -6.8416, -5.792, -8.0272, -8.2995, -9.2487, -2.1514, -2.2852, -2.3148, -2.4306, -2.6284, -2.791, -3.2718, -3.2857, -3.7199, -4.1415, -4.444, -4.485, -4.5246, -4.5627, -4.7014, -4.8745, -4.9613, -5.0652, -5.0776, -5.1128, -5.2051, -5.2059, -5.6258, -5.6695, -6.0933, -1.4581, -1.9617, -2.2351, -2.5682, -3.2053, -3.4366, -3.787, -3.8516, -3.8874, -3.9362, -4.1927, -4.3141, -4.6391, -4.6919, -4.9726, -5.1399, -5.1802, -5.6187, -6.2198, -6.6166, -6.8973, -7.371, -7.8748, -7.9018, -8.0964, -6.8091, -2.153, -2.3829, -2.4895, -2.6443, -2.8611, -3.2508, -3.4531, -3.5426, -3.5573, -3.6141, -3.8831, -3.9615, -4.0014, -4.0214, -4.0614, -4.392, -4.4207, -4.4757, -4.6385, -4.8543, -5.0187, -5.0701, -5.09, -5.1426, -5.338, -1.9184, -2.6106, -2.9224, -3.0216, -3.0248, -3.0354, -3.4047, -3.4641, -3.5054, -3.6121, -3.7958, -3.8617, -3.9878, -4.0906, -4.2452, -4.2923, -4.563, -4.73, -4.7575, -4.817, -4.8325, -4.8451, -4.926, -4.9607, -5.0941, -1.0088, -2.1087, -2.3502, -2.9366, -3.1312, -3.3071, -3.8668, -5.722, -5.7666, -5.94, -6.7563, -6.9714, -7.4155, -7.6004, -7.8576, -8.6404, -12.1271, -12.1271, -12.1271, -12.1271, -12.1271, -12.1271, -12.1271, -12.1271, -12.1271, -12.1271, -12.1271, -12.1271, -12.1271, -12.1271, -12.1271, -12.1271, -12.1271, -12.1271, -12.1271, -12.1271, -12.1271, -12.1271, -12.1271, -1.6679, -1.7196, -2.3411, -2.8577, -2.9976, -3.0119, -3.5815, -4.2871, -4.3084, -4.6113, -4.6929, -5.0638, -5.1406, -5.5672, -6.0716, -6.206, -6.4938, -6.5977, -6.6665, -7.5239, -8.2639, -4.4781, -8.4678, -12.0616, -12.0616, -12.0616, -12.0616, -1.802, -2.4171, -2.8131, -2.9324, -2.9343, -3.5953, -3.805, -3.8779, -4.0932, -4.3799, -4.3943, -4.4028, -4.4795, -4.6353, -4.7835, -4.8454, -5.1862, -5.2831, -5.3368, -5.4019, -5.533, -5.5613, -6.0314, -6.4827, -6.5832, -2.3118, -2.3715, -2.4615, -2.5035, -2.8722, -2.9589, -3.2497, -3.467, -3.684, -3.6959, -3.7546, -4.0298, -4.1833, -4.2681, -4.4219, -4.4901, -4.5272, -4.6351, -4.7262, -4.816, -4.8216, -4.9006, -4.9059, -5.0215, -5.0789, -5.1855, -2.1513, -2.2538, -2.704, -3.1954, -3.2847, -3.3316, -3.6738, -3.6787, -3.7636, -3.9918, -4.3112, -4.579, -4.6647, -4.8395, -4.8764, -5.0209, -5.0238, -5.0346, -3.007, -5.3703, -5.4574, -5.7479, -5.8966, -5.8966, -5.9025, -4.4334, -4.6942, -2.2351, -2.4502, -3.1059, -3.1344, -3.2562, -3.3986, -3.5606, -3.6535, -3.7455, -3.7819, -4.0007, -4.0744, -4.0964, -4.2187, -4.5807, -4.7837, -4.7871, -4.9983, -5.1383, -5.1833, -5.2301, -5.9028, -7.1953, -7.3815, -7.9466, -2.5944, -2.0549, -2.1978, -2.2393, -2.4088, -3.2872, -3.3945, -3.4835, -3.63, -3.8702, -4.3245, -4.4369, -4.6303, -5.4919, -5.527, -5.718, -5.8952, -6.3671, -6.5181, -11.7596, -11.7596, -11.7596, -11.7596, -11.7596, -11.7596, -11.7596, -11.7596, -11.7596, -11.7596, -11.7596, -11.7596, -11.7596, -11.7596, -11.7596, -11.7596, -1.0071, -2.4597, -2.9941, -3.3949, -3.4567, -3.9163, -4.0511, -4.1088, -4.1762, -4.2387, -4.3733, -4.5988, -4.7292, -6.3474, -6.3659, -7.2498, -7.919, -11.9037, -11.9037, -11.9037, -11.9037, -11.9037, -11.9037, -11.9037, -11.9037, -11.9037, -11.9037, -11.9037, -11.9037, -11.9037, -11.9037, -11.9037, -11.9037, -11.9037, -11.9037, -11.9037], \"loglift\": [25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.4803, 0.4803, 0.4803, 0.4803, 0.4803, 0.4803, 0.4803, 0.4803, 0.4803, 0.4802, 0.4802, 0.4802, 0.4802, 0.4802, 0.4802, 0.4802, 0.4802, 0.4802, 0.4802, 0.4802, 0.4802, 0.4802, 0.4802, 0.4802, 0.4802, 0.476, 0.4761, 0.4703, 0.4759, 0.4205, 0.4193, 0.4654, 0.4062, 0.4625, -0.3064, 2.1376, 2.1375, 2.1375, 2.1374, 2.1373, 2.1373, 2.1373, 2.1373, 2.1372, 2.1372, 2.1372, 2.1372, 2.1372, 2.1372, 2.1372, 2.1371, 2.1371, 2.1371, 2.1371, 2.1371, 2.137, 2.137, 2.137, 2.137, 2.137, 2.1127, 2.1276, 2.1252, 2.1042, 2.0994, 1.9632, 2.0595, 2.0622, 2.0113, 1.7683, 1.6907, 1.6802, 1.9623, 1.6247, 1.7399, 1.1239, 1.3058, 2.2154, 2.2154, 2.2154, 2.2153, 2.2152, 2.2152, 2.2151, 2.215, 2.2138, 2.2133, 2.2128, 2.2045, 2.202, 2.2004, 2.1921, 2.1868, 2.1726, 2.1658, 2.1534, 2.1447, 2.1407, 2.1239, 2.0982, 2.0061, 1.9171, 1.7258, 1.6078, 1.6859, 1.7206, 1.3418, 1.6172, 0.8165, 1.1592, -0.6829, 3.1084, 3.1081, 3.1081, 3.108, 3.1078, 3.1078, 3.1077, 3.1077, 3.1077, 3.1076, 3.1076, 3.1075, 3.1075, 3.1074, 3.1074, 3.1074, 3.1074, 3.1071, 3.1071, 3.1071, 3.1071, 3.107, 3.107, 3.107, 3.1069, 3.1032, 3.0792, 2.9038, 2.7703, 3.0623, 2.7305, 2.9071, 2.5524, 4.5867, 4.5863, 4.586, 4.586, 4.5857, 4.5856, 4.5856, 4.5856, 4.5853, 4.5851, 4.585, 4.5843, 4.5838, 4.5837, 4.5837, 4.5827, 4.5826, 4.5825, 4.5825, 4.582, 4.5816, 4.5806, 4.5802, 4.5793, 4.5788, 4.5853, 4.5846, 4.5845, 4.5843, 4.5842, 4.5841, 4.5841, 4.584, 4.5837, 4.5824, 4.5822, 4.5822, 4.5821, 4.5816, 4.5814, 4.5814, 4.5811, 4.5809, 4.5808, 4.5805, 4.5804, 4.5803, 4.5704, 4.5151, 3.9706, 3.8005, 3.8374, 3.5574, 3.3399, 3.7595, 3.0453, 3.1825, 3.0783, 3.4215, 3.1916, 2.4665, 2.6926, 2.883, 4.9648, 4.9637, 4.9637, 4.9635, 4.9629, 4.9623, 4.9622, 4.962, 4.9617, 4.9617, 4.9609, 4.9605, 4.9605, 4.96, 4.9596, 4.9586, 4.9584, 4.9581, 4.9581, 4.9578, 4.9575, 4.9569, 4.9568, 4.9568, 4.9567, 4.9109, 4.7631, 4.824, 4.5758, 4.1831, 5.05, 5.0496, 5.0495, 5.049, 5.0485, 5.0484, 5.0483, 5.0465, 5.0463, 5.0463, 5.046, 5.0457, 5.0445, 5.0441, 5.0438, 5.043, 5.043, 5.043, 5.0429, 5.0414, 5.0411, 5.0407, 5.0405, 5.0404, 5.0404, 3.6379, 5.1185, 5.1184, 5.1182, 5.1181, 5.1154, 5.1146, 5.1144, 5.1116, 5.1088, 5.1079, 5.1079, 5.1071, 5.1066, 5.1053, 5.1052, 5.1037, 5.1016, 5.0988, 5.0985, 5.0985, 5.0927, 5.0911, 5.0905, 5.0894, 5.0854, 5.0637, 5.1352, 5.1351, 5.1346, 5.1337, 5.1331, 5.133, 5.1328, 5.1325, 5.1308, 5.1307, 5.1301, 5.1291, 5.1276, 5.122, 5.1202, 5.1182, 5.1154, 5.1112, 5.1016, 5.1011, 5.0953, 5.092, 5.0885, 5.0821, 5.0337, 5.1526, 5.1525, 5.1524, 5.152, 5.152, 5.151, 5.1435, 5.1434, 5.1394, 5.1385, 5.133, 5.1286, 5.1168, 5.1162, 5.1152, 5.0695, 5.0386, 4.8406, 4.6043, 1.7976, 1.7976, 1.7976, 1.7976, 1.7976, 1.7976, 1.7976, 1.7976, 1.7976, 1.7976, 1.7976, 1.7976, 1.7976, 1.7976, 1.7976, 5.1967, 5.1962, 5.1955, 5.1918, 5.1914, 5.1906, 5.1895, 5.1871, 5.1869, 5.1853, 5.1848, 5.1813, 5.1807, 5.1796, 5.1791, 5.1651, 5.1627, 5.1405, 5.0963, 5.0764, 5.039, 4.0366, 1.951, 1.951, 1.951, 1.951, 1.951, 1.951, 5.2516, 5.2512, 5.2507, 5.2498, 5.2495, 5.2495, 5.2487, 5.2467, 5.246, 5.2459, 5.2457, 5.2447, 5.2436, 5.2434, 5.2405, 5.2175, 5.2148, 5.2099, 5.1934, 5.1752, 5.1484, 4.9857, 4.9465, 4.8673, 4.458, 5.255, 5.2548, 5.2548, 5.2546, 5.2544, 5.2541, 5.2529, 5.2528, 5.2511, 5.2486, 5.246, 5.2456, 5.2452, 5.2448, 5.2431, 5.2407, 5.2394, 5.2376, 5.2373, 5.2367, 5.2348, 5.2348, 5.224, 5.2225, 5.2053, 5.2609, 5.2606, 5.2603, 5.2599, 5.2585, 5.2578, 5.2562, 5.2559, 5.2557, 5.2554, 5.2536, 5.2526, 5.2493, 5.2486, 5.2445, 5.2414, 5.2406, 5.2294, 5.2037, 5.1767, 5.1508, 5.0892, 4.9906, 4.9841, 4.9334, 2.4491, 5.284, 5.2837, 5.2836, 5.2833, 5.2829, 5.2819, 5.2812, 5.2809, 5.2808, 5.2806, 5.2792, 5.2787, 5.2785, 5.2783, 5.278, 5.2753, 5.275, 5.2745, 5.2726, 5.2697, 5.267, 5.266, 5.2656, 5.2646, 5.2602, 5.3215, 5.3206, 5.32, 5.3197, 5.3197, 5.3197, 5.3185, 5.3183, 5.3181, 5.3177, 5.3167, 5.3164, 5.3156, 5.3148, 5.3136, 5.3132, 5.3104, 5.3082, 5.3078, 5.3069, 5.3067, 5.3065, 5.3052, 5.3046, 5.3021, 5.3988, 5.398, 5.3977, 5.3966, 5.396, 5.3954, 5.3927, 5.3586, 5.3568, 5.3489, 5.289, 5.2642, 5.1961, 5.1594, 5.0988, 4.8302, 2.1394, 2.1394, 2.1394, 2.1394, 2.1394, 2.1394, 2.1394, 2.1394, 2.1394, 2.1394, 2.1394, 2.1394, 2.1394, 2.1394, 2.1394, 2.1394, 2.1394, 2.1394, 2.1394, 2.1394, 2.1394, 2.1394, 2.1394, 5.5519, 5.5518, 5.5511, 5.55, 5.5495, 5.5495, 5.547, 5.5413, 5.541, 5.5369, 5.5356, 5.528, 5.526, 5.5121, 5.4863, 5.4771, 5.4531, 5.4427, 5.4353, 5.2953, 5.0732, 5.0542, 4.9906, 2.2049, 2.2049, 2.2049, 2.2049, 5.6009, 5.6001, 5.5992, 5.5988, 5.5988, 5.5959, 5.5945, 5.5939, 5.592, 5.5887, 5.5886, 5.5884, 5.5874, 5.5849, 5.5822, 5.581, 5.5726, 5.5697, 5.5679, 5.5657, 5.5607, 5.5595, 5.5349, 5.4986, 5.4883, 5.1057, 5.6782, 5.678, 5.6779, 5.6769, 5.6766, 5.6755, 5.6743, 5.6729, 5.6729, 5.6724, 5.67, 5.6683, 5.6673, 5.6652, 5.6642, 5.6636, 5.6617, 5.66, 5.6581, 5.658, 5.6562, 5.656, 5.6531, 5.6516, 5.6484, 5.7462, 5.746, 5.745, 5.7431, 5.7427, 5.7425, 5.7403, 5.7402, 5.7396, 5.7375, 5.7336, 5.7293, 5.7276, 5.7238, 5.7229, 5.7191, 5.719, 5.7187, 5.7138, 5.7073, 5.7037, 5.6893, 5.6802, 5.6802, 5.6798, 4.0363, 3.1198, 5.7624, 5.7619, 5.7598, 5.7597, 5.7591, 5.7583, 5.7573, 5.7567, 5.7559, 5.7556, 5.7536, 5.7527, 5.7525, 5.751, 5.7453, 5.7411, 5.741, 5.7356, 5.7314, 5.7299, 5.7283, 5.6951, 5.5326, 5.4911, 5.3244, 4.8231, 5.8916, 5.8913, 5.8912, 5.8908, 5.8873, 5.8867, 5.886, 5.8849, 5.8826, 5.8766, 5.8746, 5.8707, 5.8405, 5.8387, 5.8276, 5.8153, 5.7711, 5.7525, 2.5069, 2.5069, 2.5069, 2.5069, 2.5069, 2.5069, 2.5069, 2.5069, 2.5069, 2.5069, 2.5069, 2.5069, 2.5069, 2.5069, 2.5069, 2.5069, 5.8977, 5.8957, 5.8938, 5.8916, 5.8912, 5.8871, 5.8854, 5.8847, 5.8837, 5.8828, 5.8806, 5.8762, 5.8731, 5.7773, 5.7751, 5.6227, 5.4162, 2.3628, 2.3628, 2.3628, 2.3628, 2.3628, 2.3628, 2.3628, 2.3628, 2.3628, 2.3628, 2.3628, 2.3628, 2.3628, 2.3628, 2.3628, 2.3628, 2.3628, 2.3628, 2.3628]}, \"token.table\": {\"Topic\": [3, 6, 19, 10, 15, 8, 15, 15, 3, 13, 23, 23, 9, 20, 15, 19, 13, 16, 5, 19, 8, 14, 19, 23, 2, 9, 14, 4, 2, 7, 11, 6, 8, 20, 7, 8, 1, 6, 3, 20, 22, 20, 16, 15, 8, 5, 7, 8, 17, 2, 6, 24, 24, 12, 9, 2, 6, 1, 4, 6, 7, 1, 6, 19, 17, 16, 14, 1, 6, 1, 2, 4, 19, 17, 25, 1, 2, 24, 20, 1, 4, 6, 10, 3, 1, 5, 19, 11, 7, 17, 16, 6, 19, 2, 21, 7, 1, 6, 15, 12, 14, 4, 19, 3, 7, 12, 1, 2, 16, 1, 3, 8, 11, 2, 11, 8, 8, 21, 2, 1, 2, 3, 1, 2, 4, 17, 15, 13, 12, 23, 23, 5, 24, 6, 22, 2, 4, 4, 8, 22, 14, 21, 14, 14, 7, 2, 5, 12, 16, 6, 23, 13, 8, 1, 4, 13, 2, 17, 4, 6, 19, 6, 17, 13, 3, 1, 3, 20, 5, 11, 1, 2, 1, 2, 2, 6, 12, 18, 14, 15, 6, 6, 7, 11, 11, 5, 15, 14, 6, 18, 1, 8, 23, 25, 9, 13, 2, 24, 6, 9, 13, 21, 24, 25, 7, 24, 23, 17, 2, 1, 18, 12, 18, 20, 21, 12, 1, 3, 1, 25, 25, 10, 7, 13, 21, 14, 15, 14, 10, 2, 16, 24, 9, 22, 9, 21, 17, 9, 1, 16, 20, 9, 1, 2, 21, 22, 10, 9, 16, 1, 2, 15, 7, 13, 19, 7, 15, 12, 3, 3, 12, 5, 24, 3, 16, 4, 20, 22, 14, 1, 2, 1, 3, 6, 8, 5, 1, 20, 12, 3, 3, 18, 5, 23, 6, 1, 3, 6, 3, 9, 7, 16, 12, 18, 15, 16, 16, 25, 9, 16, 1, 3, 10, 19, 11, 10, 4, 3, 4, 18, 25, 16, 12, 1, 3, 18, 20, 6, 22, 4, 6, 24, 17, 18, 8, 20, 10, 14, 4, 21, 22, 6, 9, 16, 22, 1, 2, 21, 6, 7, 2, 11, 24, 16, 22, 11, 24, 7, 4, 6, 5, 10, 24, 25, 20, 19, 7, 4, 6, 9, 1, 2, 6, 1, 3, 2, 1, 4, 21, 5, 25, 14, 4, 13, 4, 1, 6, 1, 3, 3, 6, 5, 25, 19, 20, 1, 2, 4, 1, 24, 4, 1, 1, 2, 7, 4, 6, 1, 2, 21, 15, 6, 7, 21, 15, 12, 17, 21, 8, 14, 13, 25, 23, 20, 14, 6, 1, 6, 12, 20, 1, 4, 20, 23, 6, 22, 17, 23, 1, 6, 21, 1, 17, 9, 18, 12, 10, 10, 12, 17, 4, 19, 22, 4, 13, 13, 10, 20, 5, 19, 1, 3, 24, 22, 14, 19, 9, 1, 2, 3, 15, 13, 25, 2, 6, 23, 25, 12, 11, 13, 4, 8, 3, 6, 1, 15, 14, 1, 2, 19, 20, 1, 3, 17, 1, 1, 3, 12, 23, 11, 5, 11, 21, 6, 2, 19, 13, 5, 21, 17, 1, 6, 14, 3, 25, 22, 10, 4, 7, 21, 10, 13, 4, 13, 14, 4, 23, 2, 22, 8, 23, 5, 14, 1, 3, 13, 1, 3, 6, 10, 18, 15, 17, 18, 14, 17, 11, 15, 1, 2, 6, 17, 5, 2, 9, 2, 4, 6, 16, 23, 15, 19, 23, 1, 6, 4, 24, 14, 14, 1, 18, 19, 11, 9, 20, 12, 17, 10, 1, 7, 17, 22, 22, 25, 10, 16, 19, 1, 2, 6, 16, 16, 2, 6, 8, 8, 4, 7, 4, 2, 13, 17, 22, 25, 5, 21, 8, 21, 21, 17, 4, 4, 6, 19, 6, 13, 20, 20, 5, 6, 9, 1, 2, 22, 1, 1, 3, 4, 6, 22, 9, 8, 18, 5, 15, 1, 2, 10, 10, 23, 2, 3, 6, 12, 11, 8, 23, 1, 2, 6, 7, 6, 16, 9, 6, 15, 5, 1, 18, 7, 10, 4, 6, 16, 23, 9, 1, 3, 6, 21, 8, 18, 11, 10, 20, 11, 1, 8, 12, 22, 10, 5, 23, 20, 3, 8, 23, 2, 2, 15, 17, 16, 1, 2, 24, 22, 6, 1, 2, 7, 22, 20, 21, 13, 21, 9, 25, 21, 4, 21, 8, 6, 23, 1, 9, 15, 17, 1, 2, 14, 23, 7, 4, 1, 5, 20, 16, 9, 13, 7, 5, 11, 1, 1, 3, 6, 1, 3, 22, 24, 10, 13, 23, 1, 9, 22, 1, 22, 10, 10, 1, 3, 22, 6, 7, 6, 7, 7, 7, 15, 15], \"Freq\": [0.98665235599206, 0.013199847749967283, 0.8724072443743135, 0.9969575876299627, 0.7238900953248106, 0.9933675321653515, 0.9414853659486285, 0.9628458728060783, 0.9989079427751194, 0.99230526336493, 0.9676198403542086, 0.9755768861375853, 0.9774789331863107, 0.9379657762309924, 0.9906858363379788, 0.6912334446154599, 0.9072984599780957, 0.9749431597596359, 0.994987032803036, 0.94315037927766, 0.9896116834753262, 0.9972566920661191, 0.9957726525356908, 0.9885616753004993, 0.999230181236697, 0.9995804314494199, 0.9989927757379754, 0.9990990701232574, 0.9989139238513828, 0.9899136506236593, 0.9990529756955014, 0.9948369161105463, 0.9921796486585901, 0.9893144117102588, 0.9962164034972739, 0.9977770033071591, 0.9822283481543407, 0.017517623959500246, 0.999726535278698, 0.9539678282531343, 0.9777523983864619, 0.9890024975780247, 0.9932694545082749, 0.9810613734156101, 0.9968277094365948, 0.9927171980220724, 0.9976662211767161, 0.9967518753503546, 0.9880588809787064, 0.9989021935527036, 0.9956637651613227, 0.9982659964517873, 0.969697028921072, 0.9985247031133434, 0.987847326548895, 0.9991378031047867, 0.9964288189472648, 0.2689728690729796, 0.3729757117811984, 0.35683733963681963, 0.9952270433092665, 0.7120016728376893, 0.28677845155962484, 0.9990997063752045, 0.9923496662624705, 0.9875947478100575, 0.979569391529177, 0.9780776799393904, 0.021715496044258892, 0.032423952409240064, 0.9669112972188305, 0.9987643725697263, 0.9897517291678471, 0.9833417476898904, 0.9779500991278396, 0.6371285036844284, 0.3626465525945737, 0.9186909722209947, 0.980881903875132, 0.8522097561067933, 0.02747049763274645, 0.11987126239743905, 0.994638007131846, 0.9998343722427776, 0.9997566320848128, 0.9972284667473588, 0.9706648011027915, 0.9893107984200179, 0.9971301052577863, 0.9969091458433227, 0.9966073116545846, 0.9936259667845118, 0.9389796627856655, 0.9993157842648339, 0.9976602730867054, 0.9952945314559959, 0.7507635679756064, 0.2472342094540359, 0.973472958565117, 0.9832808078564413, 0.944343877661798, 0.38745073148442966, 0.6070061459922731, 0.9401361982783824, 0.9951737716023096, 0.9885670689786706, 0.024764819559967222, 0.9750055136168272, 0.996626577500886, 0.25779416015015155, 0.741955552055566, 0.9966495462552415, 0.9789114899228011, 0.9993586367344571, 0.9995502239140784, 0.9985385885031824, 0.9893939402647549, 0.9855179076288049, 0.9993758967541111, 0.0442931396979873, 0.8810643704924641, 0.07419100899412874, 0.16708932474212695, 0.018189470794712555, 0.8145075817492796, 0.9973225277104715, 0.9865225189373277, 0.9936858033941715, 0.9927705640775009, 0.9788991457521189, 0.9934992317815455, 0.9948284424731377, 0.9977058305306136, 0.9940752853764443, 0.996685065064333, 0.9995222524190829, 0.9983584191222716, 0.9984287687365868, 0.9974867128557671, 0.9987166968912535, 0.9968520745820977, 0.9896656909541967, 0.9914724524643639, 0.9987522342050488, 0.9970495999105122, 0.9988775018729281, 0.9996244004336973, 0.8934266745991762, 0.9972752658129633, 0.9962153409769255, 0.9853884941675742, 0.9969148658529268, 0.992085990715065, 0.04414161083601963, 0.9547199829390532, 0.9911238675298104, 0.9991721890990424, 0.9804706440692974, 0.7853740589563998, 0.2138982994131598, 0.9803168490312835, 0.9922078031002898, 0.9948544320665749, 0.9614474433454531, 0.9445487098588592, 0.38969307151486854, 0.6098409646598143, 0.9674293952061291, 0.9957738591090713, 0.8171773698942081, 0.012311792951955737, 0.9872113980160298, 0.3670627311224664, 0.6326710019279633, 0.9269174173778083, 0.07265403620984319, 0.9900904219127638, 0.8766404405893726, 0.9827006071821399, 0.879069368859812, 0.9909673514268246, 0.5378864545593657, 0.4569143001095687, 0.9996988245695235, 0.9649369271313609, 0.9964259314294606, 0.9861397539455987, 0.9993661342036899, 0.995536290506325, 0.6629359328674176, 0.7558144894400277, 0.2435177699933779, 0.994410876490291, 0.9850557421079479, 0.9693416681741202, 0.41974980206201373, 0.999534567863636, 0.99734393541191, 0.9944219913683408, 0.9993325502286882, 0.997027019240264, 0.9824391726747418, 0.9124038254184027, 0.9971397116093215, 0.9952136355387471, 0.9852945979198858, 0.9926514272790816, 0.99350877575727, 0.9995207793669021, 0.999771191221725, 0.8141715264841188, 0.981861217316805, 0.9509939090858838, 0.9966971715768955, 0.9925332864802103, 0.9899727223562613, 0.9999296595849239, 0.9997391647682646, 0.9997996928122397, 0.9893801335666768, 0.9948740017153511, 0.9790807758134621, 0.9946355224271805, 0.7628339678342487, 0.20192663854435994, 0.9767498572340598, 0.738897598974102, 0.9987600035987895, 0.996311661461055, 0.9992731698341191, 0.9904180100823314, 0.9903088315183339, 0.983331530268364, 0.9949758961698679, 0.9950131926453171, 0.9770673694228401, 0.9969974887075503, 0.96370827911651, 0.9997915486245497, 0.9928390598754487, 0.9829133630508426, 0.9865020774473381, 0.00994956497958778, 0.9898101712451981, 0.9864263966866332, 0.9478188115998598, 0.9798723487882474, 0.990166399179885, 0.9979023340746774, 0.9997634670857152, 0.9328017075049848, 0.05891379205294641, 0.993285809190111, 0.9937615742483171, 0.9686658966844637, 0.9925676874988837, 0.9984969214400062, 0.9996995775534258, 0.9994286154965867, 0.9999150966429993, 0.9650249197235415, 0.9980006880108061, 0.9933426423213951, 0.9999862992336144, 0.9885205983679564, 0.9984527168903768, 0.9917273669352629, 0.9722366774602311, 0.9921322775022419, 0.16051626636683994, 0.8386974917667387, 0.9418993042653849, 0.05509490858103158, 0.0029478825624537196, 0.9858100578827954, 0.9988531658699835, 0.999883724845232, 0.9976797304744319, 0.9780932637715406, 0.9999375830371587, 0.975845561572264, 0.9719112762198407, 0.9968076402268976, 0.9938492720261607, 0.993648794453542, 0.009942065900807984, 0.9517042583548443, 0.03827695371811074, 0.9998312947720993, 0.9942918055855469, 0.9944262751248655, 0.9855951082462598, 0.9842606281340144, 0.9966719304628769, 0.9892273107146634, 0.9984527967229598, 0.9903195282254714, 0.9907770960741041, 0.9962819960505258, 0.997275033452084, 0.11013555391171213, 0.889442933153068, 0.9509931376825398, 0.9636597343110639, 0.6430251391624353, 0.9871736076199831, 0.9985409733990399, 0.9832345762173685, 0.9993729306170832, 0.9992231112665995, 0.9831668449016134, 0.9875909416474944, 0.9935298683169765, 0.9955679522273256, 0.004302684483189361, 0.8424293040484615, 0.9961748702121428, 0.028685567247029725, 0.9667036162249018, 0.9942113006900772, 0.0037143635144087565, 0.9377258701151531, 0.9949977433961321, 0.9970357905941012, 0.9958368476089877, 0.9990304382096601, 0.9984109286381904, 0.9863675426539649, 0.9990832219768636, 0.973002693330274, 0.9907483685335593, 0.9933890362506742, 0.9832035938777794, 0.9957624740340055, 0.9821232097381951, 0.40110020074929403, 0.5984860996905319, 0.9956171828836569, 0.12908440673633637, 0.8671824247415417, 0.9995318575052089, 0.9985109403579595, 0.8985709835511423, 0.9757921174348576, 0.9530205643720701, 0.9062078708999068, 0.9899978217750843, 0.9987659453711912, 0.8168254477007834, 0.18151676615572965, 0.9959691031976284, 0.9991945125124755, 0.9946106101558831, 0.5789829736994306, 0.9952011001372449, 0.9961129677830298, 0.9992725798328262, 0.9979347334970559, 0.9936559200128061, 0.9638321277403861, 0.12132096869247873, 0.8396820571745001, 0.0387623012005015, 0.45531293745833207, 0.544629916244088, 0.9993767656362638, 0.31432037942008456, 0.6847478515197619, 0.9924640060347139, 0.9980968267579885, 0.8628102364453394, 0.9905123055655382, 0.9985622094262546, 0.8066977724759371, 0.9992276115434596, 0.9812590322500663, 0.01856780777477375, 0.9956594895723514, 0.004149629092685981, 0.9890361706512498, 0.010768085919231231, 0.9942807685408007, 0.979004387728531, 0.5801147755547382, 0.8971046941960187, 0.07525811626025065, 0.3512045425478364, 0.5730766186336125, 0.9999007681890202, 0.9290097076479613, 0.999400516333178, 0.9998614578868398, 0.3603795773817236, 0.6394283551987002, 0.9950583104072512, 0.7538081838178803, 0.24498765974081108, 0.5646148021795856, 0.4351070440986526, 0.9862834739343791, 0.9960126076929751, 0.9948223930028179, 0.9899660117162602, 0.995848830775101, 0.9291852476476192, 0.8677728673977378, 0.9930558370593879, 0.9985480930708227, 0.996783434415593, 0.9849668348363014, 0.9988110137407239, 0.9833128867005057, 0.7886114803908827, 0.9807405858051175, 0.9644914819919761, 0.9944554624719104, 0.9899144546423482, 0.009956894534724885, 0.9991117930409812, 0.9907975950520349, 0.028678850646710347, 0.9708634439518709, 0.6085281170276066, 0.3903618266719986, 0.995498478447269, 0.9916417233541338, 0.980099734870282, 0.9960103099410494, 0.9850206921161652, 0.014767537993324939, 0.9938790784958186, 0.9998253980293187, 0.9899766774262853, 0.9901358867614571, 0.9995158311827034, 0.9922362213589219, 0.9955225949860792, 0.9673658401384102, 0.5248714370103587, 0.9871444878998746, 0.9984425526797271, 0.9842538345220656, 0.9767025447950998, 0.9988339422599557, 0.9493370989272604, 0.9926983742575222, 0.9837473769335716, 0.9486358055469857, 0.9991008475268378, 0.9845119050277132, 0.9285098945007929, 0.07135912121657659, 0.9983914383032777, 0.9633820387401273, 0.9770390708229302, 0.9300619032878817, 0.9893775484191009, 0.3571475330159453, 0.2945472055470917, 0.347739391315424, 0.9932322940001286, 0.9923336202815204, 0.9967369931436115, 0.5429750767712131, 0.45567320168250824, 0.9593002781208707, 0.8770330124781105, 0.9866267514501975, 0.9764156810836907, 0.9990803940462815, 0.9985131198296696, 0.9947683331002526, 0.927781969088131, 0.07140073218328014, 0.9998456645572131, 0.9895223721038884, 0.9762441291855314, 0.07513673358807371, 0.9245867516376736, 0.998222807768777, 0.9966093460928976, 0.002575036134297626, 0.9973629955361566, 0.9988746358686186, 0.9998424952035883, 0.027616883027061844, 0.9717289306963853, 0.9880905779789156, 0.542956727815218, 0.9903381830293596, 0.9976716825989341, 0.9157931418212257, 0.9957706993686484, 0.9968706081352371, 0.9991925640696331, 0.6548958136098142, 0.997402437739903, 0.9968629286007965, 0.9764114144824118, 0.9982367601348696, 0.6872741946432352, 0.31165178277378214, 0.9907958922603335, 0.8703839788296045, 0.7290724234091075, 0.9985064682081367, 0.9930358275594486, 0.9978177032458241, 0.9904162677355813, 0.9847920225830891, 0.9947473969392538, 0.9632402361334845, 0.9977734354730442, 0.9530985332932841, 0.9946927906606837, 0.9980277266034773, 0.966371936566799, 0.9994145566782648, 0.9689889108431632, 0.9976985847376388, 0.9946705530571404, 0.998908170818237, 0.9987394190699327, 0.5824630386392984, 0.417449722290596, 0.7339327738144168, 0.03086220510028234, 0.9580391081323166, 0.010843477467666769, 0.9741457290264981, 0.7928352588537783, 0.9992597330886506, 0.9969224856031117, 0.9985763151948868, 0.9745680682719741, 0.9897339918165744, 0.9626823401411488, 0.9964832087277937, 0.26079363408047557, 0.6717188227164724, 0.06713499491180559, 0.9913175526235253, 0.9922839863740203, 0.9992193526312799, 0.9859633579427481, 0.999516625896125, 0.5245582672328353, 0.47235104632340613, 0.9904443646392167, 0.9954209791751716, 0.9973156991245847, 0.9979010932983189, 0.9916639581260775, 0.9747299077483075, 0.0250819275398244, 0.9983147545798097, 0.8745791827931344, 0.955842221817999, 0.9982421444896492, 0.9997208321918619, 0.9929094915888849, 0.9276580098207321, 0.9882564108791221, 0.9860462880953123, 0.9932196842628043, 0.8783670152485509, 0.9993661717013156, 0.9975484667572456, 0.9998123446350617, 0.9917898828362054, 0.994498376236595, 0.9963874455487804, 0.9478188115998598, 0.9994354989332306, 0.9969304530109357, 0.9985614566508714, 0.9962031985924485, 0.7065714165495628, 0.142884442013356, 0.14995015617885166, 0.9987104470816155, 0.9809935869015758, 0.5595013578783524, 0.435427835001563, 0.9871097456859834, 0.9991495266572307, 0.9976803406668596, 0.9937887211219332, 0.9971203096778986, 0.9995531913984429, 0.9982823817895902, 0.9836906501013342, 0.9909643717061531, 0.9868374421439504, 0.9976733185130419, 0.9713152839031349, 0.9948036725917591, 0.9977112180831536, 0.9983970238454791, 0.9903892975832178, 0.998521776988014, 0.7775953125442001, 0.2211928015155056, 0.9992991002230751, 0.9955378808023984, 0.999363702239772, 0.9686549374740688, 0.9000031963889945, 0.9948705947500917, 0.052044908610773453, 0.9457816150992279, 0.9997592649815467, 0.9987035561472917, 0.9750180145883437, 0.9998430479301609, 0.00839487033051268, 0.24680918771707278, 0.7127244910605265, 0.03190050725594818, 0.9939119584061236, 0.9991048313222856, 0.9869608231518212, 0.9973954674674481, 0.99888753698716, 0.9920845051757292, 0.03693879086165751, 0.9620875983513525, 0.9993152753858521, 0.9605800010235587, 0.9892832742534388, 0.34084403933890073, 0.6127533291485856, 0.04627564204507548, 0.957673669395666, 0.9861414300727336, 0.988099747377691, 0.9170285066407172, 0.3086916222416429, 0.6908577679880858, 0.3214930565209702, 0.6763651735550466, 0.9944636635170075, 0.9946947900530511, 0.9938007209385713, 0.9899793416878612, 0.9939249316857637, 0.9943713751153194, 0.9998672151755332, 0.9352115789009088, 0.9982933566578376, 0.9575475815670355, 0.9982403448274254, 0.9803564190149306, 0.9933770542296275, 0.9900178566066473, 0.9821389540397898, 0.9407103212259312, 0.05531589791038131, 0.003967328743572429, 0.9916177539406836, 0.9932908958292413, 0.8676593050615246, 0.9985175696060601, 0.8889623710308093, 0.9911301502461903, 0.9988156835644358, 0.9998386264622567, 0.9919673013902263, 0.9934143811344484, 0.9810332489528221, 0.994037571945028, 0.9948455131173864, 0.9931909078257145, 0.976526857937891, 0.9240268838929764, 0.9876690357908708, 0.7291703377883699, 0.9991773829836517, 0.9988780144362591, 0.9989720512655054, 0.9853365221546608, 0.980211204677039, 0.9997237687726326, 0.9992925331025251, 0.9831167371510049, 0.976002988094995, 0.9964649031462237, 0.9955461547366142, 0.004347363121120586, 0.8167295393219645, 0.1807516193581397, 0.9680649017176838, 0.971401177998266, 0.9050044159703435, 0.962984936496108, 0.9874475404248687, 0.993441283505277, 0.985861724029776, 0.9981584105002291, 0.9856792193972945, 0.9983287095102769, 0.9978714385894384, 0.9968839924537398, 0.9999127277066119, 0.9988822705895969, 0.9993335112776218, 0.9810870696197056, 0.9998906073494417, 0.9992078661224968, 0.9832212388625605, 0.9762898424199983, 0.9890755795370645, 0.9988584648649727, 0.9997417240188672, 0.9985320653722541, 0.9789235690159832, 0.9948564310159173, 0.9824114576192781, 0.9923462516788485, 0.9951959986452518, 0.9988905510048761, 0.9837011976826098, 0.9997609360324764, 0.38161929240794906, 0.5889541419679429, 0.02935533018522685, 0.9998374411622624, 0.9983107028055952, 0.9926371831661371, 0.9932568390910147, 0.9996632564710579, 0.9932966510541739, 0.9977491581638256, 0.9998180065693799, 0.9728127839474204, 0.9546367035074776, 0.9266953240493239, 0.0718188876138226, 0.9861363674038873, 0.9475141217201444, 0.4496762098186741, 0.5495429508949686, 0.9544285769461932, 0.9275353236422749, 0.06525374136176808, 0.050629577384203615, 0.9472630607367128, 0.9922601971116595, 0.9953346114978157, 0.84475391157994, 0.6399225009649367], \"Term\": [\"10\", \"10\", \"10\\u20ac\", \"11\", \"126\", \"14\", \"144\", \"1440p\", \"15\", \"300\", \"3070\", \"3080\", \"34\", \"36\", \"4k\", \"50fp\", \"53\", \"55\", \"70\", \"8th\", \"95\", \"<\", \">\", \"@\", \"abil\", \"access\", \"account\", \"act\", \"action\", \"adapt\", \"addict\", \"adult\", \"aggress\", \"aint\", \"aliv\", \"alot\", \"also\", \"also\", \"amaz\", \"ambiti\", \"amd\", \"american\", \"ammo\", \"amus\", \"angri\", \"announc\", \"anti\", \"app\", \"applic\", \"area\", \"ark\", \"art\", \"artist\", \"ass\", \"aswel\", \"attack\", \"audienc\", \"audio\", \"audio\", \"audio\", \"automat\", \"averag\", \"averag\", \"awesom\", \"babi\", \"backpack\", \"backup\", \"bad\", \"bad\", \"balanc\", \"balanc\", \"ball\", \"bang\", \"banger\", \"barrier\", \"base\", \"base\", \"bb\", \"beaten\", \"beauti\", \"beauti\", \"beauti\", \"benefit\", \"best\", \"better\", \"biggest\", \"birth\", \"bite\", \"block\", \"board\", \"bodi\", \"boi\", \"boomer\", \"boss\", \"bot\", \"boy\", \"brain\", \"brain\", \"braindead\", \"breaker\", \"breathtak\", \"brother\", \"brother\", \"brrrrrr\", \"brutal\", \"buddi\", \"build\", \"build\", \"bullet\", \"buy\", \"buy\", \"c\", \"ca\", \"campaign\", \"cant\", \"car\", \"cargo\", \"carrier\", \"certain\", \"challeng\", \"challeng\", \"challeng\", \"charact\", \"charact\", \"charact\", \"cheap\", \"cheaper\", \"children\", \"chines\", \"christma\", \"clean\", \"clip\", \"co\", \"coffe\", \"com\", \"combat\", \"combin\", \"complex\", \"complic\", \"con\", \"consol\", \"conveni\", \"corrupt\", \"cost\", \"count\", \"craft\", \"crash\", \"craze\", \"cross\", \"cup\", \"cure\", \"currenc\", \"customis\", \"cut\", \"cut\", \"dad\", \"damag\", \"dang\", \"dark\", \"dark\", \"dc\", \"deaf\", \"dedic\", \"defi\", \"defiantli\", \"definit\", \"definit\", \"demon\", \"depress\", \"derang\", \"design\", \"design\", \"differ\", \"differ\", \"difficulti\", \"difficulti\", \"digit\", \"dinner\", \"divis\", \"dlss\", \"do\", \"don\", \"don\", \"dont\", \"don\\u00b4t\", \"doubt\", \"downgrad\", \"download\", \"dri\", \"drifter\", \"drive\", \"drive\", \"driver\", \"drug\", \"dual\", \"dualsens\", \"due\", \"e\", \"eargasm\", \"earli\", \"earn\", \"east\", \"easter\", \"eat\", \"effici\", \"egg\", \"elden\", \"email\", \"enemi\", \"enjoy\", \"enthral\", \"enthusiast\", \"enviro\", \"epic\", \"establish\", \"euro\", \"even\", \"ever\", \"everi\", \"everybodi\", \"everytim\", \"evolut\", \"exact\", \"exchang\", \"exchang\", \"exercis\", \"exhilar\", \"expans\", \"expens\", \"explor\", \"explos\", \"extens\", \"ez\", \"faction\", \"factorio\", \"fashion\", \"faster\", \"fav\", \"feel\", \"feet\", \"fellow\", \"fi\", \"fight\", \"fight\", \"fighter\", \"filedetail\", \"filler\", \"filter\", \"fire\", \"first\", \"float\", \"float\", \"foot\", \"forgot\", \"forgotten\", \"format\", \"fp\", \"friend\", \"fuck\", \"fun\", \"funki\", \"funni\", \"g\", \"game\", \"gap\", \"gave\", \"gay\", \"geforc\", \"gen\", \"gener\", \"gener\", \"get\", \"get\", \"get\", \"girlfriend\", \"glitch\", \"go\", \"gon\", \"goo\", \"good\", \"goofi\", \"goog\", \"googl\", \"gpu\", \"grandma\", \"graphic\", \"graphic\", \"graphic\", \"great\", \"greatest\", \"greatli\", \"grenad\", \"grim\", \"grindi\", \"gta\", \"gun\", \"gunplay\", \"h\", \"hang\", \"harder\", \"hate\", \"hate\", \"haunt\", \"healthi\", \"heartfelt\", \"hello\", \"hero\", \"hesit\", \"high\", \"highli\", \"hill\", \"horrend\", \"hot\", \"hour\", \"hour\", \"howard\", \"hr\", \"http\", \"http\", \"human\", \"human\", \"humour\", \"hurt\", \"hype\", \"ill\", \"im\", \"imagin\", \"immens\", \"immers\", \"inclus\", \"induc\", \"infin\", \"infrastructur\", \"instantli\", \"intel\", \"interest\", \"interest\", \"internet\", \"it\", \"it\", \"item\", \"ive\", \"jaw\", \"jerk\", \"juic\", \"junki\", \"k\", \"kick\", \"kid\", \"kid\", \"killer\", \"kinda\", \"l\", \"lama\", \"languag\", \"laptop\", \"launch\", \"lead\", \"leaderboard\", \"leap\", \"level\", \"level\", \"level\", \"like\", \"like\", \"limit\", \"line\", \"line\", \"linux\", \"lock\", \"lockdown\", \"logist\", \"lol\", \"loneli\", \"longer\", \"look\", \"look\", \"lot\", \"lot\", \"love\", \"love\", \"lover\", \"luckili\", \"ly\", \"mach\", \"main\", \"main\", \"main\", \"make\", \"mama\", \"man\", \"mani\", \"map\", \"map\", \"mass\", \"master\", \"master\", \"mechan\", \"mechan\", \"media\", \"meh\", \"mehh\", \"men\", \"met\", \"microwav\", \"milki\", \"minimum\", \"mmo\", \"mob\", \"monoton\", \"month\", \"moron\", \"moss\", \"mother\", \"mr\", \"ms\", \"much\", \"much\", \"multiplay\", \"murder\", \"music\", \"music\", \"na\", \"na\", \"nasa\", \"nation\", \"nativ\", \"neat\", \"need\", \"need\", \"network\", \"new\", \"newest\", \"newli\", \"nice\", \"nobodi\", \"nonsens\", \"noon\", \"norman\", \"notif\", \"npc\", \"nut\", \"nvidia\", \"object\", \"oblig\", \"obtain\", \"obtus\", \"og\", \"ok\", \"omg\", \"one\", \"one\", \"op\", \"orang\", \"os\", \"ost\", \"outstand\", \"overal\", \"overal\", \"overal\", \"overpr\", \"own\", \"p\", \"paint\", \"paint\", \"pan\", \"pandem\", \"panel\", \"panic\", \"pass\", \"path\", \"paus\", \"pc\", \"pc\", \"peopl\", \"perspect\", \"photo\", \"place\", \"place\", \"planet\", \"platform\", \"play\", \"play\", \"playabl\", \"player\", \"pleas\", \"pleas\", \"pod\", \"poignant\", \"polici\", \"polish\", \"pong\", \"popular\", \"potato\", \"power\", \"powerhous\", \"practic\", \"pre\", \"predatori\", \"prefer\", \"press\", \"press\", \"pretend\", \"priceless\", \"primal\", \"pro\", \"profit\", \"promis\", \"protect\", \"proton\", \"ps\", \"ps5\", \"pure\", \"purpl\", \"puzzl\", \"pve\", \"q\", \"quest\", \"quot\", \"race\", \"ram\", \"rate\", \"realist\", \"realli\", \"realli\", \"recipi\", \"recommend\", \"recommend\", \"recommend\", \"redempt\", \"refreshingli\", \"refund\", \"reinstal\", \"relax\", \"remark\", \"render\", \"rep\", \"request\", \"requir\", \"requir\", \"requir\", \"resolut\", \"resolv\", \"resourc\", \"revisit\", \"reward\", \"rich\", \"rich\", \"rifl\", \"ring\", \"rip\", \"rock\", \"rtx\", \"run\", \"run\", \"sadli\", \"sam\", \"san\", \"sandbox\", \"say\", \"scam\", \"scanner\", \"scare\", \"sci\", \"scienc\", \"scrape\", \"screen\", \"season\", \"see\", \"sent\", \"session\", \"share\", \"sharedfil\", \"shit\", \"shitti\", \"shoot\", \"shooter\", \"short\", \"short\", \"short\", \"shot\", \"shower\", \"signific\", \"signific\", \"silver\", \"sim\", \"situat\", \"six\", \"size\", \"skill\", \"skin\", \"slaughter\", \"smart\", \"smoke\", \"smooth\", \"smoother\", \"sniper\", \"social\", \"solo\", \"sooner\", \"sorri\", \"soul\", \"soul\", \"space\", \"spare\", \"spend\", \"spice\", \"squid\", \"stabl\", \"star\", \"star\", \"start\", \"stay\", \"steamcommun\", \"still\", \"stori\", \"stori\", \"stori\", \"stori\", \"strateg\", \"strategi\", \"street\", \"stress\", \"stuck\", \"stutter\", \"style\", \"style\", \"suck\", \"summar\", \"surprisingli\", \"surviv\", \"surviv\", \"surviv\", \"suspens\", \"swamp\", \"swap\", \"sway\", \"system\", \"system\", \"t\", \"t\", \"teen\", \"teleport\", \"tend\", \"terrarium\", \"text\", \"textur\", \"thing\", \"thingi\", \"third\", \"thou\", \"thousand\", \"throat\", \"thrown\", \"ti\", \"til\", \"time\", \"time\", \"time\", \"tip\", \"titan\", \"todd\", \"ton\", \"torment\", \"trailer\", \"trash\", \"tri\", \"trial\", \"truck\", \"twitch\", \"ubisoft\", \"ugli\", \"ultra\", \"undeni\", \"underr\", \"underwhelm\", \"unforgett\", \"uniqu\", \"unlock\", \"unplay\", \"unreal\", \"unreward\", \"updat\", \"upgrad\", \"urg\", \"url\", \"usag\", \"use\", \"use\", \"v\", \"v\", \"van\", \"versa\", \"vibrat\", \"vice\", \"videogam\", \"virtual\", \"viru\", \"voic\", \"void\", \"vr\", \"w\", \"wan\", \"want\", \"war\", \"wast\", \"wasteland\", \"way\", \"weapon\", \"weather\", \"weav\", \"weight\", \"weird\", \"well\", \"went\", \"whale\", \"wheel\", \"wich\", \"wife\", \"wild\", \"window\", \"woke\", \"work\", \"worth\", \"worth\", \"worth\", \"would\", \"wow\", \"www\", \"xd\", \"ye\", \"yea\", \"yeah\", \"year\", \"yellow\", \"youtu\", \"youtub\", \"youtub\", \"z\", \"zen\", \"zombi\", \"zombi\", \"|\", \"\\u2018\", \"\\u2018\", \"\\u2019\", \"\\u2019\", \"\\u201c\", \"\\u201d\", \"\\u2705\", \"\\ud83d\\udd32\"]}, \"R\": 25, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [10, 22, 15, 20, 6, 5, 8, 13, 3, 12, 1, 18, 16, 21, 23, 24, 7, 11, 2, 4, 9, 17, 19, 14, 25]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el101225321902014408831930137\", ldavis_el101225321902014408831930137_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el101225321902014408831930137\", ldavis_el101225321902014408831930137_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el101225321902014408831930137\", ldavis_el101225321902014408831930137_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "9     -0.135429  0.537537       1        1  61.853067\n",
       "21    -0.537317  0.103513       2        1  11.789318\n",
       "14    -0.392878  0.384940       3        1  10.910450\n",
       "19     0.459158  0.274367       4        1   4.464375\n",
       "5     -0.217648 -0.408781       5        1   1.018259\n",
       "4      0.166779  0.416457       6        1   1.016925\n",
       "7      0.080476  0.306208       7        1   0.697325\n",
       "12     0.283398  0.185727       8        1   0.640359\n",
       "2     -0.364080 -0.123235       9        1   0.598171\n",
       "11     0.330181 -0.274573      10        1   0.588287\n",
       "0     -0.011291 -0.432318      11        1   0.578130\n",
       "17    -0.288767 -0.255274      12        1   0.553349\n",
       "15     0.382952 -0.115211      13        1   0.523651\n",
       "20    -0.098523  0.263334      14        1   0.521626\n",
       "22     0.184003 -0.375795      15        1   0.518786\n",
       "23    -0.268451  0.111812      16        1   0.506688\n",
       "6      0.087350 -0.281387      17        1   0.488145\n",
       "10     0.346880  0.022553      18        1   0.452051\n",
       "1      0.075760  0.115421      19        1   0.387689\n",
       "3      0.207238 -0.128155      20        1   0.369067\n",
       "8     -0.082430 -0.236539      21        1   0.341322\n",
       "16    -0.084108  0.080356      22        1   0.318968\n",
       "18     0.137794 -0.024759      23        1   0.313797\n",
       "13    -0.051780 -0.091633      24        1   0.275788\n",
       "24    -0.209265 -0.054564      25        1   0.274407, topic_info=          Term          Freq         Total Category  logprob  loglift\n",
       "57        game  65417.000000  65417.000000  Default  25.0000  25.0000\n",
       "144       play  29125.000000  29125.000000  Default  24.0000  24.0000\n",
       "174        fun  19853.000000  19853.000000  Default  23.0000  23.0000\n",
       "60        good  19789.000000  19789.000000  Default  22.0000  22.0000\n",
       "229       like  20623.000000  20623.000000  Default  21.0000  21.0000\n",
       "...        ...           ...           ...      ...      ...      ...\n",
       "2891     forev      0.039669      1.361131  Topic25 -11.9037   2.3628\n",
       "3086       mod      0.039669      1.361133  Topic25 -11.9037   2.3628\n",
       "6808  launcher      0.039669      1.361132  Topic25 -11.9037   2.3628\n",
       "2798   terribl      0.039669      1.361132  Topic25 -11.9037   2.3628\n",
       "2809     delet      0.039669      1.361132  Topic25 -11.9037   2.3628\n",
       "\n",
       "[767 rows x 6 columns], token_table=      Topic      Freq Term\n",
       "term                      \n",
       "1         3  0.986652   10\n",
       "1         6  0.013200   10\n",
       "2695     19  0.872407  10€\n",
       "1253     10  0.996958   11\n",
       "2240     15  0.723890  126\n",
       "...     ...       ...  ...\n",
       "1351      7  0.947263    ’\n",
       "1352      7  0.992260    “\n",
       "1353      7  0.995335    ”\n",
       "1115     15  0.844754    ✅\n",
       "1116     15  0.639923    🔲\n",
       "\n",
       "[751 rows x 3 columns], R=25, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[10, 22, 15, 20, 6, 5, 8, 13, 3, 12, 1, 18, 16, 21, 23, 24, 7, 11, 2, 4, 9, 17, 19, 14, 25])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_lda = lda_vis(full_fd)\n",
    "full_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c4918a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(full_lda, 'full_lda.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
