{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd36527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "494f8ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\AppData\\Local\\Temp\\ipykernel_15352\\3772581767.py:2: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(r'C:\\Users\\Patrick\\Documents\\concordia-bootcamps\\Final Project\\data\\game_reviews_combined.csv')\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "df = pd.read_csv(r'C:\\Users\\Patrick\\Documents\\concordia-bootcamps\\Final Project\\data\\game_reviews_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "469dcee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33283 entries, 0 to 33282\n",
      "Data columns (total 19 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   Unnamed: 0                   33283 non-null  int64  \n",
      " 1   game_title                   33283 non-null  object \n",
      " 2   recommendationid             33283 non-null  float64\n",
      " 3   author                       33283 non-null  object \n",
      " 4   language                     33283 non-null  object \n",
      " 5   review                       33240 non-null  object \n",
      " 6   timestamp_created            33283 non-null  float64\n",
      " 7   timestamp_updated            33283 non-null  float64\n",
      " 8   voted_up                     33283 non-null  bool   \n",
      " 9   votes_up                     33283 non-null  float64\n",
      " 10  votes_funny                  33283 non-null  float64\n",
      " 11  weighted_vote_score          33283 non-null  float64\n",
      " 12  comment_count                33283 non-null  float64\n",
      " 13  steam_purchase               33283 non-null  bool   \n",
      " 14  received_for_free            33283 non-null  bool   \n",
      " 15  written_during_early_access  33283 non-null  bool   \n",
      " 16  timestamp_dev_responded      425 non-null    float64\n",
      " 17  developer_response           425 non-null    object \n",
      " 18  genre                        33283 non-null  object \n",
      "dtypes: bool(4), float64(8), int64(1), object(6)\n",
      "memory usage: 3.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c10a0033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get summary statistics (# positive/negative reviews, # reviews/game, # reviews/genre)\n",
    "# I'd like to have this order reversed, but idk how to do it yet\n",
    "df['voted_up'].value_counts().sort_values().plot(kind='barh', title='Positive/Negative Reviews');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c79aa56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "game_title                  voted_up\n",
       "ANNO_Mutationem             True        182\n",
       "                            False        54\n",
       "ARK_Survival_Evolved        True        269\n",
       "                            False        89\n",
       "ASTRONEER                   True        219\n",
       "                                       ... \n",
       "iRacing                     True         67\n",
       "                            False        38\n",
       "tERRORbane                  True         20\n",
       "theHunter_Call_of_the_Wild  True        212\n",
       "                            False        79\n",
       "Name: voted_up, Length: 242, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('game_title')['voted_up'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "357dc076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby('genre')['voted_up'].value_counts().plot(kind='barh')\n",
    "plt.title('Reviews by Genre')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98e6b468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Reviews by Genre'}, ylabel='genre'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_1 = df.groupby('genre').agg(['count', 'sum'])\n",
    "df_1['voted_up'].plot(kind='barh', title='Reviews by Genre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d7161e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = df['genre'].unique().tolist()\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=3, ncols=2)\n",
    "# for genre in genres:\n",
    "#     gca = plt.gca()\n",
    "# df[df['genre'] == genre].groupby('game_title')['voted_up'].value_counts().plot(kind='bar', subplots=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7418432b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# When I clean this up, put labels with # of reviews\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "df.groupby('genre')['voted_up'].sum().div(df.groupby('genre')['voted_up'].count()).sort_values().plot(kind='barh')\n",
    "plt.title('% Positive Game Reviews')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6258e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby('genre')['voted_up'].count().sort_values().plot(kind='barh', title='# Reviews by Genre');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e47bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean review data\n",
    "# df[(df['genre'] == 'action') & (df['voted_up'] == True)].groupby('game_title')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90fdbcf",
   "metadata": {},
   "source": [
    "## Cleaning Review Data for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7068906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Data\n",
    "# Remove punctuation\n",
    "# Remove stopwords\n",
    "# Tokenize\n",
    "# lemmatize/stem - rn I use Porter, but may be worth trying different stemmers/lemmatizers for different results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd04d2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Immersive story, lovely characters and voice a...\n",
       "1    This game is a slow burn, it builds over time....\n",
       "2    The character interactions in the cutscenes in...\n",
       "3                       Daryl Dixon Simulator.....YES!\n",
       "4    This game is incredibly underrated. I find mys...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_text = df['review']\n",
    "review_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85146082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower\n",
    "review_text = review_text.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b49b25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        immersive story, lovely characters and voice a...\n",
       "1        this game is a slow burn, it builds over time....\n",
       "2        the character interactions in the cutscenes in...\n",
       "3                           daryl dixon simulator.....yes!\n",
       "4        this game is incredibly underrated. i find mys...\n",
       "                               ...                        \n",
       "33278    great potential. played 14 hours on the early ...\n",
       "33279    the game is much closer to wargame 4 than stee...\n",
       "33280                                        a10 goes brrr\n",
       "33281    not worth the price for its stage. no campain ...\n",
       "33282    i am decently surprised, this is a very solid ...\n",
       "Name: review, Length: 33283, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cd311e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7baa7156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to keep dataframe whole. I may just use review text and the voted_up for prediction?\n",
    "df.dropna(subset='review', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ed689ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text.isna().sum() #43 NAN\n",
    "review_text.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd41fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sent_tokens = review_text.apply(nltk.tokenize.sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8924cf66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the character interactions in the cutscenes in this game felt more real to me than most tv show or movie characters.',\n",
       " 'it was actually enjoyable to watch a cutscene most of the time.',\n",
       " 'and the gameplay is pretty addictive once you get over your fear of running out of petrol.',\n",
       " \"i was scared of freakers for a good long time before i got decent gear and realised how they worked (or didn't - they're pretty blind and deaf) and by the games end i was disappointed if a horde wasn't very big.\",\n",
       " 'a lot of games i play i feel towards the end that they should have finished 10 hours ago but this one i wanted to keep going.',\n",
       " \"it has bugs and weird things but it's so much fun i can forgive my character yelling at the top of his voice about the latest copeland radio sound bite whilst i try to sneak up on some freakers and they don't hear me.\",\n",
       " \"it's a solidly fun game made by a team that understand how to entertain a player whilst telling a heartfelt story.\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_sent_tokens[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e54cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_word_tokens = review_text.apply(nltk.tokenize.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d7dc33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [immersive, story, ,, lovely, characters, and,...\n",
       "1        [this, game, is, a, slow, burn, ,, it, builds,...\n",
       "2        [the, character, interactions, in, the, cutsce...\n",
       "3                 [daryl, dixon, simulator, ....., yes, !]\n",
       "4        [this, game, is, incredibly, underrated, ., i,...\n",
       "                               ...                        \n",
       "33278    [great, potential, ., played, 14, hours, on, t...\n",
       "33279    [the, game, is, much, closer, to, wargame, 4, ...\n",
       "33280                                    [a10, goes, brrr]\n",
       "33281    [not, worth, the, price, for, its, stage, ., n...\n",
       "33282    [i, am, decently, surprised, ,, this, is, a, v...\n",
       "Name: review, Length: 33240, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "375a0811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation\n",
    "import re\n",
    "replaceDict = dict({\n",
    "'{':\" \", '}':\" \", ',':\"\", '.':\" \", '!':\" \", '\\\\':\" \", '/':\" \", '$':\" \", '%':\" \",\n",
    "'^':\" \", '?':\" \", '\\'':\" \", '\"':\" \", '(':\" \", ')':\" \", '*':\" \", '+':\" \", '-':\" \",\n",
    "'=':\" \", ':':\" \", ';':\" \", ']':\" \", '[':\" \", '`':\" \", '~':\" \", '': ' ', '':' ',\n",
    "})\n",
    "\n",
    "rep = dict((re.escape(k),v) for k, v in replaceDict.items())\n",
    "pattern = re.compile('|'.join(rep.keys()))\n",
    "def replacer(text):\n",
    "    return rep[re.escape(text.group(0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4977e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = review_text.str.replace(pattern, replacer).str.lower().str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67430cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "# stopwords = stopwords.extend(['', ''])\n",
    "def remove_stopwords(lst):\n",
    "    return ' '.join(word for word in lst if word not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a486db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_clean = words.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "540d7e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# stemmed_words = [ps.stem(w) for lst in words for w in lst]\n",
    "def get_stems(lst):\n",
    "    return ' '.join(ps.stem(w) for w in lst.split())\n",
    "\n",
    "review_stemmed = review_clean.apply(get_stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4b954c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        immers stori love charact voic act optim run s...\n",
       "1        game slow burn build time visual stun well pro...\n",
       "2        charact interact cutscen game felt real tv sho...\n",
       "3                                     daryl dixon simul ye\n",
       "4        game incred underr find struggl enjoy open wor...\n",
       "                               ...                        \n",
       "33278    great potenti play 14 hour earli access far de...\n",
       "33279    game much closer wargam 4 steel divis 85 warga...\n",
       "33280                                         a10 goe brrr\n",
       "33281    worth price stage campain one mode singl playe...\n",
       "33282    decent surpris solid game satisfi great number...\n",
       "Name: review, Length: 33240, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e08d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_stemmed'] = review_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943e0514",
   "metadata": {},
   "source": [
    "### Word Frequency Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "94827289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'good': 237, 'good game': 207, '': 175, 'ye': 148, 'fun': 110, 'great game': 70, 'nice': 47, 'fun game': 47, 'ok': 44, 'nice game': 40, ...})"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequency Distribution of words\n",
    "word_dist = nltk.FreqDist(review_stemmed)\n",
    "word_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17b687f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FrequencyDistribution(series):\n",
    "    fd_words = series.apply(nltk.word_tokenize)\n",
    "    fd = fd_words.apply(nltk.FreqDist)\n",
    "    return fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b33c22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        {'immers': 1, 'stori': 3, 'love': 2, 'charact'...\n",
       "1        {'game': 5, 'slow': 1, 'burn': 1, 'build': 1, ...\n",
       "2        {'charact': 3, 'interact': 1, 'cutscen': 2, 'g...\n",
       "3            {'daryl': 1, 'dixon': 1, 'simul': 1, 'ye': 1}\n",
       "4        {'game': 5, 'incred': 1, 'underr': 1, 'find': ...\n",
       "                               ...                        \n",
       "33278    {'great': 1, 'potenti': 1, 'play': 1, '14': 1,...\n",
       "33279    {'game': 9, 'much': 3, 'closer': 1, 'wargam': ...\n",
       "33280                      {'a10': 1, 'goe': 1, 'brrr': 1}\n",
       "33281    {'worth': 1, 'price': 1, 'stage': 1, 'campain'...\n",
       "33282    {'decent': 1, 'surpris': 1, 'solid': 1, 'game'...\n",
       "Name: review, Length: 33240, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = FrequencyDistribution(review_stemmed)\n",
    "fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75d094d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd2 = nltk.FreqDist(sum(review_stemmed.map(nltk.word_tokenize), []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef08b608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  game   play   like    get   time   good    one realli    fun   make   feel   even  stori     go   much  great  thing  would   hour   also \n",
      " 63671  17379  16126  12730  11375  10749   9147   8697   8582   8193   7170   6937   6240   6167   6021   5919   5907   5866   5527   5300 \n"
     ]
    }
   ],
   "source": [
    "fd2.tabulate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f3a4e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fd2.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e47d9f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = nltk.Text(review_text)\n",
    "# text.concordance('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6c1afb",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86991b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8ef836",
   "metadata": {},
   "source": [
    "### Positive/Negative Review Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e43ea4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer()\n",
    "# tf = TfidfTransformer(ngram_range=(1, 2))\n",
    "\n",
    "# unigrams and bigrams, ignore words that appear in more than 75% of docs\n",
    "tf = TfidfVectorizer(ngram_range=(1, 2), max_df=0.75)\n",
    "\n",
    "X = review_stemmed\n",
    "y = df['voted_up']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "\n",
    "# X_train = vectorizer.fit_transform(X_train) #fit_transform CountVectorizer on training data\n",
    "X_train = tf.fit_transform(X_train)         #fit_transform TfidfTransformer on training data\n",
    "\n",
    "# X_test = vectorizer.transform(X_test)       #transform CountVectorizer on testing data\n",
    "X_test = tf.transform(X_test)               #transform TfidfTransformer on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "967f9769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398509"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.DataFrame(X_train.toarray(), columns=tf.get_feature_names())\n",
    "feature_names = tf.get_feature_names_out()\n",
    "len(feature_names)\n",
    "# X=pd.DataFrame(X_txt.toarray(), columns=vect.get_feature_names())\n",
    "# print('Top 5 rows of the DataFrame: ', X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c77881be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4fc2e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(clf, X_train=X_train, X_test=X_test):\n",
    "    print(\"_\" * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split(\"(\")[0]\n",
    "    return clf_descr, score, train_time, test_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b6cfd73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: \n",
      "VotingClassifier(estimators=[('Ridge Classifier',\n",
      "                              RidgeClassifier(solver='sparse_cg', tol=0.01)),\n",
      "                             ('Perceptron', Perceptron()),\n",
      "                             ('Passive Aggressive Classifier',\n",
      "                              PassiveAggressiveClassifier()),\n",
      "                             ('Complement Naive Bayes',\n",
      "                              ComplementNB(alpha=0.01)),\n",
      "                             ('Multinomial Naive Bayes',\n",
      "                              MultinomialNB(alpha=0.01)),\n",
      "                             ('Bernoulli Naive Bayes', BernoulliNB(alpha=0.01)),\n",
      "                             ('LinearSVC', LinearSVC(dual=False, tol=0.001))])\n",
      "train time: 3.518s\n",
      "test time:  0.185s\n",
      "accuracy:   0.964\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('VotingClassifier', 0.964049338146811, 3.5180866718292236, 0.1850416660308838)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensemble Voting classifier\n",
    "# Uses all 7 classifiers to predict a class. Majority vote wins.\n",
    "rc = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n",
    "pc = Perceptron(max_iter=1000)\n",
    "pac = PassiveAggressiveClassifier()\n",
    "cnb = ComplementNB(alpha=0.01)\n",
    "mnb = MultinomialNB(alpha=0.01)\n",
    "bnb = BernoulliNB(alpha=0.01)\n",
    "lsvc = LinearSVC(penalty='l2', dual=False, tol=1e-3)\n",
    "\n",
    "eclf_sentiment = VotingClassifier(\n",
    "     estimators=[('Ridge Classifier', rc), ('Perceptron', pc), ('Passive Aggressive Classifier', pac),\n",
    "                 ('Complement Naive Bayes', cnb), ('Multinomial Naive Bayes', mnb), ('Bernoulli Naive Bayes', bnb),\n",
    "                 ('LinearSVC', lsvc)\n",
    "                ],\n",
    "     voting='hard')\n",
    "\n",
    "benchmark(eclf_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dfb57be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.90      0.92      1525\n",
      "        True       0.97      0.98      0.98      5123\n",
      "\n",
      "    accuracy                           0.96      6648\n",
      "   macro avg       0.96      0.94      0.95      6648\n",
      "weighted avg       0.96      0.96      0.96      6648\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = eclf_sentiment.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')\n",
    "plt.title('Review Sentiment Classification');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6eaac9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.Series('Trash')\n",
    "a = tf.transform(a)\n",
    "eclf_sentiment.predict(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f2a8b4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier(estimators=[('Ridge Classifier',\n",
      "                              RidgeClassifier(alpha=2.1886611994497143,\n",
      "                                              normalize=True,\n",
      "                                              solver='sparse_cg', tol=0.01)),\n",
      "                             ('Perceptron', Perceptron(penalty='l1')),\n",
      "                             ('Passive Aggressive Classifier',\n",
      "                              PassiveAggressiveClassifier(C=678.055010376656)),\n",
      "                             ('Complement Naive Bayes',\n",
      "                              ComplementNB(alpha=163.61028818957934)),\n",
      "                             ('Multinomial Naive Bayes',\n",
      "                              MultinomialNB(alpha=0.01)),\n",
      "                             ('Bernoulli Naive Bayes', BernoulliNB(alpha=0.01)),\n",
      "                             ('LinearSVC', LinearSVC(dual=False, tol=0.001))])\n"
     ]
    }
   ],
   "source": [
    "# RandomSearch\n",
    "\n",
    "# eclf_sentiment = VotingClassifier(\n",
    "#      estimators=[('Ridge Classifier', rc), ('Perceptron', pc), ('Passive Aggressive Classifier', pac),\n",
    "#                  ('Complement Naive Bayes', cnb), ('Multinomial Naive Bayes', mnb), ('Bernoulli Naive Bayes', bnb),\n",
    "#                  ('LinearSVC', lsvc)\n",
    "#                 ],\n",
    "#      voting='hard')\n",
    "\n",
    "params = {'Ridge Classifier__alpha':uniform(loc=1.0, scale=1000), 'Ridge Classifier__normalize':[True, False],\n",
    "         'Perceptron__penalty':['l2','l1','elasticnet'],\n",
    "         'Passive Aggressive Classifier__C':uniform(loc=1.0, scale=1000),\n",
    "         'Complement Naive Bayes__alpha':uniform(loc=1.0, scale=1000), 'Complement Naive Bayes__norm':[True, False],\n",
    "#           'Logistic Regression__penalty':['l2','l1','elasticnet', None], 'Logistic Regression__C':uniform(loc=1.0, scale=10000),\n",
    "         }\n",
    "eclf_sentiment_RS = RandomizedSearchCV(eclf_sentiment, params, random_state=101, n_iter=100)\n",
    "search_sentiment = eclf_sentiment_RS.fit(X_train, y_train)\n",
    "print(search_sentiment.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3894127f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9535198555956679"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eclf_sentiment_best = search_sentiment.best_estimator_\n",
    "eclf_sentiment_best.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e61a4e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>voted_up</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's better than most survival games out there...</td>\n",
       "      <td>True</td>\n",
       "      <td>simulation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bonding session with Bros and Fam!</td>\n",
       "      <td>True</td>\n",
       "      <td>simulation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was good for 7 days</td>\n",
       "      <td>True</td>\n",
       "      <td>simulation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The best combination of all the games I love t...</td>\n",
       "      <td>True</td>\n",
       "      <td>simulation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's a sandbox zombie survival game with a sim...</td>\n",
       "      <td>True</td>\n",
       "      <td>simulation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52782</th>\n",
       "      <td>It lists me as having over 200 hours of use, b...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52783</th>\n",
       "      <td>Could be cool but in League of Legends it alwa...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52784</th>\n",
       "      <td>doesnt work.   at all.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52785</th>\n",
       "      <td>Had the free version of this software for year...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52786</th>\n",
       "      <td>I am trying to get it to work with my Dark Age...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52787 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  voted_up       genre\n",
       "0      It's better than most survival games out there...      True  simulation\n",
       "1                     Bonding session with Bros and Fam!      True  simulation\n",
       "2                                 It was good for 7 days      True  simulation\n",
       "3      The best combination of all the games I love t...      True  simulation\n",
       "4      It's a sandbox zombie survival game with a sim...      True  simulation\n",
       "...                                                  ...       ...         ...\n",
       "52782  It lists me as having over 200 hours of use, b...     False         NaN\n",
       "52783  Could be cool but in League of Legends it alwa...     False         NaN\n",
       "52784                             doesnt work.   at all.     False         NaN\n",
       "52785  Had the free version of this software for year...     False         NaN\n",
       "52786  I am trying to get it to work with my Dark Age...     False         NaN\n",
       "\n",
       "[52787 rows x 3 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df = pd.read_csv('steamcharts_with_genre.csv', usecols=['review', 'voted_up', 'genre'])\n",
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b470917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df['review_stem'] = validation_df['review'].str.replace(pattern, replacer).str.lower().str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "df9dcb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>voted_up</th>\n",
       "      <th>genre</th>\n",
       "      <th>review_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's better than most survival games out there...</td>\n",
       "      <td>True</td>\n",
       "      <td>simulation</td>\n",
       "      <td>[it, s, better, than, most, survival, games, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bonding session with Bros and Fam!</td>\n",
       "      <td>True</td>\n",
       "      <td>simulation</td>\n",
       "      <td>[bonding, session, with, bros, and, fam]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was good for 7 days</td>\n",
       "      <td>True</td>\n",
       "      <td>simulation</td>\n",
       "      <td>[it, was, good, for, 7, days]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The best combination of all the games I love t...</td>\n",
       "      <td>True</td>\n",
       "      <td>simulation</td>\n",
       "      <td>[the, best, combination, of, all, the, games, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's a sandbox zombie survival game with a sim...</td>\n",
       "      <td>True</td>\n",
       "      <td>simulation</td>\n",
       "      <td>[it, s, a, sandbox, zombie, survival, game, wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52569</th>\n",
       "      <td>Don't bother, even with the sale going on. The...</td>\n",
       "      <td>False</td>\n",
       "      <td>strategy</td>\n",
       "      <td>[don, t, bother, even, with, the, sale, going,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52570</th>\n",
       "      <td>I literally can't play the game because of the...</td>\n",
       "      <td>False</td>\n",
       "      <td>strategy</td>\n",
       "      <td>[i, literally, can, t, play, the, game, becaus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52571</th>\n",
       "      <td>I've played most of the Xcom-style games that ...</td>\n",
       "      <td>False</td>\n",
       "      <td>strategy</td>\n",
       "      <td>[i, ve, played, most, of, the, xcom, style, ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52572</th>\n",
       "      <td>just missed 4 shots that were over 90% chance ...</td>\n",
       "      <td>False</td>\n",
       "      <td>strategy</td>\n",
       "      <td>[just, missed, 4, shots, that, were, over, 90,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52573</th>\n",
       "      <td>I like these types of games in general and pla...</td>\n",
       "      <td>False</td>\n",
       "      <td>strategy</td>\n",
       "      <td>[i, like, these, types, of, games, in, general...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51605 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  voted_up  \\\n",
       "0      It's better than most survival games out there...      True   \n",
       "1                     Bonding session with Bros and Fam!      True   \n",
       "2                                 It was good for 7 days      True   \n",
       "3      The best combination of all the games I love t...      True   \n",
       "4      It's a sandbox zombie survival game with a sim...      True   \n",
       "...                                                  ...       ...   \n",
       "52569  Don't bother, even with the sale going on. The...     False   \n",
       "52570  I literally can't play the game because of the...     False   \n",
       "52571  I've played most of the Xcom-style games that ...     False   \n",
       "52572  just missed 4 shots that were over 90% chance ...     False   \n",
       "52573  I like these types of games in general and pla...     False   \n",
       "\n",
       "            genre                                        review_stem  \n",
       "0      simulation  [it, s, better, than, most, survival, games, o...  \n",
       "1      simulation           [bonding, session, with, bros, and, fam]  \n",
       "2      simulation                      [it, was, good, for, 7, days]  \n",
       "3      simulation  [the, best, combination, of, all, the, games, ...  \n",
       "4      simulation  [it, s, a, sandbox, zombie, survival, game, wi...  \n",
       "...           ...                                                ...  \n",
       "52569    strategy  [don, t, bother, even, with, the, sale, going,...  \n",
       "52570    strategy  [i, literally, can, t, play, the, game, becaus...  \n",
       "52571    strategy  [i, ve, played, most, of, the, xcom, style, ga...  \n",
       "52572    strategy  [just, missed, 4, shots, that, were, over, 90,...  \n",
       "52573    strategy  [i, like, these, types, of, games, in, general...  \n",
       "\n",
       "[51605 rows x 4 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df['review_stem'].isna().sum()\n",
    "validation_df.dropna(inplace=True)\n",
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "df57fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df['review_stem'] = validation_df['review_stem'].apply(remove_stopwords).apply(get_stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f6154282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8463327196977037"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = validation_df['review_stem']\n",
    "y = validation_df['voted_up']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "X_train = tf.fit_transform(X_train)         #fit_transform TfidfTransformer on training data\n",
    "X_test = tf.transform(X_test)               #transform TfidfTransformer on testing data\n",
    "\n",
    "X.shape, y.shape\n",
    "\n",
    "eclf_sentiment2 = VotingClassifier(\n",
    "     estimators=[('Ridge Classifier', rc), ('Perceptron', pc), ('Passive Aggressive Classifier', pac),\n",
    "                 ('Complement Naive Bayes', cnb), ('Multinomial Naive Bayes', mnb), ('Bernoulli Naive Bayes', bnb),\n",
    "                 ('LinearSVC', lsvc)\n",
    "                ],\n",
    "     voting='hard')\n",
    "\n",
    "eclf_sentiment2.fit(X_train, y_train)\n",
    "eclf_sentiment2.score(X_test, y_test)\n",
    "# benchmark(eclf_sentiment2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d01b5bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7972924187725632"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.transform(review_stemmed)\n",
    "y = df['voted_up']\n",
    "eclf_sentiment2.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "67864371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [\"This is my first souls like and has gotten me hooked on the genre. Elden Ring is a masterpiece with beautiful scenery, good character design, and incredible storytelling. The difficulty is moderate and defeating a boss feels like an achievement as it should. This game is so expansive that I will likely never see the end of it.\",\n",
    "    \"\"\"I want to preface this by saying \"I LOVE THIS GAME\" However, there are a lot of bugs and it seems like they are getting worse over time. I can't complete Episode VII or access episodes after that due to a glitch with one of the quests. I go back to other quests to try to get full completion on other stages and I can no longer complete a level without the game crashing on me. I haven't seen any improvement since the last patch and no fixes so far. Once this changes I will change my review to a positive one. Can't justify a positive review for a game that has game breaking bugs regardless of how good it is.\"\"\"]\n",
    "a = tf.transform(a)\n",
    "eclf_sentiment2.predict(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8c61df",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8e464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "file_name = 'sentiment_classifier.pkl'\n",
    "\n",
    "with open(file_name, 'wb') as file:\n",
    "    pickle.dump(eclf_sentiment,file)\n",
    "    \n",
    "with open('tfidf_sentiment.pkl', 'wb') as output:\n",
    "    pickle.dump(tf, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dfe722",
   "metadata": {},
   "source": [
    "### Game Category Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8f5010c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigrams and bigrams, ignore words that appear in more than 75% of docs\n",
    "tf = TfidfVectorizer(ngram_range=(1, 2), max_df=0.75)\n",
    "\n",
    "X = review_stemmed\n",
    "y = df['genre']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "X_train = tf.fit_transform(X_train)         #fit_transform TfidfTransformer on training data\n",
    "X_test = tf.transform(X_test)               #transform TfidfTransformer on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "99bf1aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RidgeClassifier(solver='sag', tol=0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:729: UserWarning: \"sag\" solver requires many iterations to fit an intercept with sparse inputs. Either set the solver to \"auto\" or \"sparse_cg\", or set a low \"tol\" and a high \"max_iter\" (especially if inputs are not standardized).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 8.673s\n",
      "test time:  0.023s\n",
      "accuracy:   0.819\n",
      "\n",
      "Ridge Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RidgeClassifier(solver='sparse_cg', tol=0.01)\n",
      "train time: 3.966s\n",
      "test time:  0.037s\n",
      "accuracy:   0.818\n",
      "\n",
      "Perceptron Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "Perceptron()\n",
      "train time: 0.672s\n",
      "test time:  0.029s\n",
      "accuracy:   0.812\n",
      "\n",
      "Passive Aggressive Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "PassiveAggressiveClassifier()\n",
      "train time: 1.484s\n",
      "test time:  0.027s\n",
      "accuracy:   0.815\n",
      "\n",
      "l1\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(max_iter=50, penalty='l1')\n",
      "train time: 1.514s\n",
      "test time:  0.027s\n",
      "accuracy:   0.539\n",
      "\n",
      "l2\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(max_iter=50)\n",
      "train time: 1.181s\n",
      "test time:  0.038s\n",
      "accuracy:   0.799\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(max_iter=50, penalty='elasticnet')\n",
      "train time: 3.694s\n",
      "test time:  0.038s\n",
      "accuracy:   0.708\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB()\n",
      "train time: 0.227s\n",
      "test time:  0.025s\n",
      "accuracy:   0.759\n",
      "\n",
      "Bernoulli Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "BernoulliNB()\n",
      "train time: 0.224s\n",
      "test time:  0.097s\n",
      "accuracy:   0.466\n",
      "\n",
      "Complement Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "ComplementNB()\n",
      "train time: 0.216s\n",
      "test time:  0.025s\n",
      "accuracy:   0.800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for clf, name in (\n",
    "    (RidgeClassifier(tol=1e-2, solver=\"sag\"), 'Ridge Classifier'),\n",
    "    (RidgeClassifier(tol=1e-2, solver=\"sparse_cg\"), 'Ridge Classifier'),\n",
    "    (Perceptron(max_iter=1000), 'Perceptron Classifier'),\n",
    "    (PassiveAggressiveClassifier(), 'Passive Aggressive Classifier'),\n",
    "):\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))\n",
    "\n",
    "for penalty in ['l1', 'l2']:\n",
    "    print(f'{penalty}')\n",
    "    results.append(LinearSVC(penalty=penalty, dual=False, class_weight='balanced'))\n",
    "    results.append(benchmark(SGDClassifier(alpha=0.0001, max_iter=50, penalty=penalty)))\n",
    "\n",
    "results.append(benchmark(SGDClassifier(alpha=0.0001, max_iter=50, penalty=\"elasticnet\")))\n",
    "\n",
    "for clf, name in (\n",
    "    (MultinomialNB(), 'Multinomial Naive Bayes'),\n",
    "    (BernoulliNB(), 'Bernoulli Naive Bayes'),\n",
    "    (ComplementNB(), 'Complement Naive Bayes'),\n",
    "):\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f42fb0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: \n",
      "VotingClassifier(estimators=[('Ridge Classifier',\n",
      "                              RidgeClassifier(solver='sparse_cg', tol=0.01)),\n",
      "                             ('Perceptron', Perceptron()),\n",
      "                             ('Passive Aggressive Classifier',\n",
      "                              PassiveAggressiveClassifier()),\n",
      "                             ('Complement Naive Bayes', ComplementNB()),\n",
      "                             ('Logistic Regression',\n",
      "                              LogisticRegression(multi_class='multinomial',\n",
      "                                                 solver='saga'))])\n",
      "train time: 11.087s\n",
      "test time:  0.251s\n",
      "accuracy:   0.820\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('VotingClassifier',\n",
       " 0.8203971119133574,\n",
       " 11.087048053741455,\n",
       " 0.25072193145751953)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using an Ensemble voting classifier to get the best predictions from multiple models\n",
    "rc = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n",
    "pc = Perceptron(max_iter=1000)\n",
    "pac = PassiveAggressiveClassifier()\n",
    "cnb = ComplementNB()\n",
    "lr = LogisticRegression(solver='saga', multi_class='multinomial')\n",
    "\n",
    "eclf_genre = VotingClassifier(\n",
    "     estimators=[('Ridge Classifier', rc), ('Perceptron', pc), ('Passive Aggressive Classifier', pac),\n",
    "                 ('Complement Naive Bayes', cnb), ('Logistic Regression', lr)],\n",
    "     voting='hard')\n",
    "\n",
    "benchmark(eclf_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c5056b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      action       0.87      0.84      0.86      1186\n",
      "   adventure       0.72      0.82      0.76      1272\n",
      "         rpg       0.78      0.79      0.78      1009\n",
      "  simulation       0.81      0.78      0.80       918\n",
      "      sports       0.92      0.87      0.90      1076\n",
      "    strategy       0.85      0.81      0.83      1187\n",
      "\n",
      "    accuracy                           0.82      6648\n",
      "   macro avg       0.83      0.82      0.82      6648\n",
      "weighted avg       0.82      0.82      0.82      6648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = eclf_genre.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "514a83fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceba085",
   "metadata": {},
   "source": [
    "## Saving Genre Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "796ede01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "file_name = 'genre_classifier.pkl'\n",
    "\n",
    "with open(file_name, 'wb') as file:\n",
    "    pickle.dump(eclf_genre,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "dc19a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf_genre.pkl', 'wb') as output:\n",
    "    pickle.dump(tf, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "806526ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True]\n",
      "['action']\n"
     ]
    }
   ],
   "source": [
    "# # eclf.classes_\n",
    "validation = \"\"\"\n",
    "This is Hello Games.\n",
    "\n",
    "Hello Games acknowledged that they messed up by overhyping the game and lying about features that weren't present at launch.\n",
    "\n",
    "Hello Games buckled down and started fixing the game and adding features free of charge instead of taking the money and running.\n",
    "\n",
    "Hello Games went from being maligned to admired by most everyone who has played their vastly-improved No Man's Sky as well as their other work, all thanks to admitting fault and fixing their mistake.\n",
    "\n",
    "Be like Hello Games.\n",
    "\"\"\"\n",
    "validation = pd.Series(validation)\n",
    "\n",
    "val = tf.transform(validation)\n",
    "print(eclf_sentiment.predict(val))\n",
    "print(eclf_genre.predict(val))\n",
    "# review2 = \"\"\"One of the best comeback stories in modern videogames. Strongly recommend for any would-be scifi adventurer into spaceships, alien worlds and space exploration, and base building or resource exploiting. I absolutely HATED this game at release after it absolutely botched the landing in apocalyptic fashion, turning all of the hype around it into one of the most stunningly underwhelming and disappointing slogs I'd ever played. That said, the commitment of the development team to rectify that massive blunder is one of the most admirable efforts I've ever seen out of a developer. They could have taken the money and ran, but they really dug in to make their game as great as they dreamed about in their head before launch and this game has come a LONG way in a BIG way because of that, and is an entirely different and far more worthwhile experience. Just watch a video of what this game was like at launch and compare that against the history of even just the first 4 updates yourself for the proof in the pudding. BARRING the catastrophic launch, this game could serve as a great template for continuing to support a released videogame. If they stopped the updates right now, hell even 3 updates ago, the game would stand as a valiant success against the odds of a pointedly deserved animosity. The history of this game starting as one of the hardest failures in the industry wont be forgotten but that arguably serves it well, and makes for a great story for just how much hard work and dedication can improve even the worst of failures into a rousing success.\"\"\"\n",
    "# review2 = pd.Series(review2)\n",
    "# validation = validation.append(review2)\n",
    "# validation\n",
    "# validation = validation.str.replace(pattern, replacer).str.lower().str.split()\n",
    "# validation = validation.apply(remove_stopwords).apply(get_stems)\n",
    "# val = tf.fit_transform(validation)\n",
    "# eclf.predict(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6329eeb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.Series('Looks like shit, runs like shit and feels like shit')\n",
    "a = tf.transform(a)\n",
    "eclf_sentiment.predict(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a87706f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7925691937424789"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec1a54ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sports'], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4203dda9",
   "metadata": {},
   "source": [
    "## Using RandomSearch and Gridsearch for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "37c1a8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7c56e394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), RidgeClassifier())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier(estimators=[('Ridge Classifier',\n",
      "                              RidgeClassifier(alpha=2.1886611994497143,\n",
      "                                              normalize=True,\n",
      "                                              solver='sparse_cg', tol=0.01)),\n",
      "                             ('Perceptron', Perceptron(penalty='l1')),\n",
      "                             ('Passive Aggressive Classifier',\n",
      "                              PassiveAggressiveClassifier(C=678.055010376656)),\n",
      "                             ('Complement Naive Bayes', ComplementNB()),\n",
      "                             ('Logistic Regression',\n",
      "                              LogisticRegression(multi_class='multinomial',\n",
      "                                                 solver='saga'))])\n"
     ]
    }
   ],
   "source": [
    "# RandomSearch\n",
    "params = {'Ridge Classifier__alpha':uniform(loc=1.0, scale=1000), 'Ridge Classifier__normalize':[True, False],\n",
    "         'Perceptron__penalty':['l2','l1','elasticnet'],\n",
    "         'Passive Aggressive Classifier__C':uniform(loc=1.0, scale=1000),\n",
    "#          'Complement Naive Bayes__alpha':uniform(loc=1.0, scale=1000), 'Complement Naive Bayes_norm':[True, False],\n",
    "#           'Logistic Regression_penalty':['l2','l1','elasticnet', None], 'Logistic Regression__C':uniform(loc=1.0, scale=10000),\n",
    "         }\n",
    "eclf_genre_RS = RandomizedSearchCV(eclf_genre, params, random_state=101, n_iter=100)\n",
    "search_genre = eclf_genre_RS.fit(X_train, y_train)\n",
    "print(search_genre.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4b910ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8164861612515042"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eclf_genre_best = search_genre.best_estimator_\n",
    "eclf_genre_best.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe107f",
   "metadata": {},
   "source": [
    "## Topic Modelling with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "93433a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "60a1cdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "id2word = corpora.Dictionary(fd)\n",
    "\n",
    "corpus = []\n",
    "for text in fd:\n",
    "    new = id2word.doc2bow(text)\n",
    "    corpus.append(new)\n",
    "\n",
    "print (corpus[0][0:20])\n",
    "\n",
    "word = id2word[[0][:1][0]]\n",
    "print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8feb4c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10,\n",
    "                                           random_state=100,\n",
    "                                           update_every=5,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "beb6ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model2 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=25,\n",
    "                                           random_state=100,\n",
    "                                           update_every=5,\n",
    "                                           chunksize=100,\n",
    "                                           passes=20,\n",
    "                                           alpha=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "685aad56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('world', 0.00740813),\n",
       " ('stori', 0.007114074),\n",
       " ('interest', 0.0066644936),\n",
       " ('combat', 0.0062972703),\n",
       " ('charact', 0.006271747),\n",
       " ('feel', 0.006109626),\n",
       " ('mechan', 0.0058412957),\n",
       " ('end', 0.005727996),\n",
       " ('build', 0.0056521534),\n",
       " ('cool', 0.0053102127)]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topic(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8be78b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1535228673056927369685498979\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1535228673056927369685498979_data = {\"mdsDat\": {\"x\": [0.14930721095162572, -0.03578914936185036, -0.22627837510461213, -0.23010584550714608, 0.39945118978937716, 0.33136479418854803, 0.1450164452436984, -0.31688653725291854, -0.19055924123476037, -0.025520491711961763], \"y\": [-0.3434848902288574, -0.3931900240364713, -0.3419996702628593, 0.41555299965305714, -0.11458661262188957, 0.1245989458106676, 0.347960436293196, 0.03467169727616735, 0.14340340871338847, 0.12707370940360088], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [42.41894660997853, 26.262320490161468, 21.55459366825702, 3.682688125044923, 2.417011252221419, 1.9130037215522553, 0.7607122992111659, 0.4803105684386925, 0.3583591288230273, 0.15205413631148107]}, \"tinfo\": {\"Term\": [\"game\", \"play\", \"good\", \"recommend\", \"love\", \"time\", \"get\", \"fun\", \"worth\", \"graphic\", \"bug\", \"10\", \"like\", \"look\", \"price\", \"2\", \"stori\", \"gameplay\", \"much\", \"bad\", \"need\", \"great\", \"space\", \"money\", \"hard\", \"3\", \"run\", \"state\", \"empir\", \"easi\", \"fun\", \"updat\", \"releas\", \"friend\", \"crash\", \"amaz\", \"unit\", \"support\", \"dlc\", \"mod\", \"month\", \"awesom\", \"multiplay\", \"game\", \"buy\", \"polish\", \"concept\", \"divis\", \"bought\", \"commun\", \"relax\", \"rt\", \"dev\", \"eugen\", \"sim\", \"steel\", \"combin\", \"cost\", \"na\", \"best\", \"play\", \"great\", \"wait\", \"good\", \"year\", \"realist\", \"fix\", \"race\", \"content\", \"recommend\", \"love\", \"like\", \"better\", \"still\", \"access\", \"hour\", \"realli\", \"10\", \"ever\", \"time\", \"get\", \"worth\", \"earli\", \"bug\", \"would\", \"one\", \"look\", \"lot\", \"much\", \"even\", \"make\", \"enjoy\", \"2\", \"want\", \"feel\", \"see\", \"go\", \"puzzl\", \"dont\", \"lose\", \"cant\", \"ton\", \"promis\", \"damag\", \"fli\", \"pass\", \"specif\", \"week\", \"shot\", \"poor\", \"button\", \"throw\", \"tank\", \"spot\", \"path\", \"box\", \"destruct\", \"solv\", \"fish\", \"success\", \"defens\", \"target\", \"block\", \"research\", \"wheel\", \"useless\", \"luck\", \"attack\", \"deal\", \"hit\", \"click\", \"group\", \"call\", \"random\", \"fire\", \"ship\", \"basic\", \"forc\", \"often\", \"move\", \"kill\", \"except\", \"minut\", \"speed\", \"shoot\", \"enemi\", \"anoth\", \"reason\", \"use\", \"tell\", \"weapon\", \"instead\", \"anyth\", \"mean\", \"allow\", \"two\", \"point\", \"sometim\", \"problem\", \"back\", \"go\", \"find\", \"need\", \"take\", \"real\", \"right\", \"start\", \"want\", \"player\", \"everi\", \"thing\", \"come\", \"make\", \"give\", \"cool\", \"strategi\", \"screen\", \"depth\", \"engag\", \"complex\", \"quest\", \"im\", \"push\", \"playabl\", \"market\", \"board\", \"rough\", \"fail\", \"imagin\", \"somewhat\", \"dialogu\", \"grow\", \"classic\", \"endless\", \"plot\", \"editor\", \"faster\", \"prefer\", \"behold\", \"trade\", \"immedi\", \"store\", \"aesthet\", \"4x\", \"combat\", \"side\", \"uniqu\", \"seri\", \"voic\", \"act\", \"present\", \"repetit\", \"explor\", \"genr\", \"action\", \"charact\", \"offer\", \"immers\", \"interest\", \"felt\", \"world\", \"addit\", \"choic\", \"manag\", \"citi\", \"compar\", \"detail\", \"main\", \"stori\", \"open\", \"line\", \"end\", \"gener\", \"mechan\", \"design\", \"build\", \"system\", \"feel\", \"complet\", \"well\", \"experi\", \"differ\", \"way\", \"war\", \"challeng\", \"also\", \"map\", \"expect\", \"can\", \"not\", \"ea\", \"broken\", \"buggi\", \"worst\", \"countri\", \"compani\", \"horribl\", \"defend\", \"dog\", \"80\", \"unfinish\", \"imo\", \"steep\", \"vs\", \"spam\", \"1000\", \"delet\", \"flesh\", \"competit\", \"proper\", \"code\", \"singleplay\", \"pointless\", \"outcom\", \"log\", \"70\", \"doesnt\", \"public\", \"footbal\", \"third\", \"per\", \"fuck\", \"unabl\", \"cheat\", \"terribl\", \"server\", \"four\", \"state\", \"wors\", \"remov\", \"due\", \"mess\", \"half\", \"parti\", \"us\", \"rpg\", \"armi\", \"chill\", \"nation\", \"ball\", \"plane\", \"skirmish\", \"intuit\", \"cold\", \"rome\", \"legion\", \"role\", \"copi\", \"histor\", \"artilleri\", \"leader\", \"militari\", \"roman\", \"expedit\", \"popul\", \"demand\", \"settlement\", \"victori\", \"strength\", \"default\", \"oper\", \"peac\", \"territori\", \"deploy\", \"trait\", \"squad\", \"citizen\", \"histori\", \"battl\", \"field\", \"wargam\", \"send\", \"15\", \"expans\", \"cup\", \"audienc\", \"workshop\", \"w\", \"infin\", \"potato\", \"adult\", \"dri\", \"\\u2018\", \"ark\", \"leaderboard\", \"usag\", \"perspect\", \"coffe\", \"nasa\", \"teen\", \"boi\", \"ms\", \"eargasm\", \"grandma\", \"do\", \"deaf\", \"greatli\", \"mehh\", \"terrarium\", \"penni\", \"trail\", \"32\", \"spare\", \"evolv\", \"paint\", \"dark\", \"averag\", \"soul\", \"rich\", \"comput\", \"rank\", \"master\", \"realiti\", \"brain\", \"necessari\", \"easi\", \"short\", \"requir\", \"hard\", \"price\", \"long\", \"learn\", \"space\", \"biggest\", \"drift\", \"planet\", \"companion\", \"galaxi\", \"ban\", \"men\", \"22\", \"format\", \"conflict\", \"legendari\", \"republ\", \"\\u201c\", \"\\u201d\", \"defin\", \"asteroid\", \"boat\", \"vr\", \"battleship\", \"fellow\", \"assault\", \"won\", \"women\", \"cruis\", \"\\u2013\", \"revisit\", \"iii\", \"fate\", \"navi\", \"doesn\", \"\\u2019\", \"it\", \"t\", \"don\", \"autom\", \"star\", \"empir\", \"account\", \"n\", \"children\", \"codemast\", \"de\", \"pour\", \"bugfix\", \"un\", \"si\", \"franc\", \"dice\", \"www\", \"su\", \"brick\", \"jungl\", \"lego\", \"_\", \"en\", \"bom\", \"se\", \"shader\", \"ici\", \"dad\", \"ha\", \"premier\", \"mayor\", \"vi\", \"au\", \"civ\", \"com\", \"http\", \"youtub\", \"builder\", \"stuck\", \"truck\", \"wont\", \"alright\", \"min\", \"polici\", \"lord\", \"closest\", \"maker\", \"usa\", \"stone\", \"sex\", \"pve\", \"assist\", \"enthusiast\", \"unbear\", \"heap\", \"wiki\", \"8th\", \"hail\", \"miser\", \"shower\", \"allot\", \"unstuck\", \"blueprint\", \"chair\", \"thingi\", \"yellow\", \"ign\", \"11\", \"ring\", \"sold\", \"holi\", \"pokemon\", \"elden\", \"rn\", \"gloriou\", \"honor\", \"fox\", \"simultan\", \"goti\", \"seat\", \"drove\", \"slack\", \"hi\", \"daddi\", \"throwback\", \"pizza\", \"glu\", \"reb\", \"ooh\", \"oldschool\", \"johnni\", \"christma\", \"drunkenli\", \"church\", \"nuzlock\", \"kawaii\", \"tamer\", \"disservic\", \"wholesom\", \"coromon\"], \"Freq\": [20801.0, 9636.0, 6747.0, 3921.0, 3738.0, 6656.0, 6796.0, 6118.0, 2652.0, 2351.0, 2425.0, 2796.0, 8578.0, 3713.0, 1935.0, 3167.0, 2442.0, 2513.0, 4373.0, 2161.0, 3712.0, 4720.0, 758.0, 1817.0, 2067.0, 2386.0, 2131.0, 1332.0, 649.0, 1676.0, 6117.496280688801, 1920.9451066340341, 1963.5360720573976, 1607.4428923024318, 1581.5615701158654, 1449.831717277975, 1356.62436596958, 1249.5013836812998, 1027.2377086439449, 984.527640968279, 865.7493813144631, 837.0405601507719, 831.5594399774202, 20781.146942353163, 2940.4981308250717, 777.8802274766565, 759.8182846090069, 701.7302174295336, 679.197452189585, 1341.4906699262924, 614.8030070566757, 612.8431133271206, 1917.9108185289988, 588.9074156147609, 704.6298682058791, 577.320550122649, 574.8326939452063, 557.7323147686093, 546.3592004201224, 2483.9001414720865, 9621.387880729966, 4697.177871382098, 2372.106509699311, 6624.143222877442, 2094.2332450767735, 897.5158507350905, 2141.0391644106053, 916.1486788913126, 2133.80419165045, 3794.912372422217, 3618.9464471088704, 8102.8242674544335, 2943.9993918188447, 3547.839295469018, 2159.68102592606, 3607.867981375862, 4949.566015803264, 2662.7259017363713, 1807.9446871125667, 5952.661985592099, 6036.260949692281, 2497.359501504745, 2452.807894366667, 2272.329201974776, 3711.6544895547145, 4331.257547528459, 3162.2172726663925, 3217.561269982673, 3432.515634042493, 3422.978938334942, 3536.9106569171668, 2658.9010508551833, 2497.3371770246035, 2654.387823478079, 2715.9939525487516, 2503.6685837122186, 2506.8968876601352, 924.5712095008902, 765.7934588658272, 723.7882691178363, 715.3491864739159, 718.2920931937047, 551.1570488830395, 540.8999784350614, 538.0770122450318, 475.15664492629134, 463.10908714618137, 451.2210819745631, 430.68717066161366, 411.41464757520845, 402.7339383754945, 402.27551405786113, 394.33877811743486, 377.9911012077272, 373.0973708551097, 367.1266693007664, 351.50444201640676, 343.20400164805005, 342.06675739127445, 347.8820029100166, 337.6583462478758, 335.580101386966, 330.22395701094354, 325.74244875822916, 315.79080453601756, 314.2405901095067, 313.9788012906173, 685.8824308054355, 593.2078352684205, 971.5215683080542, 533.1449165967355, 495.15044982040814, 753.1347636552008, 826.9416131297648, 627.8493105674336, 859.7822349861145, 1279.7170773484752, 827.8528334301038, 712.5977856311647, 1237.5383353994189, 956.1596337029196, 646.7526077580994, 846.7892594276233, 626.1775526043803, 539.6528747800745, 1096.4811479279754, 1097.4688850854343, 1173.1570945655533, 2119.98430775544, 833.9429055562389, 735.7284610658422, 977.6578782742926, 1090.1871733390121, 1069.075146535281, 763.2001501142764, 1140.0384044658713, 1400.2582003158227, 863.4100886931948, 927.6122202358132, 1245.8001611154862, 1651.304308946655, 1059.0520082105368, 1343.2632585153922, 1179.9970176425147, 943.6623790865517, 1058.9778424300323, 1070.205586918821, 1103.2831688001063, 960.3844471360779, 961.6320851266593, 991.7370512291064, 952.3172620658689, 1000.4365334193708, 936.0156092620268, 1333.6176291593556, 843.5306100655787, 812.7315884247728, 714.0499599722452, 652.2998854171676, 638.5435695318799, 831.682286662287, 606.9313559987321, 466.6651103404164, 460.069408961802, 414.051910663151, 410.4654052117422, 392.48346779082226, 391.6337591560651, 390.65991113916647, 377.3405097443929, 366.5083692203034, 362.967765518768, 342.4802942375646, 321.8423997059801, 309.8852862318103, 309.1599216366782, 307.70422219324297, 300.7154322623694, 299.7133999863129, 287.8553992077294, 286.2378741684032, 285.10068024287887, 289.964131444267, 276.91283135289376, 1581.5092984307094, 990.9590197800857, 914.7380238808644, 1199.5688486133413, 750.5775199972431, 697.9382118724383, 514.2915673008696, 501.9453414769774, 842.7730265351852, 739.167865291467, 504.71628685659203, 1575.0993090999903, 591.1302779588958, 569.6113621975769, 1673.734509540544, 639.2365397710505, 1860.4928834385578, 643.1659634544748, 854.8193414353826, 1315.448722508856, 1018.8800309406168, 776.4393260322465, 789.229073859692, 1091.8943364410438, 1786.6431076168694, 855.314203230514, 919.000746894579, 1438.5406091761888, 1067.1597838252228, 1466.9948992651848, 1110.1187929799253, 1419.493312606457, 1197.2936421323586, 1534.383930700243, 1091.3539237374396, 1276.6573583483853, 1039.401212233033, 1063.8964910138457, 1233.7050151802887, 889.0797643316049, 876.7655235487073, 932.8892130406485, 913.5259837899415, 871.1382613820701, 825.2641867881802, 755.4950985072074, 644.4147748358489, 549.9599446892906, 508.7076129427222, 410.48338917128433, 364.2050660523215, 336.888920830569, 321.11585174692766, 279.415388750856, 269.0956821514169, 268.87107704281755, 261.6843729367849, 258.6418438827289, 246.22226633914408, 245.30346540383286, 223.13015975971354, 220.1116277464393, 215.5716313973046, 208.98571248879583, 204.32083561935633, 183.96984647017413, 178.21619129162616, 171.3335163051067, 164.35418379587136, 161.2756391997595, 160.89870982771316, 160.6411262713117, 157.72436688686506, 153.74895037480746, 168.34607454616759, 318.6070920079792, 295.3484048049037, 489.3548887015316, 174.56328621608688, 208.87935424726263, 397.55655694466316, 374.0388175902674, 181.7118667968809, 575.4013635593445, 301.06103923014194, 276.25725030632265, 284.83540314091607, 249.84104406609447, 246.50127939282984, 225.9737991019113, 223.66352945715607, 391.6959633004499, 375.12088615538653, 351.0017833117158, 287.72027049803523, 268.39795806743376, 261.52727332204296, 253.2507372240906, 251.52936794735987, 232.47638665252674, 225.93375661997948, 222.11281619897693, 214.83342689728286, 202.02998230144834, 197.2623700837771, 195.82676352880742, 183.34237758197688, 179.54171108910256, 154.71232572977206, 153.69539021582582, 146.57080927218735, 143.03578893849402, 140.68099710261038, 130.8799953701301, 129.8434022764528, 126.96298328194285, 126.95810494994822, 119.03947056381733, 114.78616829909667, 108.59834512258935, 108.25718317095914, 178.54723950716388, 117.58604236051238, 187.46761051804268, 278.8960760647066, 138.0010093613424, 163.43798310232734, 137.81315423816187, 547.0305804464717, 532.6250256205226, 254.4075867683896, 230.21978503810158, 223.4969644427113, 220.84056402422576, 200.05362408299652, 192.26823916851603, 187.83628291614565, 186.4167972908168, 179.18497876423123, 172.93515051636336, 169.0335734296553, 168.3415272212463, 161.4509573484503, 156.94166537859752, 150.52397368168357, 150.21225396578657, 147.33200447751946, 137.985436552786, 135.25900740180097, 131.17595968236412, 130.5974831183911, 126.00692169908363, 123.12498039469122, 121.96511070029602, 120.48362987425284, 115.34695436139377, 73.55415554816919, 70.72103299926978, 177.23121137898784, 163.83752866381013, 207.20394481684664, 187.0279477024122, 178.5240910343999, 189.51670263589233, 150.9533213248692, 174.04475355128102, 162.0539704358602, 174.35499941509337, 159.522110518525, 165.33940469651722, 159.09971696616117, 177.51423249619486, 171.08223820651608, 170.10272856616618, 170.14469183361769, 167.6333620530668, 165.6918421710618, 164.52479004256412, 757.283894954683, 312.5750980974302, 207.8018951381147, 192.5831897473818, 189.30180637469886, 137.453170561454, 119.57635956895116, 116.59713045588337, 111.84756992728454, 111.29879063175747, 111.13457205343862, 110.6794998706629, 98.1667029261276, 81.89167106451258, 81.45538653048793, 79.79725270802564, 71.60296522436863, 70.58120109325837, 67.65835969047991, 67.52533554301795, 62.33783541731249, 58.90083896886915, 55.30500186528856, 53.23079096229135, 52.547373729613895, 52.2899309372484, 51.91577936989742, 51.86086511975837, 50.61380604253086, 49.354608460255704, 78.88173454663198, 302.00171315250685, 152.76398991194662, 191.73954507354327, 80.21749840131069, 70.51035118586364, 56.05136927783229, 648.9337115905273, 174.88493178328767, 93.03790100952709, 90.781868669524, 71.41879678681265, 69.96453216385022, 66.87443353081123, 52.66259101563247, 51.08787777378185, 51.077281839459054, 46.371462187269024, 40.242222348576135, 39.880516866004854, 38.43042442475684, 37.933559538926445, 30.547468852157696, 27.745649473092094, 26.14570248012303, 25.165177233600836, 23.09929578876035, 22.976298516707445, 22.509965349156882, 21.667320154057357, 20.04510632369993, 19.959859347908694, 19.861229337017928, 19.845365404359296, 19.41262989624223, 19.036092451005388, 18.7721947911107, 67.05538269353562, 48.506592467023324, 22.190549788422853, 413.9947678891396, 390.54907427905846, 198.6123937774698, 180.98798498444734, 133.01424809144638, 118.21046737094616, 116.31549216446541, 81.1516103450987, 67.04975323507439, 59.614074445345906, 57.603097093650554, 55.22752823878632, 50.61143147628952, 45.291882866145095, 40.87204986520929, 26.297292789356817, 21.081314328910548, 20.143206135487397, 18.524567707670197, 16.61458427888862, 16.467451592508272, 16.057416421024087, 14.930669125160632, 14.26865564229941, 13.501531134474662, 13.351280727604692, 12.134246890639947, 11.624471883997392, 11.562803819906822, 10.675860462582396, 66.65331373588435, 64.0735891443241, 52.66117450158565, 46.7530842049851, 34.161278972311216, 31.86794579289416, 31.541237083322795, 29.974326219636964, 24.756739882445373, 18.774583753838293, 18.702491294708853, 15.19008722073998, 13.760972563122301, 12.677990148110382, 11.801897564513348, 11.236875205429916, 10.77014364539438, 8.350861922069678, 7.679909368937564, 7.344414530126636, 7.081434544639573, 6.4727368733489, 6.380284931549174, 6.29760704291055, 6.260256093778473, 6.203813257053721, 4.820363371799266, 4.761651403762457, 4.276534558922386, 4.224350726160005, 2.830607457382355, 8.699596178912204, 6.812625189273941], \"Total\": [20801.0, 9636.0, 6747.0, 3921.0, 3738.0, 6656.0, 6796.0, 6118.0, 2652.0, 2351.0, 2425.0, 2796.0, 8578.0, 3713.0, 1935.0, 3167.0, 2442.0, 2513.0, 4373.0, 2161.0, 3712.0, 4720.0, 758.0, 1817.0, 2067.0, 2386.0, 2131.0, 1332.0, 649.0, 1676.0, 6118.830203519294, 1921.7339998419868, 1964.343890557588, 1608.2329535364215, 1582.3505365454243, 1450.6206544625325, 1357.4135460233663, 1250.347388788091, 1028.0266181294332, 985.3171843610007, 866.5383090123566, 837.8294286354216, 832.3564311967143, 20801.470336285325, 2943.4679169514084, 778.6691491710485, 760.6072768167961, 702.5196244868185, 679.9863370189888, 1343.091866901099, 615.5918996537044, 613.632212026404, 1920.425699222736, 589.6972117150876, 705.5850249375557, 578.1102008086783, 575.6216648644872, 558.5212539787524, 547.1480665820233, 2487.5956041955583, 9636.229864343682, 4720.075704330617, 2380.799325069473, 6747.755830833452, 2114.6069954009613, 899.2534850779344, 2164.5007516947344, 918.3701663588997, 2171.5583766154095, 3921.2230379048706, 3738.1629684060813, 8578.266108987915, 3030.875364457141, 3693.117333985685, 2219.458161551417, 3793.3763304394324, 5318.829290284016, 2796.286429485479, 1865.300973768236, 6656.904190801833, 6796.594281266709, 2652.5125010329616, 2621.82927636753, 2425.3826097922342, 4272.946327810004, 5456.871882305112, 3713.478518032214, 3820.2094740426323, 4373.03830667068, 4555.911617807319, 5199.628137124305, 3293.3601759961566, 3167.0437805494357, 3875.643816390689, 4440.520340335199, 3316.4361027704977, 4447.9753268611885, 925.3522391925886, 766.575116368139, 724.5703183068664, 716.1299753152662, 719.076441497445, 551.9379348869604, 541.6807509556629, 538.8578848604861, 475.9374148482979, 463.89008685818027, 452.0018610403183, 431.468762906062, 412.1954572086404, 403.5147159736606, 403.05630715545925, 395.12654131203334, 378.7718927712171, 373.8781890181146, 367.90791937187214, 352.28548066303847, 343.9848528770172, 342.8475926186419, 348.67869575676764, 338.43916735503285, 336.3609086194989, 331.0047481544978, 326.5256798464267, 316.5719119937533, 315.0213814952863, 314.75959184337535, 688.5075489368708, 595.1966245484754, 981.3526631352387, 535.5522688436474, 497.2657323012233, 760.0937795856178, 836.0001989103043, 635.7776004396886, 876.4712925290337, 1362.7848429917524, 862.6309335794797, 736.7948206006357, 1335.308795835953, 1015.6416686567884, 670.9918196237713, 908.248170088264, 652.4544177875812, 556.0757273582431, 1226.7099557220859, 1279.3307127186383, 1388.5303017102037, 3018.0008154226325, 960.3246444981718, 821.865470788987, 1215.2412370857696, 1411.5925693783608, 1392.2819224284065, 879.8978041447826, 1577.466537205487, 2208.510923741256, 1082.1644151528415, 1275.5351489190898, 2394.0586175615035, 4447.9753268611885, 1876.4849867798891, 3712.4956002891477, 2709.7361511294766, 1551.3254001327634, 2356.3508847549274, 2469.5177948729756, 3875.643816390689, 2676.8014736911714, 2894.3113774014323, 3813.805533897609, 2761.0860988899262, 5199.628137124305, 2571.847956085972, 1334.3971365967689, 844.3101639011629, 813.5111352005856, 714.8294509137861, 653.079388400293, 639.3236433778251, 832.7228225404141, 607.7108787422408, 467.44463255905197, 460.848986506527, 414.8314585077542, 411.24500578691146, 393.2629994481019, 392.41330906909457, 391.4394186833007, 378.1199887295056, 367.287791857115, 363.7513930141972, 343.259755300321, 322.62198047007723, 310.6647222737793, 309.93956446879207, 308.4837216962582, 301.49490551857286, 300.4932107564852, 288.63496096159446, 287.0173930040917, 285.8801851309112, 290.7715443191511, 277.69277996990184, 1593.5032250284914, 993.9476847759724, 918.6130396617705, 1208.307564138904, 753.47103648753, 700.4762555329902, 516.037433761769, 504.7609496865866, 852.9876482065453, 750.1016477370061, 510.0994809325096, 1636.1843034348917, 600.3592293795645, 577.8954252469608, 1759.576729010645, 654.7122738373922, 2017.0272095441921, 671.2266426965487, 917.9263889544329, 1492.4979045762757, 1127.0664253337973, 831.3564557036949, 854.2162935254225, 1250.2230931333484, 2442.001510780325, 977.4788386457597, 1080.847742207695, 2001.6166596384578, 1389.9305152687016, 2463.623982210699, 1601.608357849874, 2475.3684835606145, 2086.3022312163466, 4440.520340335199, 1973.8517280358544, 3284.493648861162, 1959.3577570089121, 2122.8643767465232, 3787.9137079058055, 1261.4779548621273, 1270.6350781364902, 3566.1966322488656, 2675.2481245777412, 1554.5764205898122, 826.0533439602323, 756.282928097619, 645.2026629205639, 550.747798365745, 509.49543975694303, 411.2783052442434, 364.9930015650921, 337.67679311907244, 321.9036716563175, 280.2033282601502, 269.88421501765356, 269.65897175126133, 262.47223231742703, 259.42974361818756, 247.01020778011923, 246.091386153312, 223.9181239076774, 220.8995601670207, 216.3594511991002, 209.77363956182236, 205.1087493363298, 184.75775304839468, 179.00424324767917, 172.12143057574423, 165.1420501142955, 162.0636547208431, 161.68650984283477, 161.43001950649625, 158.51223614698606, 154.53682556677822, 169.41453294849168, 330.78525700143103, 310.49933559528273, 559.5244297756982, 178.1189771572635, 223.60699057666764, 558.8904611105442, 521.7147714451683, 188.65615336289008, 1332.6566057728035, 579.3779748211406, 573.719737794927, 757.219693343444, 634.3164685863818, 679.6949479975087, 365.4424861677234, 816.6911729586373, 392.49627936098244, 375.9224461350053, 351.80214712331076, 288.5210436865888, 269.1983817886178, 262.3278201679365, 254.0515787463058, 252.3298529322243, 233.27699734638264, 226.73415193948904, 222.9132305283113, 215.63380800174141, 202.83044892583436, 198.06274951814794, 196.62733619479724, 184.14295734080986, 180.3422378988234, 155.51267234556764, 154.4957608936696, 147.37123581779971, 143.83624905698514, 141.4815838955005, 131.6804802804121, 130.6438049832401, 127.76341039575809, 127.75868918697883, 119.83980796877673, 115.5892947731759, 109.39884734182985, 109.05757423298202, 199.22531845436745, 122.01228205773111, 309.26607384088584, 1398.8064037863003, 201.19754476447181, 1001.3089191920765, 274.8652520393648, 547.8149650198571, 533.4095031393284, 255.19178613499398, 231.0040381202714, 224.28166429914722, 221.62470505791592, 200.8377449728813, 193.05240029017148, 188.6204102861314, 187.20096214242454, 179.96921060185778, 173.71926241204707, 169.8176898130951, 169.12572413627356, 162.2357466526529, 157.72576491358657, 151.30804462941038, 150.99633974372387, 148.11617826761417, 138.76950787467533, 136.04307448788782, 131.96006626881217, 131.38157723811557, 126.79100162823977, 123.90945701200353, 122.74917767425374, 121.26769692237106, 116.13129107855232, 74.33868933605085, 71.50549372040439, 194.32175938305718, 198.2839815901245, 314.0034900954464, 367.2128240684923, 343.7641234752601, 453.9426513280316, 239.22889834965022, 421.8464511407246, 318.08836077257996, 435.299166643117, 303.3267199360015, 417.9895526471208, 338.1702651424302, 1676.2019841279173, 1072.5004492636838, 1029.4837943495547, 2067.999950039706, 1935.5757097803705, 1878.348938763342, 1445.336403191922, 758.0837486663877, 313.3749655019954, 208.60166033996535, 193.3829523654024, 190.10185281619857, 138.25300247474772, 120.37623176508698, 117.39691363453802, 112.64737879323798, 112.0986769413491, 111.93458078604752, 111.47947748597562, 98.96673331699553, 82.69136541577488, 82.2550863675423, 80.59711686754937, 72.40293232135713, 71.38102961431268, 68.45807495122735, 68.32529548899569, 63.13766666270816, 59.70098540141928, 56.104732556827095, 54.03055179132491, 53.347169535921665, 53.089708542833485, 52.715652828989484, 52.660868861255224, 51.4137594808178, 50.15498623253766, 80.31649166284139, 327.7024119299327, 183.46808791100833, 334.76331069408013, 224.39987490264753, 256.82146186893686, 355.9867591766171, 649.7309462935292, 175.68155101090156, 93.83450094470565, 91.57867846721314, 72.21567220669357, 70.76114089911303, 67.67111774940699, 53.45945421188542, 51.88449771368444, 51.874316658786924, 47.16873362056499, 41.03916063359099, 40.677114793252365, 39.227353300795514, 38.73037442735667, 31.344618324175105, 28.542193673177785, 26.94225939310003, 25.961701148980605, 23.895849054274, 23.772912937822774, 23.306572058792586, 22.463964188256966, 20.841904749350647, 20.75648182441854, 20.657895808754827, 20.641939055012692, 20.20947101019303, 19.832964995874473, 19.569027477988147, 156.79466329515242, 137.33114715647255, 255.8159846248272, 414.8104999265889, 391.3646431664998, 199.42806498842108, 181.8035309519733, 133.82982940330712, 119.02601026313799, 117.13174357270044, 81.96769199233732, 67.86545459218527, 60.42966471057094, 58.4189116865651, 56.04311796816796, 51.42690089829235, 46.10738879602434, 41.687621363197, 27.112806792598523, 21.89690115486697, 20.9588550257447, 19.340137923891707, 17.430303495764612, 17.28334699687807, 16.872925159718797, 15.746168277983289, 15.084240807619093, 14.317060830592158, 14.166790478990967, 12.949782044377079, 12.43993939340939, 12.378348186980318, 11.491348685139936, 176.85493179357942, 64.90958906972322, 53.497202125185375, 47.58906885952455, 34.997251005742015, 32.70390086642963, 32.37721926632782, 30.810508193866582, 25.592920177624364, 19.610718358057742, 19.538682227564, 16.026071941528194, 14.597023771160911, 13.514165473982393, 12.638194695392055, 12.073010966794053, 11.606292239801448, 9.186822880812404, 8.516011493858613, 8.180474468643501, 7.917481499053083, 7.30873581094463, 7.216313399980424, 7.1335684244176445, 7.09631019611625, 7.039896333670198, 5.6566228059854975, 5.597724180788767, 5.113061700191229, 5.060417022106417, 3.666629711027328, 14.580997593195075, 32.76915443621622], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.3919, -5.5502, -5.5283, -5.7284, -5.7446, -5.8316, -5.898, -5.9803, -6.1762, -6.2186, -6.3472, -6.3809, -6.3875, -3.169, -5.1244, -6.4542, -6.4777, -6.5572, -6.5899, -5.9092, -6.6895, -6.6927, -5.5518, -6.7325, -6.5531, -6.7524, -6.7567, -6.7869, -6.8075, -5.2932, -3.939, -4.6561, -5.3392, -4.3123, -5.4638, -6.3111, -5.4417, -6.2906, -5.4451, -4.8694, -4.9168, -4.1108, -5.1233, -4.9367, -5.4331, -4.9199, -4.6037, -5.2237, -5.6108, -4.4192, -4.4052, -5.2878, -5.3058, -5.3822, -4.8915, -4.7372, -5.0518, -5.0344, -4.9697, -4.9725, -4.9398, -5.2251, -5.2878, -5.2268, -5.2039, -5.2853, -5.284, -5.802, -5.9904, -6.0468, -6.0585, -6.0544, -6.3193, -6.3381, -6.3433, -6.4677, -6.4934, -6.5194, -6.5659, -6.6117, -6.633, -6.6342, -6.6541, -6.6964, -6.7095, -6.7256, -6.7691, -6.793, -6.7963, -6.7795, -6.8093, -6.8155, -6.8315, -6.8452, -6.8762, -6.8812, -6.882, -6.1006, -6.2458, -5.7525, -6.3525, -6.4265, -6.0071, -5.9136, -6.189, -5.8746, -5.4769, -5.9125, -6.0624, -5.5104, -5.7684, -6.1594, -5.8899, -6.1917, -6.3404, -5.6315, -5.6306, -5.5639, -4.9722, -5.9052, -6.0305, -5.7462, -5.6372, -5.6568, -5.9938, -5.5925, -5.3869, -5.8704, -5.7987, -5.5038, -5.222, -5.6662, -5.4285, -5.5581, -5.7816, -5.6663, -5.6557, -5.6253, -5.764, -5.7627, -5.7319, -5.7724, -5.7231, -5.7897, -5.2381, -5.6962, -5.7334, -5.8628, -5.9533, -5.9746, -5.7103, -6.0254, -6.2882, -6.3024, -6.4078, -6.4165, -6.4613, -6.4634, -6.4659, -6.5006, -6.5298, -6.5395, -6.5976, -6.6597, -6.6976, -6.6999, -6.7046, -6.7276, -6.7309, -6.7713, -6.777, -6.7809, -6.764, -6.8101, -5.0676, -5.5351, -5.6151, -5.3441, -5.8129, -5.8856, -6.191, -6.2153, -5.6971, -5.8282, -6.2098, -5.0717, -6.0517, -6.0888, -5.011, -5.9735, -4.9052, -5.9674, -5.6829, -5.2518, -5.5073, -5.7791, -5.7627, -5.4381, -4.9457, -5.6823, -5.6105, -5.1624, -5.461, -5.1428, -5.4216, -5.1757, -5.346, -5.0979, -5.4386, -5.2818, -5.4874, -5.4641, -5.316, -5.6436, -5.6575, -5.5955, -5.6165, -5.664, -3.9511, -4.0395, -4.1985, -4.357, -4.435, -4.6495, -4.7691, -4.8471, -4.895, -5.0341, -5.0718, -5.0726, -5.0997, -5.1114, -5.1606, -5.1643, -5.2591, -5.2727, -5.2935, -5.3246, -5.3471, -5.4521, -5.4838, -5.5232, -5.5648, -5.5837, -5.5861, -5.5877, -5.606, -5.6315, -5.5408, -4.9029, -4.9787, -4.4737, -5.5045, -5.3251, -4.6815, -4.7425, -5.4644, -4.3118, -4.9595, -5.0455, -5.0149, -5.146, -5.1595, -5.2464, -5.2567, -4.2752, -4.3185, -4.3849, -4.5837, -4.6532, -4.6792, -4.7113, -4.7182, -4.7969, -4.8255, -4.8425, -4.8759, -4.9373, -4.9612, -4.9685, -5.0344, -5.0553, -5.2041, -5.2107, -5.2582, -5.2826, -5.2992, -5.3714, -5.3794, -5.4018, -5.4019, -5.4663, -5.5026, -5.5581, -5.5612, -5.0609, -5.4785, -5.0121, -4.6149, -5.3185, -5.1493, -5.3198, -3.7074, -3.734, -4.4729, -4.5728, -4.6025, -4.6144, -4.7133, -4.753, -4.7763, -4.7839, -4.8234, -4.8589, -4.8818, -4.8859, -4.9277, -4.956, -4.9977, -4.9998, -5.0192, -5.0847, -5.1047, -5.1353, -5.1397, -5.1755, -5.1987, -5.2081, -5.2203, -5.2639, -5.7138, -5.7531, -4.8344, -4.913, -4.6782, -4.7806, -4.8271, -4.7674, -4.9949, -4.8525, -4.9239, -4.8508, -4.9397, -4.9039, -4.9423, -4.8328, -4.8697, -4.8755, -4.8752, -4.8901, -4.9017, -4.9088, -2.4599, -3.3448, -3.7531, -3.8292, -3.8463, -4.1664, -4.3057, -4.331, -4.3725, -4.3775, -4.3789, -4.383, -4.503, -4.6843, -4.6896, -4.7102, -4.8185, -4.8329, -4.8752, -4.8772, -4.9571, -5.0138, -5.0768, -5.115, -5.128, -5.1329, -5.1401, -5.1411, -5.1655, -5.1907, -4.7217, -3.3793, -4.0608, -3.8335, -4.7049, -4.8339, -5.0634, -2.1545, -3.4657, -4.0969, -4.1214, -4.3613, -4.3819, -4.427, -4.666, -4.6963, -4.6965, -4.7932, -4.9349, -4.944, -4.981, -4.994, -5.2106, -5.3068, -5.3662, -5.4044, -5.4901, -5.4954, -5.5159, -5.5541, -5.6319, -5.6361, -5.6411, -5.6419, -5.6639, -5.6835, -5.6975, -4.4243, -4.7482, -5.5302, -2.3111, -2.3694, -3.0456, -3.1385, -3.4465, -3.5645, -3.5807, -3.9406, -4.1315, -4.2491, -4.2834, -4.3255, -4.4128, -4.5238, -4.6265, -5.0675, -5.2886, -5.3341, -5.4179, -5.5267, -5.5356, -5.5608, -5.6335, -5.6789, -5.7342, -5.7454, -5.8409, -5.8839, -5.8892, -5.969, -4.1375, -3.3196, -3.5158, -3.6348, -3.9486, -4.0181, -4.0284, -4.0793, -4.2706, -4.5472, -4.551, -4.759, -4.8578, -4.9398, -5.0114, -5.0605, -5.1029, -5.3573, -5.4411, -5.4857, -5.5222, -5.6121, -5.6265, -5.6395, -5.6454, -5.6545, -5.9068, -5.9191, -6.0265, -6.0388, -6.4392, -5.3164, -5.5609], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.8574, 0.8572, 0.8572, 0.8571, 0.8571, 0.857, 0.857, 0.8569, 0.8568, 0.8568, 0.8567, 0.8566, 0.8566, 0.8566, 0.8566, 0.8566, 0.8565, 0.8565, 0.8564, 0.8564, 0.8563, 0.8563, 0.8563, 0.8562, 0.8562, 0.8562, 0.8562, 0.8562, 0.8561, 0.8561, 0.856, 0.8527, 0.8539, 0.8391, 0.8479, 0.8556, 0.8467, 0.8552, 0.84, 0.8248, 0.8252, 0.8006, 0.8285, 0.8174, 0.8303, 0.8074, 0.7856, 0.8086, 0.8263, 0.7458, 0.7389, 0.7973, 0.7909, 0.7924, 0.7167, 0.6266, 0.6969, 0.6859, 0.6154, 0.5717, 0.4722, 0.6436, 0.62, 0.4791, 0.366, 0.5764, 0.2842, 1.3362, 1.336, 1.336, 1.3359, 1.3359, 1.3356, 1.3356, 1.3356, 1.3354, 1.3353, 1.3353, 1.3352, 1.3351, 1.3351, 1.3351, 1.335, 1.335, 1.3349, 1.3349, 1.3348, 1.3348, 1.3348, 1.3347, 1.3347, 1.3347, 1.3347, 1.3346, 1.3346, 1.3346, 1.3346, 1.3332, 1.3337, 1.327, 1.3325, 1.3328, 1.3278, 1.3261, 1.3245, 1.3178, 1.2741, 1.2959, 1.3036, 1.261, 1.2767, 1.3002, 1.267, 1.2959, 1.3071, 1.2248, 1.1837, 1.1685, 0.9838, 1.1959, 1.2263, 1.1195, 1.0787, 1.0729, 1.1947, 1.0123, 0.8814, 1.1112, 1.0185, 0.6838, 0.3462, 0.765, 0.3204, 0.5057, 0.8399, 0.5372, 0.5009, 0.0806, 0.312, 0.2352, -0.0099, 0.2726, -0.3111, 0.3263, 1.534, 1.5337, 1.5336, 1.5335, 1.5334, 1.5334, 1.5333, 1.5333, 1.5329, 1.5329, 1.5327, 1.5327, 1.5326, 1.5326, 1.5326, 1.5325, 1.5325, 1.5324, 1.5323, 1.5322, 1.5321, 1.5321, 1.5321, 1.532, 1.532, 1.5319, 1.5319, 1.5319, 1.5318, 1.5318, 1.527, 1.5316, 1.5304, 1.5273, 1.5307, 1.531, 1.5312, 1.529, 1.5225, 1.5199, 1.524, 1.4965, 1.5191, 1.5201, 1.4846, 1.5107, 1.4538, 1.4919, 1.4634, 1.4083, 1.4337, 1.4662, 1.4555, 1.3992, 1.2221, 1.4011, 1.3724, 1.2043, 1.2703, 1.0162, 1.168, 0.9785, 0.9793, 0.4719, 0.942, 0.5896, 0.9006, 0.8438, 0.4128, 1.1847, 1.1635, 0.1936, 0.4601, 0.9554, 3.3006, 3.3005, 3.3003, 3.3001, 3.3, 3.2996, 3.2994, 3.2992, 3.2991, 3.2987, 3.2986, 3.2986, 3.2985, 3.2985, 3.2983, 3.2983, 3.298, 3.298, 3.2979, 3.2978, 3.2977, 3.2973, 3.2971, 3.2969, 3.2967, 3.2967, 3.2966, 3.2966, 3.2965, 3.2964, 3.2952, 3.264, 3.2515, 3.1675, 3.2814, 3.2334, 2.9609, 2.9688, 3.264, 2.4617, 2.6469, 2.5707, 2.3238, 2.3698, 2.2873, 2.8208, 2.0064, 3.7206, 3.7205, 3.7204, 3.7199, 3.7197, 3.7196, 3.7195, 3.7195, 3.7192, 3.7191, 3.719, 3.7189, 3.7187, 3.7186, 3.7186, 3.7183, 3.7182, 3.7175, 3.7174, 3.7172, 3.7171, 3.717, 3.7165, 3.7165, 3.7164, 3.7164, 3.7159, 3.7157, 3.7153, 3.7153, 3.6131, 3.6857, 3.222, 2.1101, 3.3456, 1.91, 3.0323, 3.9551, 3.955, 3.9534, 3.9531, 3.953, 3.953, 3.9526, 3.9524, 3.9523, 3.9523, 3.9521, 3.952, 3.9519, 3.9518, 3.9516, 3.9515, 3.9513, 3.9513, 3.9512, 3.9508, 3.9507, 3.9505, 3.9505, 3.9503, 3.9501, 3.9501, 3.95, 3.9497, 3.9459, 3.9455, 3.8644, 3.7657, 3.5408, 3.2818, 3.3013, 3.083, 3.496, 3.0712, 3.2821, 3.0416, 3.3139, 3.029, 3.2025, 1.7113, 2.1209, 2.1561, 1.4588, 1.5101, 1.5285, 1.7835, 4.8776, 4.8761, 4.8748, 4.8745, 4.8745, 4.8729, 4.872, 4.8718, 4.8715, 4.8715, 4.8715, 4.8715, 4.8706, 4.869, 4.8689, 4.8687, 4.8676, 4.8674, 4.8669, 4.8669, 4.8659, 4.8652, 4.8643, 4.8638, 4.8636, 4.8635, 4.8634, 4.8634, 4.863, 4.8626, 4.8606, 4.797, 4.6955, 4.3214, 3.85, 3.586, 3.03, 5.3373, 5.3339, 5.33, 5.3298, 5.3274, 5.3272, 5.3266, 5.3235, 5.323, 5.323, 5.3214, 5.3189, 5.3187, 5.318, 5.3177, 5.3127, 5.3102, 5.3085, 5.3073, 5.3046, 5.3044, 5.3037, 5.3024, 5.2995, 5.2994, 5.2992, 5.2991, 5.2983, 5.2975, 5.2969, 4.4891, 4.2978, 2.8937, 5.6294, 5.6293, 5.6273, 5.6269, 5.6253, 5.6245, 5.6244, 5.6214, 5.6193, 5.6178, 5.6173, 5.6167, 5.6154, 5.6135, 5.6116, 5.6008, 5.5934, 5.5917, 5.5883, 5.5835, 5.583, 5.5819, 5.5782, 5.5758, 5.5727, 5.5721, 5.5663, 5.5636, 5.5632, 5.5578, 4.6556, 6.4757, 6.4729, 6.471, 6.4645, 6.4628, 6.4625, 6.4612, 6.4555, 6.4451, 6.4449, 6.4351, 6.4297, 6.4248, 6.4202, 6.4169, 6.4139, 6.3933, 6.3853, 6.3809, 6.3771, 6.3672, 6.3656, 6.364, 6.3633, 6.3623, 6.3287, 6.3269, 6.31, 6.3081, 6.2299, 5.9722, 4.918]}, \"token.table\": {\"Topic\": [1, 2, 6, 4, 2, 3, 9, 6, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 6, 6, 3, 4, 4, 9, 8, 1, 2, 3, 8, 2, 3, 5, 2, 3, 5, 1, 2, 3, 6, 3, 9, 1, 2, 3, 9, 1, 2, 3, 6, 1, 1, 2, 3, 4, 1, 2, 3, 6, 5, 5, 7, 9, 7, 2, 3, 8, 6, 1, 2, 3, 7, 1, 2, 6, 1, 1, 2, 3, 1, 2, 3, 4, 6, 5, 7, 1, 2, 3, 1, 3, 5, 7, 3, 1, 2, 3, 1, 2, 3, 7, 2, 9, 3, 7, 6, 8, 1, 2, 1, 2, 3, 6, 8, 4, 1, 2, 6, 8, 4, 1, 2, 3, 5, 9, 2, 1, 2, 1, 2, 3, 4, 2, 9, 1, 2, 3, 1, 2, 3, 2, 4, 8, 5, 1, 2, 3, 5, 10, 10, 1, 3, 5, 2, 5, 8, 3, 2, 3, 9, 4, 8, 6, 5, 5, 8, 1, 2, 3, 1, 1, 2, 3, 1, 2, 4, 7, 1, 2, 3, 4, 1, 2, 3, 3, 1, 6, 1, 7, 1, 2, 3, 3, 5, 1, 10, 1, 4, 1, 7, 6, 8, 10, 2, 3, 6, 8, 6, 1, 2, 5, 4, 2, 7, 4, 5, 5, 3, 1, 2, 3, 2, 1, 3, 1, 2, 3, 8, 1, 2, 3, 10, 1, 1, 6, 5, 7, 4, 4, 6, 7, 2, 6, 7, 10, 10, 1, 2, 3, 4, 4, 6, 1, 2, 3, 1, 2, 3, 6, 3, 10, 8, 8, 1, 2, 3, 3, 2, 3, 3, 1, 2, 3, 9, 1, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 3, 6, 1, 2, 3, 6, 1, 2, 3, 5, 1, 2, 3, 1, 2, 3, 3, 3, 7, 1, 2, 3, 7, 1, 2, 3, 2, 4, 5, 1, 2, 3, 2, 4, 2, 1, 2, 4, 4, 2, 4, 2, 3, 4, 7, 4, 5, 10, 8, 1, 1, 4, 1, 7, 1, 6, 1, 2, 3, 6, 1, 2, 3, 5, 1, 3, 1, 2, 3, 6, 1, 2, 3, 4, 10, 10, 1, 2, 3, 1, 2, 3, 6, 10, 6, 1, 3, 6, 1, 3, 6, 1, 2, 3, 8, 9, 1, 2, 3, 4, 1, 2, 3, 6, 9, 10, 5, 1, 2, 3, 5, 1, 2, 3, 10, 10, 4, 1, 2, 3, 5, 6, 8, 8, 9, 7, 3, 3, 3, 1, 3, 4, 6, 1, 2, 3, 4, 6, 1, 3, 5, 6, 7, 10, 8, 10, 1, 2, 3, 4, 5, 6, 1, 2, 3, 6, 7, 5, 8, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 6, 1, 2, 3, 6, 9, 2, 1, 2, 3, 1, 3, 6, 2, 1, 2, 3, 1, 2, 3, 9, 1, 2, 3, 5, 1, 2, 3, 5, 3, 1, 3, 6, 8, 1, 2, 3, 4, 1, 2, 3, 6, 7, 1, 2, 4, 5, 9, 1, 2, 3, 9, 1, 1, 2, 4, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 8, 1, 6, 5, 7, 2, 3, 6, 1, 2, 3, 6, 4, 10, 2, 3, 2, 3, 4, 10, 1, 2, 3, 10, 1, 2, 3, 4, 5, 4, 2, 5, 6, 1, 4, 2, 2, 5, 6, 1, 4, 6, 10, 5, 7, 1, 2, 3, 3, 1, 2, 3, 4, 3, 1, 2, 3, 4, 4, 10, 9, 1, 2, 5, 6, 8, 3, 8, 2, 3, 1, 3, 6, 1, 2, 4, 2, 4, 4, 3, 2, 9, 3, 1, 2, 2, 3, 1, 2, 4, 6, 1, 2, 3, 1, 3, 1, 2, 3, 4, 6, 1, 2, 3, 1, 2, 3, 4, 10, 1, 3, 6, 1, 1, 1, 2, 3, 4, 1, 3, 7, 1, 2, 3, 5, 6, 2, 7, 3, 6, 1, 2, 3, 10, 10, 5, 5, 5, 3, 5, 1, 1, 2, 3, 6, 3, 8, 10, 1, 2, 3, 1, 5, 2, 3, 1, 2, 4, 5, 9, 8, 2, 3, 1, 2, 1, 3, 6, 2, 9, 8, 1, 3, 5, 1, 10, 4, 5, 10, 10, 2, 1, 2, 3, 3, 1, 3, 6, 7, 4, 5, 6, 2, 1, 2, 2, 2, 5, 1, 3, 6, 7, 1, 2, 3, 4, 1, 3, 4, 1, 4, 1, 2, 3, 9, 3, 1, 3, 6, 3, 5, 9, 8, 2, 1, 1, 2, 3, 6, 7, 1, 2, 3, 10, 2, 2, 6, 1, 2, 3, 6, 1, 2, 4, 5, 1, 2, 3, 9, 2, 4, 2, 10, 1, 2, 3, 6, 2, 3, 6, 5, 9, 1, 2, 3, 8, 1, 2, 4, 9, 4, 1, 3, 1, 9, 1, 1, 2, 3, 4, 9, 6, 1, 2, 3, 2, 8, 5, 1, 3, 7, 4, 6, 1, 2, 1, 2, 3, 1, 3, 8, 1, 5, 1, 2, 3, 2, 3, 5, 2, 1, 2, 3, 2, 2, 10, 9, 7, 7, 9, 6, 1, 2, 3, 1, 2, 3, 4, 4, 1, 3, 6, 1, 2, 3, 8, 1, 2, 3, 9, 1, 8, 7, 6, 6, 7, 7, 7], \"Freq\": [0.9523344861670684, 0.0032185544031181432, 0.04434452733184997, 0.9959277412488258, 0.5823982342783575, 0.03958046252377187, 0.37884156987038786, 0.9985123352374509, 0.788432422480376, 0.1616649580736694, 0.0031575187123763556, 0.0031575187123763556, 0.043258006359556074, 0.994253050535457, 0.7450500209531985, 0.19150048345085025, 0.0037713443130364377, 0.0037713443130364377, 0.05573208818153847, 0.9929307009279464, 0.997505228728032, 0.9973361862445977, 0.9975562772972776, 0.9753129085865216, 0.9650267121494144, 0.9732105057975703, 0.004956164612857997, 0.021626900128834895, 0.9961205316837209, 0.0014276001393353486, 0.9964648972560732, 0.0014276001393353486, 0.0019604019164495253, 0.9900029678070104, 0.007841607665798101, 0.01042866828390272, 0.03128600485170816, 0.957947672364207, 0.9967107998270693, 0.9973465618138196, 0.9281209560728148, 0.05796127659344427, 0.8671461576627054, 0.07387221526615446, 0.9937993688925183, 0.5019353065989564, 0.201054533425392, 0.26162326316023815, 0.03533175901199358, 0.999572145577396, 0.111777196137282, 0.8574796095286598, 0.01954146785616818, 0.010161563285207453, 0.10909663548867982, 0.7721774849523441, 0.1183060917312307, 0.9958596277576804, 0.9975461796854928, 0.9968095168915082, 0.9882583947868536, 0.9835053826361018, 0.9944348618427672, 0.9963579935459783, 0.0029048338004255925, 0.9580009849234473, 0.9956535906106168, 0.31150044633273766, 0.2803504016994639, 0.1284939341122543, 0.2764566461203047, 0.17744725477261747, 0.29962405314064916, 0.5207058787589922, 0.9990100268538282, 0.462394693212461, 0.5204550928118576, 0.017125729378239297, 0.6500737014101309, 0.2632682819233911, 0.012029833620400998, 0.0013880577254308845, 0.07310437353935992, 0.9955483321234866, 0.9968745344527714, 0.037423368965594415, 0.9392531818815852, 0.02348132954703963, 0.4911330103582761, 0.3088347314043308, 0.19945576403196366, 0.9952390181898579, 0.9983586625626464, 0.9985545865294608, 0.0008039892001042358, 0.0008039892001042358, 0.9713365434039544, 0.015837008859847082, 0.012537632014045607, 0.998803460571923, 0.9969645506292591, 0.9176390389396037, 0.9969725935406093, 0.9946620325264084, 0.9924641704865117, 0.9625102647644249, 0.9985494752389984, 0.9975322103057139, 0.3397214095441295, 0.17225310906462904, 0.0909113631174431, 0.39474670827310826, 0.9811420767768054, 0.9986422127006881, 0.9367594171851618, 0.002061530407537768, 0.060608993981610376, 0.9914055573769163, 0.9990275874555827, 0.25612348392189394, 0.16886374807468715, 0.5732479868851222, 0.0012119407756556494, 0.9980460959239645, 0.9987244183339916, 0.9988218261420698, 0.000679470630028619, 0.005262508531750767, 0.9906672311020818, 0.003946881398813075, 0.9987248475320222, 0.9984221086196416, 0.9266565227798963, 0.3045721046577536, 0.004722048134228738, 0.6902060356197672, 0.02872537030292459, 0.008556493281722219, 0.9626054941937496, 0.06260984937856784, 0.9346756085800485, 0.9936810786429909, 0.9977198913370202, 0.0021788239493562294, 0.03268235924034344, 0.9314472383497882, 0.033771771215021555, 0.8455098261183324, 0.8839196410107637, 0.08340236022216749, 0.9041170751743476, 0.012421628118195159, 0.032783584837036056, 0.9671157526925637, 0.9709220359249734, 0.9963300233107175, 0.9952343235345483, 0.003734462752474853, 0.9872474943638714, 0.9943898355175321, 0.983166088889762, 0.9953985646290306, 0.994525832547105, 0.5676213598702985, 0.42731046192483146, 0.003765288896665221, 0.003765288896665221, 0.9927811724207299, 0.9989200113504526, 0.6370681452878969, 0.3447918557783274, 0.0177466396356492, 0.998442499018384, 0.0007445507076945444, 0.9979957369506474, 0.9942038817619315, 0.061345527120290975, 0.0036085604188406456, 0.9334142950067803, 0.9945943342742941, 0.27053703802330387, 0.1763050360151868, 0.5527264203809448, 0.9994937722369923, 0.5855211044968653, 0.4124723570139861, 0.9992015895254939, 0.9916506518407043, 0.9827044130980499, 0.005065486665453866, 0.011972968481981863, 0.9997023850052753, 0.9959056989212798, 0.7629125752591953, 0.2136155210725747, 0.9990667249007282, 0.9972793956025621, 0.9997784709915227, 0.9934922594967687, 0.9953298413203491, 0.9596051915851463, 0.947761763423267, 0.9987432616823435, 0.49017895945384116, 0.5092414745437128, 0.9892435185549338, 0.9937613740874212, 0.001680117055029696, 0.9963094136326096, 0.9940248120068699, 0.9957055176052977, 0.9987023743189506, 0.9925913371252392, 0.9983386387924906, 0.9941861035554825, 0.9963541906380091, 0.9988396520138814, 0.10489455750938795, 0.20167227425912088, 0.6930533264013132, 0.9991896326169868, 0.07492247629211872, 0.9236536530387761, 0.9987368950416995, 0.0010414357612530756, 0.9992164404494366, 0.9746788039144147, 0.21998579142192712, 0.2788684979053123, 0.5012095975865749, 0.8181900645646191, 0.9992603416777174, 0.9990013700897149, 0.9970956564372492, 0.012450743045374482, 0.9836087005845842, 0.9967684756745778, 0.9967237245883546, 0.6372552572145522, 0.35650643760254674, 0.999249758626573, 0.9935846369127588, 0.9971157452007582, 0.9619535904771723, 0.8522852774554912, 0.4278811061680178, 0.01584744837659325, 0.1796044149347235, 0.3763768989440897, 0.9981359920073487, 0.992332763047187, 0.9356063043885763, 0.056830550083121836, 0.00724684866831755, 0.5142578329833414, 0.26488454506334524, 0.11514125494870638, 0.10619245275062039, 0.99696855588475, 0.9784765472074869, 0.9988750015714982, 0.9629569286133484, 0.12340025189669154, 0.1573727908803961, 0.7189188764345714, 0.9980721075818486, 0.8934467311426153, 0.10515933240638448, 0.9983472324812808, 0.8073820833142615, 0.015182062491806347, 0.17732648990429814, 0.9589564149108197, 0.9988176784606801, 0.7513315198259772, 0.1986430106463656, 0.04960587890174434, 0.00021949503938824928, 0.9692805747844124, 0.028949751680507892, 0.001072213025203996, 0.6315837384577513, 0.3323761249433024, 0.03593255404792458, 0.17147123901456576, 0.8270965646584937, 0.02533562929207094, 0.9642442442335233, 0.008941986808966212, 0.999232291256683, 0.4200501122693276, 0.01929786120686038, 0.5602812370391796, 0.9967911035823772, 0.4639275276545964, 0.0051037131755181115, 0.5302757989363318, 0.009378799349346326, 0.0023446998373365816, 0.9882909814373692, 0.9989467506337258, 0.9984319376931841, 0.9919523589600139, 0.6116400313110555, 0.04278777833177487, 0.3454550103207508, 0.9819811734762742, 0.006109554013025057, 0.01680127353581891, 0.976001253580753, 0.04473215620267973, 0.26342269763800286, 0.6858930617744224, 0.07673922307639072, 0.56435303637429, 0.35864928562785386, 0.9877667907231872, 0.011010139387041895, 0.997527785999114, 0.9891426456302526, 0.0023100015077773296, 0.008316005427998386, 0.9963120267949855, 0.9984079571171011, 0.9916504627798268, 0.9598542873534816, 0.033618084943539815, 0.005796221541989623, 0.9901990195484294, 0.9647180691207744, 0.031803892388596953, 0.9688579302957147, 0.9752222811414331, 0.999233348916455, 0.12331901223269318, 0.8739564779969126, 0.9997008899645162, 0.990936887790364, 0.9990159187810096, 0.0009133969711197335, 0.6883547085884356, 0.0035810360562404165, 0.24868305946114003, 0.05928604137553578, 0.015828129362097612, 0.21511866723941755, 0.7676642740617342, 0.0007194604255498915, 0.013331526507332924, 0.985199808891903, 0.8880918516258772, 0.07724456959966626, 0.018097299163350383, 0.016478841514595468, 0.397768459670873, 0.36394064345252897, 0.23718353900218234, 0.001166476421322208, 0.9736937739304173, 0.8556960879019467, 0.563627227170147, 0.3711801164969736, 0.06497338199129338, 0.9816597052507506, 0.0007409870963547332, 0.0013337767734385199, 0.016153518700533186, 0.9359748324310623, 0.9927245696674898, 0.9303460313815144, 0.003401630827720345, 0.06590659728708169, 0.9951111580033675, 0.0046609421920532435, 0.9926603099235967, 0.002010997209424117, 0.9954436186649379, 0.9979343226483042, 0.9635544293672836, 0.9257466162595773, 0.10298737721419193, 0.3148471246262439, 0.2177447403957201, 0.3633983167415058, 0.5933269002141133, 0.15135396884027502, 0.17263056509897184, 0.0822050309995104, 0.954250600781059, 0.9111231680526679, 0.9946342786781794, 0.04526846357936711, 0.006466923368481016, 0.33951347684525335, 0.604657334952975, 0.002038003334713917, 0.9904696206709638, 0.008152013338855669, 0.987621761180842, 0.9768326485016451, 0.9971927264710347, 0.9511315740144458, 0.01713513090657954, 0.03163408782753146, 0.6116602223113448, 0.029126677252921177, 0.3568017963482844, 0.9793462906026398, 0.95724186093358, 0.9874504755514687, 0.9988302352860425, 0.9988774286330723, 0.9964552914600644, 0.013843335057690132, 0.9863376228604219, 0.9983435067537205, 0.9958287473652205, 0.034561039173356914, 0.8047784836081682, 0.02139492901207809, 0.030446629747957282, 0.10944329071563022, 0.0483070721489894, 0.9513651620871559, 0.9986927708775192, 0.16351617516475986, 0.8339324933402753, 0.8410937756568607, 0.9890055026157613, 0.7823101371631013, 0.03839936978125216, 0.9412768592532581, 0.0059075953509618705, 0.013784389152244366, 0.9937930977251849, 0.9951848961436524, 0.6946479710763098, 0.09824702379764542, 0.09340386065269107, 0.11416027413106686, 0.9956989618467135, 0.9959032017698236, 0.9810037841034165, 0.9445964833744255, 0.015154577667367062, 0.0402179176557049, 0.0647639785572581, 0.08419317212443554, 0.8502585184874314, 0.9957540685150413, 0.36840865172559234, 0.2912131972455188, 0.2518168963384468, 0.08837548581856694, 0.8514927404711514, 0.019119539713299098, 0.09478983069128567, 0.03419974005054909, 0.9881942266664311, 0.9992128875659728, 0.8423621850753225, 0.030888358557765085, 0.12669462323693476, 0.9681225860367207, 0.00026751107655062744, 0.03156630703297404, 0.997586755533241, 0.007198715212853666, 0.11837887238914917, 0.8734441124929114, 0.6802409531455774, 0.19232144561650477, 0.12731679699812615, 0.9928898379193591, 0.10586279519424628, 0.012060318439850842, 0.8810732638002143, 0.0006700176911028246, 0.3644521758720579, 0.2908141464907293, 0.3416505525610881, 0.0029903768276681674, 0.9979956715174275, 0.5168858965091192, 0.08040447279030742, 0.39972509330038547, 0.9689012232183292, 0.12138346212614154, 0.767804266348197, 0.10989153671774944, 0.0007182453380245061, 0.34299065364745757, 0.061291820947652176, 0.5954642472199055, 0.9938966786706964, 0.9966190454054557, 0.26012252270182096, 0.34525353013150784, 0.39412503439669844, 0.9981022865036453, 0.9913799491315408, 0.051747971036850554, 0.9325644993236685, 0.015414289245019313, 0.9482647406151747, 0.9996780890803133, 0.8306456362525645, 0.019253375674728318, 0.06271099505482938, 0.0874653352080515, 0.9993787822110598, 0.9271263724620086, 0.07264237328660325, 0.994454776942999, 0.7850377150283053, 0.08117925687009857, 0.10541869694962096, 0.02858424537679527, 0.9995717805697713, 0.9911066725319144, 0.9979017259638783, 0.997964122593978, 0.9981940877520364, 0.9769716568721063, 0.17742541608360884, 0.3518937418991575, 0.4701773526215634, 0.5888222466390919, 0.3617512704649133, 0.01589227472630669, 0.033400712984102195, 0.9983036400136043, 0.8932201442078658, 0.013325355234844185, 0.9844106179741141, 0.9677049567459797, 0.027144599067208404, 0.005428919813441681, 0.8314494766837977, 0.7936781536037241, 0.06633837257089543, 0.1398236968828542, 0.820935405958328, 0.12071872590457525, 0.00306912015011632, 0.8746992427831513, 0.0010230400500387733, 0.9940615453100927, 0.9934368089953589, 0.1910806786948834, 0.1464951869994106, 0.6592283414973478, 0.38036080987092613, 0.6184283671282684, 0.9980303821068643, 0.9976511360012177, 0.9929922453731274, 0.9902585163047304, 0.045088663307956835, 0.9500825482748048, 0.9923830186740619, 0.9394069049541867, 0.9987503415850951, 0.998019720142245, 0.9984195204392087, 0.001037750255107794, 0.0004151001020431176, 0.9981577772081853, 0.6156601511906271, 0.35863698127609345, 0.017931849063804673, 0.007845183965414544, 0.9978603226368473, 0.03486512073463538, 0.6339112860842796, 0.33008666253959984, 0.0004527937757744854, 0.9930844378309154, 0.9715048760378809, 0.9903378577131996, 0.9991406502084218, 0.9970997807284536, 0.9974809479221666, 0.9945486288251809, 0.9900826560617455, 0.9983584945897457, 0.9681528160057807, 0.0019378439132027281, 0.9960517713862023, 0.8958574915143757, 0.017049191014979467, 0.08679588153080457, 0.24538720102322661, 0.7275377717238157, 0.02665547870539842, 0.9983006515267835, 0.9958986671146829, 0.9965262288466884, 0.9990488016588878, 0.9996193458256545, 0.9759824005448814, 0.9991319770266306, 0.9974191601102452, 0.002177771091943767, 0.9892342143912934, 0.009569375713579622, 0.21063329647544768, 0.17919549103135102, 0.10060097742110935, 0.509292448194366, 0.37129540968690744, 0.6085119214313205, 0.019982912674121753, 0.9986060826021421, 0.0011120334995569513, 0.1615419835428229, 0.11538713110201634, 0.17472908424019618, 0.02307742622040327, 0.5274840278949319, 0.9306559263036018, 0.020681242806746707, 0.04869492624497634, 0.106587519060775, 0.8447781071506019, 0.022325764127594763, 0.02592669382559392, 0.8841195272558815, 0.9678102886051817, 0.000765067421822278, 0.0313677642947134, 0.999038486935846, 0.9998249336283523, 0.02265914721701065, 0.4444678877182858, 0.0505473284071776, 0.48107112553037995, 0.003962271648077826, 0.9945301836675342, 0.9902317345981398, 0.06508116044928297, 0.45848220495614267, 0.3040358689645607, 0.006799524226044489, 0.16513130263250902, 0.9983900811517368, 0.9864242821518103, 0.3678485358879247, 0.631194646807689, 0.5292081107562613, 0.44942372838081857, 0.021219250631766693, 0.9859868305629514, 0.9883492382954541, 0.9970607206373859, 0.9967033403912678, 0.9967620584141864, 0.996788410173664, 0.9987355819989162, 0.998969721579126, 0.48568389039867, 0.410602322800808, 0.030971146634118086, 0.07273526861042884, 0.9993716924348435, 0.9674876638027363, 0.9590996232848205, 0.7550273614221599, 0.1733788869080439, 0.07146225425601115, 0.4947878969456742, 0.5020641895478165, 0.0066208308525331225, 0.9931246278799684, 0.27217937419455274, 0.009583780781498335, 0.7168668024560755, 0.9965961372339723, 0.9916988795584506, 0.9868461111304041, 0.9812072652356859, 0.018255018888105785, 0.02877305951837069, 0.9710907587450108, 0.09417245472422944, 0.7449880329174191, 0.1594404926519132, 0.9989135646740571, 0.9526127077514724, 0.9831454809412159, 0.001006089168793015, 0.9970343662738781, 0.00201217833758603, 0.9991708654281496, 0.9724299611770103, 0.9934846545721061, 0.9958607667329007, 0.9495027010760687, 0.9907060162880686, 0.99713692952239, 0.11458517602658985, 0.7974758621850568, 0.08686295602015683, 0.9970380070800573, 0.24893012293383962, 0.33264113772575027, 0.4185550739595533, 0.9985704103691786, 0.9958997338328185, 0.08233766537930508, 0.9108604232585624, 0.9980812548416186, 0.039849527095185966, 0.9594539985225544, 0.9979621170790428, 0.10038884693554147, 0.8984801800730962, 0.7668824554919904, 0.04213639865340607, 0.03370911892272486, 0.15730922163938266, 0.4259940957639917, 0.4332829681249725, 0.1401083242721874, 0.0004049373533878248, 0.5545314500365652, 0.012756474493398659, 0.4314689902178958, 0.9980796034957948, 0.9959102589759429, 0.9607060050190521, 0.025181978147342687, 0.014080245845826019, 0.9813872245873179, 0.9969211397757137, 0.20761657917156306, 0.7317767790524323, 0.06060602311122551, 0.9996326422274371, 0.9950720588448668, 0.9990682776973682, 0.9687117993563277, 0.9980535210065111, 0.9997221661826098, 0.13037420749985, 0.2957385515713509, 0.5737423763872075, 0.42418029534235663, 0.5735395542657217, 0.36534922397789416, 0.4354667518120355, 0.19891235527685347, 0.7904486888187301, 0.9971489100471645, 0.998927019727173, 0.9934015636046881, 0.07080938762696666, 0.8684563129542675, 0.060396242387706855, 0.9895462934108283, 0.12703741598831253, 0.161033344210537, 0.7121252332865969, 0.9949018222290197, 0.5409295208326126, 0.26010765131650593, 0.19875161259870108, 0.9646349246972644, 0.0332542027408205, 0.9643718794837943, 0.9973792566033415, 0.8708124782408506, 0.894259528058936, 0.04356379357249982, 0.04446511343951706, 0.01772595738467234, 0.9985030221610329, 0.9978001245605208, 0.9954439694985772, 0.9903026063029542, 0.9978535368708213, 0.15467839998194244, 0.722677770407436, 0.12234807867424136, 0.9829525628528701, 0.011228449836842342, 0.005614224918421171, 0.9824893607237049, 0.9590398135095194, 0.9982008294239068, 0.0032657929622951875, 0.9960668535000322, 0.9996953426429419, 0.9778543351639134, 0.9996180533611586, 0.33794904259847863, 0.380804899449735, 0.006122265264465193, 0.2742774838480406, 0.9928291768115659, 0.9933438621355642, 0.24287601125029937, 0.7024517651441128, 0.0546719534192352, 0.9967577391399969, 0.9401532573720999, 0.9948323374963167, 0.0026543820573693678, 0.9967204625421977, 0.9933086790484, 0.9955651184286797, 0.9971812481024953, 0.9963040458820626, 0.0033602160063475974, 0.684789450665159, 0.28459787644448764, 0.030188532678155085, 0.2925140297361199, 0.7047289225891886, 0.0023781628433830887, 0.8359058667682178, 0.16278692506955733, 0.45011585043277824, 0.2241339337345623, 0.32577299673551224, 0.8955236911138796, 0.10342325237048881, 0.0012167441455351626, 0.9977835023997192, 0.5446806087204045, 0.06667694427600256, 0.38879661114363134, 0.9981934215510421, 0.3429120653811431, 0.6172417176860576, 0.9824128491104751, 0.9809264988574413, 0.9803094586413341, 0.9955802236196085, 0.9942854699997333, 0.07535842812668302, 0.001983116529649553, 0.9221491862870421, 0.14498307434957566, 0.3193079613651369, 0.017259889803520913, 0.5195226830859795, 0.9968918729046885, 0.941371623706806, 0.0011310031522308442, 0.05730415971302944, 0.868721419653894, 0.06412437203264197, 0.06693273869100584, 0.9833539129632496, 0.9902549289556976, 0.007566417795267985, 0.0014187033366127471, 0.9694346789034203, 0.9108109500730046, 0.08599931717427511, 0.9794742037064623, 0.9946145754675674, 0.07628872748530562, 0.9215678280224919, 0.9916392066777628, 0.9847415348646749], \"Term\": [\"10\", \"10\", \"10\", \"1000\", \"11\", \"11\", \"11\", \"15\", \"2\", \"2\", \"2\", \"2\", \"2\", \"22\", \"3\", \"3\", \"3\", \"3\", \"3\", \"32\", \"4x\", \"70\", \"80\", \"8th\", \"_\", \"access\", \"access\", \"access\", \"account\", \"act\", \"act\", \"act\", \"action\", \"action\", \"action\", \"addit\", \"addit\", \"addit\", \"adult\", \"aesthet\", \"allot\", \"allow\", \"allow\", \"allow\", \"alright\", \"also\", \"also\", \"also\", \"also\", \"amaz\", \"anoth\", \"anoth\", \"anoth\", \"anoth\", \"anyth\", \"anyth\", \"anyth\", \"ark\", \"armi\", \"artilleri\", \"assault\", \"assist\", \"asteroid\", \"attack\", \"attack\", \"au\", \"audienc\", \"autom\", \"autom\", \"autom\", \"autom\", \"averag\", \"averag\", \"averag\", \"awesom\", \"back\", \"back\", \"back\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"ball\", \"ban\", \"basic\", \"basic\", \"basic\", \"battl\", \"battl\", \"battl\", \"battleship\", \"behold\", \"best\", \"best\", \"best\", \"better\", \"better\", \"better\", \"biggest\", \"block\", \"blueprint\", \"board\", \"boat\", \"boi\", \"bom\", \"bought\", \"box\", \"brain\", \"brain\", \"brain\", \"brain\", \"brick\", \"broken\", \"bug\", \"bug\", \"bug\", \"bugfix\", \"buggi\", \"build\", \"build\", \"build\", \"build\", \"builder\", \"button\", \"buy\", \"buy\", \"call\", \"call\", \"call\", \"can\", \"cant\", \"chair\", \"challeng\", \"challeng\", \"challeng\", \"charact\", \"charact\", \"charact\", \"cheat\", \"cheat\", \"children\", \"chill\", \"choic\", \"choic\", \"choic\", \"choic\", \"christma\", \"church\", \"citi\", \"citi\", \"citi\", \"citizen\", \"citizen\", \"civ\", \"classic\", \"click\", \"click\", \"closest\", \"code\", \"codemast\", \"coffe\", \"cold\", \"com\", \"com\", \"combat\", \"combat\", \"combat\", \"combin\", \"come\", \"come\", \"come\", \"commun\", \"commun\", \"compani\", \"companion\", \"compar\", \"compar\", \"compar\", \"competit\", \"complet\", \"complet\", \"complet\", \"complex\", \"comput\", \"comput\", \"concept\", \"conflict\", \"content\", \"content\", \"content\", \"cool\", \"copi\", \"coromon\", \"coromon\", \"cost\", \"countri\", \"crash\", \"cruis\", \"cup\", \"dad\", \"daddi\", \"damag\", \"dark\", \"dark\", \"de\", \"deaf\", \"deal\", \"deal\", \"default\", \"defend\", \"defens\", \"defin\", \"delet\", \"demand\", \"deploy\", \"depth\", \"design\", \"design\", \"design\", \"destruct\", \"detail\", \"detail\", \"dev\", \"dev\", \"dialogu\", \"dice\", \"differ\", \"differ\", \"differ\", \"disservic\", \"divis\", \"dlc\", \"do\", \"doesn\", \"doesn\", \"doesnt\", \"dog\", \"don\", \"don\", \"dont\", \"dri\", \"drift\", \"drove\", \"drunkenli\", \"due\", \"due\", \"due\", \"due\", \"ea\", \"eargasm\", \"earli\", \"earli\", \"earli\", \"easi\", \"easi\", \"easi\", \"easi\", \"editor\", \"elden\", \"empir\", \"en\", \"end\", \"end\", \"end\", \"endless\", \"enemi\", \"enemi\", \"engag\", \"enjoy\", \"enjoy\", \"enjoy\", \"enthusiast\", \"eugen\", \"even\", \"even\", \"even\", \"even\", \"ever\", \"ever\", \"ever\", \"everi\", \"everi\", \"everi\", \"evolv\", \"evolv\", \"except\", \"except\", \"except\", \"expans\", \"expect\", \"expect\", \"expect\", \"expedit\", \"experi\", \"experi\", \"experi\", \"explor\", \"explor\", \"explor\", \"fail\", \"faster\", \"fate\", \"feel\", \"feel\", \"feel\", \"fellow\", \"felt\", \"felt\", \"felt\", \"field\", \"field\", \"field\", \"find\", \"find\", \"find\", \"fire\", \"fire\", \"fish\", \"fix\", \"fix\", \"fix\", \"flesh\", \"fli\", \"footbal\", \"forc\", \"forc\", \"forc\", \"format\", \"four\", \"four\", \"fox\", \"franc\", \"friend\", \"fuck\", \"fuck\", \"fun\", \"galaxi\", \"game\", \"game\", \"gameplay\", \"gameplay\", \"gameplay\", \"gameplay\", \"gener\", \"gener\", \"gener\", \"gener\", \"genr\", \"genr\", \"get\", \"get\", \"get\", \"get\", \"give\", \"give\", \"give\", \"give\", \"gloriou\", \"glu\", \"go\", \"go\", \"go\", \"good\", \"good\", \"good\", \"good\", \"goti\", \"grandma\", \"graphic\", \"graphic\", \"graphic\", \"great\", \"great\", \"greatli\", \"group\", \"group\", \"grow\", \"ha\", \"hail\", \"half\", \"half\", \"half\", \"half\", \"hard\", \"hard\", \"hard\", \"hard\", \"heap\", \"hi\", \"histor\", \"histori\", \"histori\", \"histori\", \"histori\", \"hit\", \"hit\", \"hit\", \"holi\", \"honor\", \"horribl\", \"hour\", \"hour\", \"hour\", \"http\", \"http\", \"http\", \"ici\", \"ign\", \"iii\", \"im\", \"imagin\", \"immedi\", \"immers\", \"immers\", \"imo\", \"infin\", \"instead\", \"instead\", \"instead\", \"instead\", \"instead\", \"interest\", \"interest\", \"intuit\", \"it\", \"it\", \"johnni\", \"jungl\", \"kawaii\", \"kill\", \"kill\", \"kill\", \"kill\", \"leader\", \"leaderboard\", \"learn\", \"learn\", \"learn\", \"learn\", \"legendari\", \"legion\", \"lego\", \"like\", \"like\", \"like\", \"line\", \"line\", \"line\", \"log\", \"long\", \"long\", \"long\", \"long\", \"look\", \"look\", \"look\", \"look\", \"lord\", \"lose\", \"lot\", \"lot\", \"lot\", \"love\", \"love\", \"love\", \"luck\", \"main\", \"main\", \"main\", \"make\", \"make\", \"make\", \"maker\", \"manag\", \"manag\", \"manag\", \"manag\", \"map\", \"map\", \"map\", \"map\", \"market\", \"master\", \"master\", \"master\", \"mayor\", \"mean\", \"mean\", \"mean\", \"mean\", \"mechan\", \"mechan\", \"mechan\", \"mehh\", \"men\", \"mess\", \"mess\", \"mess\", \"militari\", \"min\", \"minut\", \"minut\", \"minut\", \"miser\", \"mod\", \"money\", \"money\", \"money\", \"money\", \"month\", \"move\", \"move\", \"ms\", \"much\", \"much\", \"much\", \"much\", \"multiplay\", \"n\", \"na\", \"nasa\", \"nation\", \"navi\", \"necessari\", \"necessari\", \"necessari\", \"need\", \"need\", \"need\", \"need\", \"not\", \"nuzlock\", \"offer\", \"offer\", \"often\", \"often\", \"often\", \"oldschool\", \"one\", \"one\", \"one\", \"ooh\", \"open\", \"open\", \"open\", \"open\", \"oper\", \"outcom\", \"paint\", \"paint\", \"paint\", \"parti\", \"parti\", \"pass\", \"path\", \"peac\", \"penni\", \"per\", \"per\", \"perspect\", \"pizza\", \"plane\", \"planet\", \"play\", \"play\", \"play\", \"playabl\", \"player\", \"player\", \"player\", \"player\", \"plot\", \"point\", \"point\", \"point\", \"point\", \"pointless\", \"pokemon\", \"polici\", \"polish\", \"poor\", \"popul\", \"potato\", \"pour\", \"prefer\", \"premier\", \"present\", \"present\", \"price\", \"price\", \"price\", \"problem\", \"problem\", \"problem\", \"promis\", \"proper\", \"public\", \"push\", \"puzzl\", \"pve\", \"quest\", \"race\", \"race\", \"random\", \"random\", \"rank\", \"rank\", \"rank\", \"rank\", \"real\", \"real\", \"real\", \"realist\", \"realist\", \"realiti\", \"realiti\", \"realiti\", \"realiti\", \"realiti\", \"realli\", \"realli\", \"realli\", \"reason\", \"reason\", \"reason\", \"reason\", \"reb\", \"recommend\", \"recommend\", \"recommend\", \"relax\", \"releas\", \"remov\", \"remov\", \"remov\", \"remov\", \"repetit\", \"repetit\", \"republ\", \"requir\", \"requir\", \"requir\", \"requir\", \"requir\", \"research\", \"revisit\", \"rich\", \"rich\", \"right\", \"right\", \"right\", \"ring\", \"rn\", \"role\", \"roman\", \"rome\", \"rough\", \"rpg\", \"rt\", \"run\", \"run\", \"run\", \"run\", \"screen\", \"se\", \"seat\", \"see\", \"see\", \"see\", \"send\", \"send\", \"seri\", \"seri\", \"server\", \"server\", \"server\", \"settlement\", \"sex\", \"shader\", \"ship\", \"ship\", \"shoot\", \"shoot\", \"short\", \"short\", \"short\", \"shot\", \"shower\", \"si\", \"side\", \"side\", \"side\", \"sim\", \"simultan\", \"singleplay\", \"skirmish\", \"slack\", \"sold\", \"solv\", \"sometim\", \"sometim\", \"sometim\", \"somewhat\", \"soul\", \"soul\", \"soul\", \"space\", \"spam\", \"spare\", \"spare\", \"specif\", \"speed\", \"speed\", \"spot\", \"squad\", \"squad\", \"star\", \"star\", \"star\", \"star\", \"start\", \"start\", \"start\", \"start\", \"state\", \"state\", \"state\", \"steel\", \"steep\", \"still\", \"still\", \"still\", \"stone\", \"store\", \"stori\", \"stori\", \"stori\", \"strategi\", \"strength\", \"stuck\", \"su\", \"success\", \"support\", \"system\", \"system\", \"system\", \"t\", \"t\", \"take\", \"take\", \"take\", \"tamer\", \"tank\", \"target\", \"teen\", \"tell\", \"tell\", \"tell\", \"terrarium\", \"terribl\", \"terribl\", \"terribl\", \"territori\", \"thing\", \"thing\", \"thing\", \"thingi\", \"third\", \"third\", \"throw\", \"throwback\", \"time\", \"time\", \"time\", \"time\", \"ton\", \"trade\", \"trail\", \"trait\", \"truck\", \"two\", \"two\", \"two\", \"un\", \"unabl\", \"unabl\", \"unabl\", \"unbear\", \"unfinish\", \"uniqu\", \"uniqu\", \"unit\", \"unstuck\", \"updat\", \"us\", \"us\", \"us\", \"us\", \"usa\", \"usag\", \"use\", \"use\", \"use\", \"useless\", \"vi\", \"victori\", \"voic\", \"voic\", \"vr\", \"vs\", \"w\", \"wait\", \"wait\", \"want\", \"want\", \"want\", \"war\", \"war\", \"war\", \"wargam\", \"wargam\", \"way\", \"way\", \"way\", \"weapon\", \"weapon\", \"weapon\", \"week\", \"well\", \"well\", \"well\", \"wheel\", \"wholesom\", \"wholesom\", \"wiki\", \"women\", \"won\", \"wont\", \"workshop\", \"world\", \"world\", \"world\", \"wors\", \"wors\", \"wors\", \"wors\", \"worst\", \"worth\", \"worth\", \"worth\", \"would\", \"would\", \"would\", \"www\", \"year\", \"year\", \"year\", \"yellow\", \"youtub\", \"youtub\", \"\\u2013\", \"\\u2018\", \"\\u2019\", \"\\u2019\", \"\\u201c\", \"\\u201d\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [8, 1, 6, 5, 4, 3, 10, 9, 7, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1535228673056927369685498979\", ldavis_el1535228673056927369685498979_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1535228673056927369685498979\", ldavis_el1535228673056927369685498979_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1535228673056927369685498979\", ldavis_el1535228673056927369685498979_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "7      0.149307 -0.343485       1        1  42.418947\n",
       "0     -0.035789 -0.393190       2        1  26.262320\n",
       "5     -0.226278 -0.342000       3        1  21.554594\n",
       "4     -0.230106  0.415553       4        1   3.682688\n",
       "3      0.399451 -0.114587       5        1   2.417011\n",
       "2      0.331365  0.124599       6        1   1.913004\n",
       "9      0.145016  0.347960       7        1   0.760712\n",
       "8     -0.316887  0.034672       8        1   0.480311\n",
       "6     -0.190559  0.143403       9        1   0.358359\n",
       "1     -0.025520  0.127074      10        1   0.152054, topic_info=            Term          Freq         Total Category  logprob  loglift\n",
       "57          game  20801.000000  20801.000000  Default  30.0000  30.0000\n",
       "144         play   9636.000000   9636.000000  Default  29.0000  29.0000\n",
       "60          good   6747.000000   6747.000000  Default  28.0000  28.0000\n",
       "100    recommend   3921.000000   3921.000000  Default  27.0000  27.0000\n",
       "78          love   3738.000000   3738.000000  Default  26.0000  26.0000\n",
       "...          ...           ...           ...      ...      ...      ...\n",
       "8551      kawaii      4.276535      5.113062  Topic10  -6.0265   6.3100\n",
       "16186      tamer      4.224351      5.060417  Topic10  -6.0388   6.3081\n",
       "8548   disservic      2.830607      3.666630  Topic10  -6.4392   6.2299\n",
       "2725    wholesom      8.699596     14.580998  Topic10  -5.3164   5.9722\n",
       "16151    coromon      6.812625     32.769154  Topic10  -5.5609   4.9180\n",
       "\n",
       "[515 rows x 6 columns], token_table=      Topic      Freq  Term\n",
       "term                       \n",
       "1         1  0.952334    10\n",
       "1         2  0.003219    10\n",
       "1         6  0.044345    10\n",
       "2956      4  0.995928  1000\n",
       "1253      2  0.582398    11\n",
       "...     ...       ...   ...\n",
       "130       6  0.994615     \n",
       "1351      6  0.076289     \n",
       "1351      7  0.921568     \n",
       "1352      7  0.991639     \n",
       "1353      7  0.984742     \n",
       "\n",
       "[867 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[8, 1, 6, 5, 4, 3, 10, 9, 7, 2])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizing the Topic Models\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=30)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "88163e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"use\" + 0.005*\"go\" + 0.005*\"point\" + 0.004*\"need\" + 0.004*\"basic\" + 0.004*\"back\" + 0.004*\"move\" + 0.004*\"take\" + 0.004*\"reason\" + 0.004*\"two\"'),\n",
       " (1,\n",
       "  '0.036*\"ring\" + 0.030*\"sold\" + 0.026*\"holi\" + 0.019*\"pokemon\" + 0.018*\"elden\" + 0.018*\"rn\" + 0.017*\"gloriou\" + 0.014*\"honor\" + 0.011*\"fox\" + 0.011*\"simultan\"'),\n",
       " (2,\n",
       "  '0.025*\"15\" + 0.024*\"expans\" + 0.011*\"cup\" + 0.010*\"audienc\" + 0.010*\"workshop\" + 0.010*\"w\" + 0.009*\"paint\" + 0.009*\"infin\" + 0.009*\"potato\" + 0.009*\"soul\"'),\n",
       " (3,\n",
       "  '0.014*\"rpg\" + 0.013*\"armi\" + 0.012*\"chill\" + 0.010*\"nation\" + 0.010*\"battl\" + 0.010*\"ball\" + 0.009*\"plane\" + 0.009*\"skirmish\" + 0.009*\"intuit\" + 0.008*\"cold\"'),\n",
       " (4,\n",
       "  '0.019*\"can\" + 0.018*\"not\" + 0.015*\"ea\" + 0.013*\"state\" + 0.013*\"broken\" + 0.012*\"buggi\" + 0.011*\"fuck\" + 0.010*\"worst\" + 0.009*\"terribl\" + 0.009*\"server\"'),\n",
       " (5,\n",
       "  '0.007*\"world\" + 0.007*\"stori\" + 0.007*\"interest\" + 0.006*\"combat\" + 0.006*\"charact\" + 0.006*\"feel\" + 0.006*\"mechan\" + 0.006*\"end\" + 0.006*\"build\" + 0.005*\"cool\"'),\n",
       " (6,\n",
       "  '0.099*\"builder\" + 0.094*\"stuck\" + 0.048*\"truck\" + 0.043*\"wont\" + 0.032*\"alright\" + 0.028*\"min\" + 0.028*\"polici\" + 0.019*\"lord\" + 0.016*\"closest\" + 0.016*\"11\"'),\n",
       " (7,\n",
       "  '0.042*\"game\" + 0.019*\"play\" + 0.016*\"like\" + 0.013*\"good\" + 0.012*\"fun\" + 0.012*\"get\" + 0.012*\"time\" + 0.010*\"realli\" + 0.010*\"great\" + 0.009*\"one\"'),\n",
       " (8,\n",
       "  '0.116*\"empir\" + 0.031*\"account\" + 0.017*\"n\" + 0.016*\"children\" + 0.013*\"codemast\" + 0.013*\"de\" + 0.012*\"com\" + 0.012*\"pour\" + 0.009*\"bugfix\" + 0.009*\"un\"'),\n",
       " (9,\n",
       "  '0.085*\"space\" + 0.035*\"biggest\" + 0.034*\"\" + 0.023*\"drift\" + 0.022*\"planet\" + 0.022*\"t\" + 0.021*\"companion\" + 0.017*\"it\" + 0.016*\"galaxi\" + 0.013*\"ban\"')]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "bf382458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el257223554182800007690155034\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el257223554182800007690155034_data = {\"mdsDat\": {\"x\": [-0.42046959626937686, -0.11198504331101936, -0.5025725523774819, -0.49516074052345466, -0.45005577794129453, -0.2399509654056908, -0.024987553231065024, 0.3144366925013235, 0.44732245482012856, 0.17909771209819503, 0.44479055983150995, 0.3579212483897264, -0.22846237540641748, -0.18520891443548712, 0.14112928103008934, 0.01456081011596003, -0.09770312769223384, -0.14224785120934802, 0.1353422667262102, 0.21288372782899623, 0.2701081558820108, 0.012057254883505513, 0.22823517936774626, 0.041428372156102324, 0.09949078217136523], \"y\": [-0.3130885061013926, 0.5158277411755309, 0.050162245797717935, -0.15232407895958938, 0.2905275871225756, 0.3751466649149533, -0.47881130976791825, -0.3868434141201516, -0.15016078775931005, 0.4214808303622908, 0.08193466532569751, 0.2727755893639237, 0.1519766134698386, -0.3113987339973937, -0.3194750341981275, -0.260934073198788, -0.14991633574505578, -0.0006513367114385266, 0.25756059139432247, -0.16531941727136257, -0.03428580175404564, 0.18227763845121128, 0.11136932720948904, 0.044444289130770614, -0.03227495413374811], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [48.505229275117316, 11.170352848177895, 10.700844977269783, 9.99536257420767, 9.036552560818757, 1.5171387583162763, 1.0970220330233058, 0.9871063378996683, 0.8625630043102883, 0.7644568747877594, 0.7292150550691658, 0.6588339876655633, 0.629464024919201, 0.554172343907536, 0.3932787891474747, 0.3081947541318336, 0.3065474089211477, 0.29289196497695785, 0.2873541163207501, 0.27841700462487146, 0.25681116092215894, 0.2297225325687218, 0.21792950185324594, 0.162602825206703, 0.05793528583595212]}, \"tinfo\": {\"Term\": [\"game\", \"play\", \"good\", \"fun\", \"great\", \"earli\", \"access\", \"love\", \"recommend\", \"friend\", \"get\", \"10\", \"time\", \"best\", \"cool\", \"much\", \"worth\", \"also\", \"bug\", \"race\", \"realli\", \"graphic\", \"could\", \"gameplay\", \"look\", \"make\", \"feel\", \"hour\", \"thing\", \"see\", \"first\", \"think\", \"say\", \"tri\", \"pretti\", \"want\", \"littl\", \"made\", \"someth\", \"got\", \"around\", \"complet\", \"keep\", \"nice\", \"though\", \"experi\", \"way\", \"finish\", \"done\", \"probabl\", \"go\", \"lot\", \"get\", \"time\", \"mani\", \"bit\", \"like\", \"even\", \"better\", \"much\", \"give\", \"one\", \"new\", \"also\", \"would\", \"need\", \"look\", \"well\", \"enjoy\", \"realli\", \"still\", \"seri\", \"space\", \"minut\", \"win\", \"lose\", \"faction\", \"wast\", \"shit\", \"cost\", \"caus\", \"offer\", \"happi\", \"exist\", \"kinda\", \"man\", \"stay\", \"group\", \"name\", \"god\", \"whatev\", \"ok\", \"shot\", \"0\", \"non\", \"chanc\", \"forc\", \"call\", \"kill\", \"happen\", \"lost\", \"5\", \"4\", \"spend\", \"often\", \"us\", \"3\", \"6\", \"entir\", \"player\", \"reason\", \"1\", \"stop\", \"left\", \"mean\", \"money\", \"month\", \"enemi\", \"choic\", \"weapon\", \"skill\", \"voic\", \"act\", \"pro\", \"b\", \"con\", \"element\", \"varieti\", \"present\", \"action\", \"repetit\", \"rpg\", \"encount\", \"scale\", \"plenti\", \"standard\", \"heavi\", \"equip\", \"impact\", \"import\", \"environ\", \"dialogu\", \"uniqu\", \"engag\", \"abil\", \"rang\", \"quest\", \"certain\", \"combat\", \"battl\", \"side\", \"visual\", \"effect\", \"main\", \"charact\", \"fight\", \"design\", \"area\", \"order\", \"limit\", \"system\", \"requir\", \"gener\", \"turn\", \"fix\", \"releas\", \"updat\", \"improv\", \"current\", \"crash\", \"potenti\", \"state\", \"featur\", \"support\", \"war\", \"dlc\", \"high\", \"patch\", \"version\", \"multiplay\", \"titl\", \"onlin\", \"edit\", \"perform\", \"product\", \"rate\", \"engin\", \"stage\", \"fp\", \"issu\", \"purchas\", \"dev\", \"commun\", \"bug\", \"campaign\", \"ad\", \"ai\", \"mode\", \"develop\", \"steam\", \"hope\", \"chang\", \"content\", \"work\", \"great\", \"amaz\", \"best\", \"fantast\", \"favorit\", \"creativ\", \"blast\", \"deserv\", \"platform\", \"glad\", \"11\", \"ive\", \"rip\", \"sequel\", \"ill\", \"aliv\", \"consol\", \"passion\", \"brand\", \"outstand\", \"irl\", \"2021\", \"trailer\", \"game\", \"notch\", \"love\", \"10\", \"fun\", \"definit\", \"recommend\", \"good\", \"graphic\", \"&\", \"play\", \"beauti\", \"music\", \"soundtrack\", \"art\", \"gameplay\", \"stori\", \"absolut\", \"style\", \"9\", \"combin\", \"world\", \"realli\", \"puzzl\", \"brain\", \"ca\", \"realiti\", \"cup\", \"audienc\", \"w\", \"infin\", \"spare\", \"potato\", \"adult\", \"dri\", \"ark\", \"leaderboard\", \"usag\", \"coffe\", \"nasa\", \"teen\", \"boi\", \"ms\", \"eargasm\", \"grandma\", \"do\", \"deaf\", \"mehh\", \"\\u2018\", \"paint\", \"averag\", \"soul\", \"dark\", \"necessari\", \"comput\", \"master\", \"short\", \"requir\", \"difficult\", \"resourc\", \"physic\", \"destroy\", \"market\", \"farm\", \"spawn\", \"food\", \"autom\", \"wave\", \"receiv\", \"gather\", \"accept\", \"thousand\", \"materi\", \"unabl\", \"instantli\", \"fuel\", \"social\", \"what\", \"transport\", \"center\", \"escap\", \"surfac\", \"repair\", \"risk\", \"station\", \"earli\", \"access\", \"pay\", \"launch\", \"stuck\", \"expans\", \"light\", \"playabl\", \"prefer\", \"hang\", \"local\", \"cheaper\", \"overpr\", \"scout\", \"dirti\", \"scam\", \"tens\", \"usd\", \"sooner\", \"incorpor\", \"licens\", \"reliev\", \"legitim\", \"scaveng\", \"dang\", \"amus\", \"soon\", \"push\", \"wall\", \"fail\", \"pull\", \"e\", \"polit\", \"wheel\", \"respons\", \"enter\", \"automat\", \"advanc\", \"hurt\", \"anti\", \"bodi\", \"flat\", \"sport\", \"doubl\", \"gap\", \"everywher\", \"g\", \"outright\", \"teleport\", \"hilari\", \"colour\", \"can\", \"not\", \"screen\", \"divis\", \"went\", \"board\", \"block\", \"it\", \"dedic\", \"black\", \"hero\", \"separ\", \"famili\", \"rage\", \"\\u201c\", \"\\u201d\", \"doesn\", \"mental\", \"stumbl\", \"sent\", \"d\", \"forgotten\", \"scienc\", \"invit\", \"colleg\", \"\\u2019\", \"v\", \"t\", \"don\", \"path\", \"faster\", \"justifi\", \"session\", \"lie\", \"becam\", \"season\", \"today\", \"period\", \"chore\", \"defeat\", \"formula\", \"nail\", \"influenc\", \"record\", \"song\", \"sweet\", \"primari\", \"piss\", \"arriv\", \"multi\", \"plagu\", \"verdict\", \"95\", \"unreal\", \"star\", \"busi\", \"friend\", \"dragon\", \"op\", \"ball\", \"co\", \"appar\", \"lol\", \">\", \"boy\", \"peac\", \"14\", \"door\", \"own\", \"mile\", \"favor\", \"hesit\", \"closest\", \"fellow\", \"buddi\", \"six\", \"beaten\", \"dear\", \"ambiti\", \"oddli\", \"calm\", \"expens\", \"store\", \"wont\", \"copi\", \"steal\", \"construct\", \"alright\", \"benefit\", \"capabl\", \"bear\", \"men\", \"publish\", \"ador\", \"fighter\", \"logist\", \"nonsens\", \"protect\", \"profit\", \"inde\", \"ga\", \"tradit\", \"lake\", \"pve\", \"dimens\", \"\\u2013\", \"ident\", \"cool\", \"sim\", \"suck\", \"aw\", \"truck\", \"scratch\", \"sight\", \"itch\", \"dive\", \"euro\", \"fi\", \"link\", \"sci\", \"spread\", \"underwhelm\", \"fetch\", \"orang\", \"lone\", \"smile\", \"mc\", \"drove\", \"monument\", \"threaten\", \"deficit\", \"zip\", \"race\", \"pure\", \"surprisingli\", \"tough\", \"legend\", \"mmo\", \"trial\", \"ring\", \"lord\", \"wife\", \"shadow\", \"elden\", \"tap\", \"revisit\", \"yard\", \"idiot\", \"hassl\", \"moron\", \"crossbow\", \"investig\", \"hail\", \"crumb\", \"rx\", \"drunkenli\", \"bb\", \"kick\", \"punch\", \"promis\", \"meet\", \"ass\", \"c\", \"complic\", \"chines\", \"languag\", \"hill\", \"45\", \"googl\", \"killer\", \"applic\", \"girlfriend\", \"daughter\", \"daunt\", \"drug\", \"everybodi\", \"videogam\", \"va\", \"bearabl\", \"weav\", \"unsuspect\", \"fold\", \"goliath\", \"trike\", \"cultur\", \"artist\", \"english\", \"outpost\", \"admir\", \"awesom\", \"dont\", \"fuck\", \"smoke\", \"excus\", \"data\", \"heck\", \"clan\", \"joe\", \"drifter\", \"lama\", \"brrrrrr\", \"noon\", \"decay\", \"typewrit\", \"itu\", \"retreat\", \"apc\", \"ventur\", \"recon\", \"gamemod\", \"heli\", \"unarm\", \"didnt\", \"waypoint\", \"deploy\", \"german\", \"unit\", \"server\", \"wargam\", \"isnt\", \"ea\", \"beta\", \"mod\", \"terribl\", \"command\", \"20\", \"cant\", \"addict\", \"crazi\", \"polici\", \"rid\", \"sold\", \"babi\", \"swarm\", \"yell\", \"h\", \"newli\", \"bite\", \"understood\", \"struck\", \"rebel\", \"geniu\", \"daddi\", \"34\", \"thou\", \"yearn\", \"heartfelt\", \"220\", \"mommi\", \"pong\", \"lowlif\", \"divers\", \"realist\", \"word\", \"gem\", \"cri\", \"laugh\", \"dread\", \"pump\", \"basi\", \"pandem\", \"nonetheless\", \"fist\", \"stealthi\", \"netflix\", \"eras\", \"fusion\", \"unholi\", \"lockdown\", \"soni\", \"bloodborn\", \"mop\", \"123\", \"gunsling\", \"74\", \"uncertain\", \"snowboard\", \"unit\", \"15\", \"grab\", \"download\", \"cheap\", \"joke\", \"dollar\", \"buck\", \"app\", \"slave\", \"darn\", \"cure\", \"yellow\", \"christma\", \"thorough\", \"export\", \"defiantli\", \"heartwarm\", \"sfw\", \"hollywood\", \"tsushima\", \"mixtap\", \"cash\", \"greedi\", \"artwork\", \"jeep\", \"2k\", \"fish\", \"20\", \"server\", \"mod\", \"unit\", \"ton\", \"wow\", \"70\", \"200\", \"bot\", \"hot\", \"children\", \"egg\", \"bend\", \"resist\", \"eastern\", \"solar\", \"dc\", \"dad\", \"wholesom\", \"duck\", \"accessori\", \"tomorrow\", \"stabilis\", \"skeleton\", \"easter\", \"160\", \"defi\", \"1300\", \"asynchron\", \"urg\", \"relax\", \"im\", \"self\", \"girl\", \"club\", \"everytim\", \"void\", \"gay\", \"email\", \"platinum\", \"shower\", \"cathart\", \"50fp\", \"strand\", \"chill\", \"nake\", \"sexi\", \"everyday\", \"laid\", \"hallway\", \"muscular\", \"handsom\", \"445\", \"dawg\", \"cackl\", \"kingdom\", \"builder\", \"youth\", \"process\", \"coal\", \"tilt\", \"solv\", \"fish\", \"research\", \"spot\", \"20\", \"low\", \"ship\", \"mod\", \"tactic\", \"depth\", \"car\", \"com\", \"http\", \"id\", \"radar\", \"juic\", \"www\", \"steamcommun\", \"url\", \"l\", \"quot\", \"aswel\", \"filedetail\", \"sharedfil\", \"thingi\", \"hog\", \"in\", \"|\", \"medit\", \"tedious\", \"ziplin\", \"del\", \"documentari\", \"v\", \"youtub\", \"na\", \"gon\", \"wan\", \"east\", \"omg\", \"3080\", \"grade\", \"ti\", \"3070\", \"banger\", \"van\", \"hq\", \"proud\", \"diagnos\", \"rdr2\", \"bachelor\", \"wreck\", \"brillianc\", \"homerun\", \"dumbass\", \"egocentr\", \"\\u56fd\\u4eba\\u73a9\\u5bb6\\u7d20\\u8d28\\u4f4e\\u4e0b\\uff0c\\u5bf9\\u840c\\u65b0\\u53cd\\u6b63\\u4e2a\\u4eba\\u4f53\\u9a8c\\u975e\\u5e38\\u6076\\u52a3\\uff0c\\u770b\\u7740\\u8bc4\\u8bba\\u533a\\u90fd\\u662f\\u7edf\\u4e00\\u7684\\u56fd\\u5916\\u73a9\\u5bb6\\u5bf9\\u4e2d\\u56fd\\u73a9\\u5bb6\\u7684\\u62b1\\u6028\\u3002\\u3002\\u3002\\u3002\\u3002\", \"feloni\", \"misstak\", \"numeri\", \"2022\", \"ea\", \"ignor\", \"rule\", \"ui\", \"report\", \"destruct\", \"casual\", \"unit\", \"fli\", \"mod\", \"low\", \"greatest\", \"format\", \"usa\", \"pl\", \"aint\", \"pod\", \"pleb\", \"mach\", \"whale\", \"squid\", \"bpm\", \"slooooow\", \"breeeeeaaaatherrrrrrr\", \"ussr\", \"halah\", \"unfold\", \"mega\", \"unrespons\", \"atm\", \"cmon\", \"nato\", \"impli\", \"rli\", \"mushroom\", \"copiou\", \"crime\", \"germani\", \"west\", \"union\", \"warno\", \"unit\", \"wargam\", \"depth\", \"skirmish\", \"ship\", \"fleet\", \"rt\", \"steel\", \"20\", \"mod\", \"ui\"], \"Freq\": [25661.0, 11917.0, 8321.0, 7554.0, 5816.0, 2739.0, 2325.0, 4592.0, 4816.0, 1889.0, 8384.0, 3427.0, 8211.0, 3074.0, 1359.0, 5334.0, 3267.0, 4242.0, 2748.0, 1067.0, 6575.0, 2850.0, 3095.0, 3009.0, 4544.0, 6306.890017638296, 5340.864480935227, 4709.83846218333, 4574.012141880901, 4043.7814670933053, 3692.1997974935903, 3393.196830868222, 3174.1286830032413, 2970.3415685695577, 2895.0011919282365, 4706.354602361346, 2652.5597606194046, 2500.5605198229273, 2494.304414996805, 2454.705839997976, 2328.269466700261, 2306.3270448557546, 2281.6068514127405, 2274.9828333043147, 2227.6678167100486, 2328.7908087691044, 4495.909257568378, 1601.0796532367528, 1545.3664501559142, 1435.17523783579, 5337.151662203654, 4681.765994498555, 8311.131045724058, 8116.079108447898, 3169.475290544235, 2580.539024995949, 10362.356021084244, 5473.345895788387, 3727.885543678908, 5239.41673566418, 3032.9578827744754, 6359.016855730157, 3671.5524409296386, 4122.1117312884535, 4986.749788916219, 4274.882326756198, 4352.198329298988, 3788.370434655373, 3817.904818136426, 5422.703343041441, 3689.4146571025904, 1402.5788096809124, 1089.112459063698, 1075.8347446755279, 915.9027772917724, 854.6016665704022, 846.3876557947882, 786.6352137717179, 737.1372094434357, 713.5867543061634, 711.1492965631109, 700.5094644233459, 665.7347521372019, 660.5729271838035, 614.4595939623952, 609.5583148053412, 593.3077480323631, 586.0352536433998, 574.2079047942666, 549.386538898831, 542.1311960475196, 516.2388590501525, 508.9509100954605, 501.75220368208437, 491.7538269292723, 491.9292514018285, 1014.5478763970842, 892.2525644478745, 1185.1519789932847, 1319.0610031477302, 663.7326889047191, 1812.512317152349, 1769.4614727969754, 1120.701955050079, 833.629396633524, 925.2921835487322, 2260.5188442671424, 969.8995754006202, 961.0722910932058, 2178.1468574189307, 1282.0524697493122, 1726.898629081789, 833.433093345699, 776.5730793595071, 880.7221856852937, 918.7449628384975, 794.7246183232602, 1508.4186635142132, 1124.881870078178, 1011.4464194563201, 929.6539256285533, 915.5858219859765, 851.0801517681043, 823.5486265381757, 748.4348068207005, 725.8286390053613, 713.2974089762131, 661.2540530314111, 625.58594000048, 622.2153963582307, 614.407305059241, 596.8082994607778, 545.0719002155413, 531.7236672213306, 519.8263228070596, 490.6282787376516, 485.5541457333824, 484.9020006363768, 473.6079448669297, 458.5759205501453, 451.85265730702486, 448.0476018547608, 1113.2915984810056, 789.6434098994528, 1112.1451700736634, 605.2897563360034, 1006.6395019258073, 983.0591660639373, 1877.1307314599817, 1792.393264749908, 1152.7193069498715, 944.6186308053441, 888.7433962580608, 1388.8374602016422, 1644.2975432283383, 1113.5027001567423, 1414.935804775476, 799.4476196565017, 793.6687403125928, 857.7314801992783, 1014.8752541003405, 826.5178099261984, 860.5547038478064, 805.1274507444132, 2470.8547440531484, 2238.5728154422295, 2195.3805769791247, 2040.4386820138466, 1912.018824110388, 1806.785900059244, 1563.276618570875, 1506.4872373176565, 1478.7215730354335, 1425.69864903459, 1344.5925919852473, 1173.5951507257855, 1035.1122813203372, 985.6674157857323, 981.0224149710629, 948.3164073902037, 935.3714476173835, 867.666487989161, 825.9276425797285, 797.8233051773492, 671.110153721358, 618.1008959537057, 581.2624797650172, 534.7981546121224, 526.610725153463, 2137.667423439479, 731.6943550839316, 2151.770325319837, 1513.7937912774055, 2621.7386354579717, 1528.983375620804, 1437.0984567594537, 1534.314725035745, 1708.0103796531325, 2120.246446910021, 999.0841859901551, 1661.358198024942, 1583.7980946400553, 1466.9195875909415, 1394.896152505369, 5813.204640636264, 1791.8295239745198, 3072.338771863708, 781.9084028782077, 656.2559542709693, 466.4431058178774, 391.776612895279, 340.80882243327596, 253.46393565993586, 252.78083993958526, 238.86235708436993, 226.22333347324354, 185.03519878379902, 182.83551534625877, 167.27345981668833, 166.5537394774178, 156.02901540309665, 148.08739334209693, 137.59076285096333, 128.38553015104986, 127.15749000882477, 125.8377534897121, 104.12031053454426, 25386.46393888633, 90.98979630445828, 4522.339303715292, 3330.251985860653, 7277.446510919485, 1446.6118475660817, 4367.778508979098, 7178.486496424195, 2549.7251695106174, 573.7579950605417, 9222.434013570579, 1039.1751702881922, 1274.2461929091758, 477.45402538553117, 649.5345277216362, 2124.208678010941, 2022.0596250778622, 1226.8946300478115, 848.1041901803422, 484.0594494604806, 507.45292224893933, 835.4223643037311, 1151.6359900904, 861.2803613867272, 406.82205793219237, 316.9116819046775, 292.6575240867234, 246.6699507218047, 223.58868868352658, 214.45574130997284, 194.0285377055563, 191.23546696257011, 187.16119134685596, 182.89675334569668, 181.05355614746094, 168.6655081332916, 164.42749300709028, 163.83785955569195, 152.67749568746657, 146.55899500678044, 146.2164030891012, 143.39402847232986, 134.17048603279875, 131.7241311515273, 127.5226882475809, 127.00175427107325, 122.60169530798021, 118.6689672803586, 146.38386320378805, 191.96467246000807, 165.24977946092224, 169.09117159429664, 161.3708355656917, 153.5103512912817, 153.36748667515386, 153.1577993490856, 156.50830220800734, 156.18286458048772, 154.66003445183574, 945.4715249910229, 881.1869385645581, 595.3086226260912, 467.2782274945417, 403.43532928226347, 350.2159944691596, 330.4090683743779, 316.92855456478014, 302.47581242861247, 274.370233321232, 264.0159873583922, 254.60578090983265, 251.9124160815392, 247.17333867217724, 217.37157397124838, 204.52204672593405, 194.99444244511088, 182.84676662875913, 162.44861352756868, 160.29965704067763, 157.77922248102067, 156.18163821018973, 156.03646820771007, 149.88914058979182, 144.75869342466737, 156.79016181467995, 2738.3069458467216, 2324.1915887532086, 920.7378451025339, 686.460303591832, 569.345247735861, 535.3881682353458, 462.077080832166, 441.34066141648515, 289.568583054182, 172.74786401847297, 142.26703100840555, 129.39831331242627, 75.48976792550168, 74.89005625254269, 69.58066089363304, 65.23697287024407, 63.6542871870578, 57.80370154306722, 50.516163306908055, 49.5712680985306, 111.63614288297549, 33.455432792573305, 30.468161498790792, 27.16651782951135, 25.599808104963337, 32.576463183544725, 622.4979343426766, 523.8640178793408, 487.68341621019505, 440.2389780403061, 421.9717379218862, 401.3993450936776, 373.2271741551137, 360.4587547089705, 309.076818440785, 302.96582378835956, 285.24308567472787, 278.33702487084764, 276.1645391844005, 212.18962517216812, 211.9005357349644, 198.11832832267447, 195.66935160550148, 188.50666144886426, 164.36895702695568, 157.06474539487587, 134.13008583286074, 131.83368461642783, 120.41390305600547, 105.35824572176658, 104.22591480016474, 897.5214910221616, 821.5688852633019, 812.4545938082373, 762.93821800497, 421.4081822215856, 409.4242769297629, 334.45579379366615, 221.0351886874614, 204.19568893650919, 182.71137860184808, 174.32765526596077, 169.516357772386, 146.49550681418418, 124.43173576194593, 101.55937088678346, 101.09871562626161, 98.42075667567909, 96.64889464411267, 75.44236112378987, 60.64399337638722, 52.466582742805606, 51.898561260340834, 50.629191894872186, 46.72584532252351, 44.052569075509474, 390.55635774075034, 203.55592130140712, 243.89977256006014, 104.66018336155206, 441.38541316498026, 360.55339537994826, 345.1318293832291, 312.90539688772583, 301.4760442427698, 279.4428753727474, 261.98551162313794, 261.1881925837533, 250.97983115625232, 204.10444764790793, 183.46260295460897, 173.0577010624464, 148.7260421798575, 144.59786948097238, 143.94391625762523, 136.48231029574987, 119.60038012093146, 116.60725669624154, 109.11313665234287, 103.22584845708407, 98.02019278887589, 95.37421594302367, 90.44717327750234, 88.47259758657708, 82.50701130379807, 450.6275152631969, 213.5628724461375, 1888.475768752028, 632.3837533187063, 363.09681976340283, 357.0773030217323, 287.19492222530545, 269.9244108932481, 246.87862711986932, 194.86986177613736, 182.4457167141985, 158.47893375346501, 149.48481826714803, 147.38432014381345, 130.44807548155845, 121.61963489912547, 111.86312459857302, 110.72752872571357, 108.56334578129508, 82.38398713322471, 79.83713047774216, 76.11560772631715, 70.171957994227, 69.44706773448257, 63.87259407737583, 61.6242153384013, 61.40708069695983, 329.7204080577233, 323.6457697688737, 310.9240997319444, 285.6155385093689, 278.86137765610334, 230.847648201702, 228.22140045540834, 199.06194270262156, 184.06815751232514, 176.2581834542023, 162.90685329023628, 155.3168477161355, 149.8177312421763, 137.55598413106802, 136.79889473940844, 128.9745220486835, 127.09391582486481, 116.1444660999276, 104.43186152519483, 103.30395270026382, 99.57647594827249, 98.53668019761439, 78.79056757494055, 73.87565444435246, 72.85784597943733, 84.4044974338186, 1358.5166163968258, 789.4849240809253, 651.354112199453, 398.2527122962366, 306.1554578845813, 273.09243302336375, 235.73300597973838, 212.62026441671418, 142.30043825598844, 120.51506020816898, 111.22080954379287, 107.95230046424723, 105.81635480879339, 94.35647604006138, 79.69730915396194, 69.12729649262027, 69.06618167734642, 48.68157948999031, 46.24507799228573, 34.649890312948905, 27.596063194469192, 23.27488254149795, 20.723364561028916, 14.877072201947788, 11.95067048436542, 1066.0552792153555, 238.7955514272125, 229.7027849654968, 169.93804856424455, 168.29865452359581, 161.40312126884177, 148.91892005186364, 146.4357087074669, 129.62635488466273, 125.92611904862669, 77.14563681258386, 73.24002009189732, 69.82202400541965, 67.93007113722442, 57.831810631225764, 38.735293181681826, 37.36891668031571, 35.210125708506155, 35.07653808783926, 26.529236984618574, 26.30617557374301, 23.265724656066062, 19.026514429526383, 13.932874529315914, 8.021058462639003, 152.62248458943372, 67.52352999076592, 501.99860984534195, 289.9216900046751, 268.4014265505595, 187.6588221530774, 184.76711467357447, 169.13515674153305, 131.5157101677359, 112.47224720369256, 110.84568766478179, 74.59292191943403, 73.40284009217659, 57.964958462774455, 47.06078782003781, 45.71962398547349, 40.76011028643136, 39.55600916187822, 39.25499558578301, 38.602820684797486, 37.1748188078715, 17.679511082777477, 9.906259704322162, 4.549764453231257, 4.342264138343647, 3.034994973220635, 2.7241025497958815, 37.14718821763842, 22.03683710332425, 61.3972784056467, 16.382946806594695, 10.70576765453525, 838.8408808598424, 708.59031971429, 554.7094934646168, 168.78285205314214, 123.09575949375485, 78.97600392478586, 53.09030477316033, 35.05045154054662, 27.300847333743935, 5.38334250125533, 4.1618540496289915, 4.05153866061547, 4.023837047590337, 3.666918591300142, 1.4423739088581176, 1.3321419922533317, 0.038742297638640476, 0.038742297638640476, 0.038742297638640476, 0.038742297638640476, 0.038742297638640476, 0.038742297638640476, 0.038742297638640476, 0.03874230088709972, 0.038742297638640476, 0.03874230088709972, 0.03874230088709972, 0.038742323626314426, 0.03874230738401821, 0.0387423138809367, 0.03874230088709972, 0.03874230738401821, 0.038742304135558965, 0.038742310632477454, 0.038742304135558965, 0.038742304135558965, 0.03874230738401821, 643.6892274361081, 486.0471222765888, 272.97680393899736, 156.14900443476117, 113.47927827936938, 100.32729926768367, 89.37560561736765, 75.89329903084605, 75.15480233785944, 73.35406034139221, 56.407410975150015, 52.05843585971806, 43.3997178683886, 42.16379744101293, 39.90931984727919, 33.09288349683654, 20.37239873340066, 18.759583105325607, 6.654648652334926, 4.346668505107742, 3.7479676533381636, 3.385239473679349, 2.957331277460458, 2.483403503766344, 1.445216710105555, 60.84391492052195, 862.7413129244028, 614.7222797685214, 174.63595452813138, 150.78971961372685, 135.45916774056823, 90.81254529348601, 85.74926040660722, 82.20868094001705, 56.833716337459165, 46.97324957590513, 24.856020074646004, 20.016977021873352, 17.61839076176233, 15.193970620047065, 14.201389858327182, 9.065988373531313, 8.197814823612978, 5.486902079855887, 5.164999754277813, 2.127985509875307, 1.3082033259300188, 0.9295271673262228, 0.5636859031745542, 0.35333404352786263, 0.03725421342574892, 0.03725422256095713, 543.5868571653107, 314.56430906167174, 276.02978881176585, 236.77743803661104, 236.29274533111132, 225.52726934136965, 191.32392579028982, 68.15217750995421, 31.103771887026234, 28.428949849972923, 19.098178709713803, 16.640142397637344, 12.72829836397756, 11.802536414306804, 10.167420593411602, 7.873297981450383, 7.530311094191002, 3.8939787092395663, 3.4203786152813636, 1.0063671083839423, 0.6794652253881848, 0.040289739374554996, 0.040289724622737395, 0.040289724622737395, 0.040289724622737395, 0.04028973347382795, 0.04028973347382795, 0.040289751176009074, 0.04028973642419147, 0.040289739374554996, 0.040289739374554996, 683.01250693422, 228.99605936475683, 163.86051578035506, 155.43315677998336, 146.3289922162918, 127.83223737698451, 101.81533526354217, 73.15616938324513, 41.04736789373219, 34.65991437123402, 33.11322800763825, 31.118091216354685, 27.807730391974978, 22.740646584476977, 22.57074572646158, 20.591629799118614, 19.807040801277545, 17.770632659331202, 14.381806107698845, 12.340916917010725, 9.808167138948821, 4.000004466942256, 2.5032410061761823, 1.374007927657109, 1.365759143883205, 6.632018322476444, 600.0060951988636, 537.2562524397664, 152.6465589852863, 88.13069382690864, 86.60826553771788, 72.1920560012271, 45.86783863150891, 44.425308811583335, 40.19137046848652, 29.480688011538756, 20.060891061211827, 14.365086431632275, 4.851776090521716, 9.610374691880093, 0.03759564230279105, 0.03759563013103127, 0.03759563013103127, 0.03759563013103127, 0.03759563013103127, 0.03759562769667931, 0.03759562769667931, 0.03759562769667931, 0.03759562769667931, 0.03759562769667931, 0.03759562769667931, 0.03759563499973518, 0.03759564717149496, 0.03759563013103127, 0.03759563499973518, 0.03759563013103127, 0.03759563013103127, 0.03759563499973518, 0.03759563499973518, 0.03759563499973518, 0.03759563499973518, 0.03759564230279105, 0.037595637434087135, 0.037595637434087135, 0.037595637434087135, 0.03759563499973518, 0.03759563499973518, 801.1645484963062, 154.0175623726056, 134.57296280769893, 91.18582453834067, 83.30680690543385, 67.43752637508076, 38.273225950932556, 35.73778835565568, 33.92370684652695, 27.09540383509265, 27.052447110668542, 26.141471316773497, 24.213905070601626, 24.213905070601626, 14.092111298215029, 11.965479121576067, 11.509713359771961, 7.3672768825467765, 5.89368623906043, 5.495705725792811, 0.9538159037596603, 0.9043484953648533, 0.5375355509785241, 29.598117227089542, 37.443977896982304, 436.0131336569693, 279.2026986647346, 149.20950776455808, 54.25679690637782, 39.583400410480444, 28.332126678361604, 25.741655064991402, 25.731047229195394, 23.87353260534701, 22.105605848216282, 12.3645830375699, 4.006793695400932, 26.6577507286002, 2.5058751374521853, 1.8034242775071505, 0.4197286074768009, 0.030806974127693704, 0.030806970681514514, 0.030806970681514514, 0.030806970681514514, 0.030806970681514514, 0.030806970681514514, 0.030806970681514514, 0.030806970681514514, 0.030806970681514514, 0.030806977573872893, 0.030806984466231275, 0.030806977573872893, 0.030806974127693704, 0.030806984466231275, 0.030806974127693704, 0.030806974127693704, 0.030806974127693704, 0.030806977573872893, 0.030806974127693704, 0.030806974127693704, 0.030806974127693704, 66.08678846166664, 58.01433099193573, 36.64550103697841, 25.388803792801614, 21.866359561336825, 9.515024886050647, 4.446041872468985, 3.868783995371948, 3.5574331864911413, 0.7478212137647301, 0.09582333816006958, 0.09582295506415868, 0.09582274878174511, 0.017955151573906498, 0.01795514789029197, 0.01795514789029197, 0.01795514789029197, 0.01795514789029197, 0.01795514911816348, 0.01795514789029197, 0.01795514789029197, 0.01795514666242046, 0.01795514666242046, 0.01795514666242046, 0.01795514666242046, 0.01795514789029197, 0.01795514789029197, 0.01795514789029197, 0.01795514789029197, 0.017955151573906498, 0.01795515771326404, 0.017955154029649516, 0.017955151573906498, 0.01795514911816348, 0.017955151573906498, 0.01795514911816348, 0.01795514911816348, 0.01795514911816348, 0.01795514911816348, 0.01795514911816348, 0.01795514911816348], \"Total\": [25661.0, 11917.0, 8321.0, 7554.0, 5816.0, 2739.0, 2325.0, 4592.0, 4816.0, 1889.0, 8384.0, 3427.0, 8211.0, 3074.0, 1359.0, 5334.0, 3267.0, 4242.0, 2748.0, 1067.0, 6575.0, 2850.0, 3095.0, 3009.0, 4544.0, 6307.885516051087, 5341.859973693565, 4710.838641538813, 4575.007635621447, 4044.8050794972187, 3693.195290213181, 3394.192323587813, 3175.124175722832, 2971.36261544051, 2895.9966846478274, 4708.012126740741, 2653.5552533389955, 2501.556013877168, 2495.299916094869, 2455.7013337385197, 2329.2650834980277, 2307.3318536364823, 2282.6023441323314, 2275.9783260239055, 2228.6665378374546, 2329.873263419775, 4498.450481201285, 1602.075145956344, 1546.3619428755053, 1436.170730579252, 5349.25322674011, 4691.5088228608165, 8384.457063318378, 8211.298176183876, 3176.639744457712, 2583.6956893663155, 10629.965559518107, 5558.4082972181795, 3762.0834871994284, 5334.66262264118, 3048.241768720532, 6677.202199631809, 3749.6941098244156, 4242.35625608854, 5260.849141215277, 4452.552746193924, 4544.472149946689, 3936.0887758287868, 4033.4766810691517, 6575.287129450865, 4511.96758558774, 1403.572997399872, 1090.1066467826577, 1076.8289323944875, 916.8969650107316, 855.5958542893615, 847.3818435137474, 787.6294014906771, 738.131397198536, 714.5809420251226, 712.1434843317202, 701.5036524997854, 666.7289398561611, 661.5676335550694, 615.4537816813544, 610.5525025243004, 594.3019357513223, 587.0294419727413, 575.2020925132258, 550.3807266177902, 543.1253837664788, 517.2330467691118, 509.94724913111486, 502.7467080368774, 492.748276823505, 492.9314998780276, 1017.486409473142, 897.7827451742608, 1206.841087986755, 1371.5458425628126, 671.9547682743569, 1928.7958723840336, 1902.9749897271972, 1209.4382270598576, 870.7517781415986, 998.6600696861634, 2955.009510979626, 1092.4677439560362, 1097.3719313848294, 3280.3569264409275, 1645.530625258252, 2641.454068619372, 1007.7906302929056, 920.3222299551446, 1636.898435369855, 2247.279132104324, 1073.1632748453449, 1509.410721572485, 1125.87392801333, 1012.4384810160961, 930.6461378699642, 916.5778796801876, 852.0722094623154, 824.5406842323868, 749.4276469045886, 726.8206966995724, 714.2894666784556, 662.2461107256222, 626.5779977228009, 623.2074540524418, 615.3993635793792, 597.8003571549889, 546.0639579097524, 532.7157250239649, 520.8189467847302, 491.6219950973814, 486.5462163337718, 485.894058330588, 474.6000025611409, 459.5682056190835, 452.8448738049425, 449.039659548972, 1117.6307425301638, 792.4313745216521, 1121.8011534620148, 607.7038731430313, 1020.2561732424138, 997.7022141583942, 1940.7744048354025, 1854.6268439433097, 1206.525109349348, 978.1810218212587, 916.9850607457274, 1514.7777786309732, 1981.0985449208688, 1342.26119085662, 1932.5663265738358, 907.5513399805304, 946.181475871513, 1243.491399155479, 2462.0402679323493, 1213.6375820990622, 1639.6513506339018, 1957.1213364946148, 2471.854177311937, 2239.572248817473, 2196.3800127436934, 2041.4381235047654, 1913.0182573691768, 1807.7853333180328, 1564.2760518296636, 1507.4866705764452, 1479.7210062942222, 1426.6981007132717, 1345.592025244036, 1174.5945839845742, 1036.1117146124177, 986.6668490445206, 982.0218482539452, 949.3158406489921, 936.3708808761719, 868.6659212479494, 826.9270758385169, 798.8227384361376, 672.1095869801463, 619.1003292124941, 582.2619130238056, 535.7975878709108, 527.6101584122514, 2149.73068646507, 735.200028307333, 2198.205499800787, 1536.4271009118477, 2748.699328166136, 1575.7492413341586, 1495.06862883357, 1627.4150782346396, 1854.0906838693575, 2435.2958275869973, 1023.0045184417355, 2208.6809316773883, 2283.764069756619, 2563.436517717612, 3657.0062942090785, 5816.30600434067, 1792.8255082999556, 3074.2975962898176, 782.9043872036428, 657.2519385964044, 467.4390901433126, 392.7726022563685, 341.8048067587112, 254.4599199853712, 253.7768242650206, 239.85834140980526, 227.21931779867887, 186.03118310923435, 183.8314996716941, 168.26944414212366, 167.54972380285312, 157.02499972853198, 149.08337766753226, 138.58674717639866, 129.3815144764852, 128.15347433426007, 126.83374479905811, 105.1163611725735, 25661.377814792497, 91.98578062989361, 4592.159423296505, 3427.859054071312, 7554.848768836863, 1535.6667907077108, 4816.384238755431, 8321.139057763154, 2850.833388369092, 619.8951530102053, 11917.209966534103, 1175.7166896784088, 1471.3607053639666, 533.8924690517168, 761.8903311990221, 3009.486631026759, 2851.6518919818236, 1612.438819388607, 1313.8259030721135, 619.6672678063021, 694.1730749719115, 2345.745873735956, 6575.287129450865, 862.2847489502651, 407.8264454957303, 317.9160694682154, 293.66191165026135, 247.67433828534269, 224.59307624706457, 215.46012887351083, 195.03292526909428, 192.2398545261081, 188.16557891039395, 183.90114090923467, 182.05794371099893, 169.6698956968296, 165.43188057062827, 164.84224711922994, 153.68188325100456, 147.56338257031842, 147.22079065263918, 144.39841603586785, 135.17487359633674, 132.7285187150653, 128.5270758111189, 128.00614183461124, 123.6060828715182, 119.67335484389659, 177.34880111991103, 338.3224167548801, 372.0033428089952, 497.78789839716336, 398.55944911813987, 374.3967782389341, 454.7386834348454, 466.65929653636357, 1205.0140387111567, 1213.6375820990622, 1051.746339587547, 946.467192673232, 882.1826062467673, 596.3042903083003, 468.27389517675095, 404.43099696447274, 351.2116621513689, 331.4047360565872, 317.9242222469894, 303.47148011082174, 275.36590100344125, 265.01165504060145, 255.60144859204195, 252.9080837637485, 248.1690067335545, 218.36724165345768, 205.51771440814335, 195.99011012732018, 183.84243431096843, 163.44428120977798, 161.29532472288693, 158.77489016322997, 157.17730589239903, 157.03213588991937, 150.88480827200112, 145.75436110687667, 229.5753357948913, 2739.309792643013, 2325.1944355495, 921.7406918988252, 687.4631503881233, 570.3480945321522, 536.391015031637, 463.07992762845726, 442.3435082127764, 290.57142985047324, 173.7507108147643, 143.26987780469688, 130.4011601087176, 76.49261472179303, 75.89290304883404, 70.58350768992439, 66.23981966653542, 64.65713398334915, 58.806548339358564, 51.5190101031994, 50.574114894821946, 114.1975730182148, 34.45827958886465, 31.47100829508213, 28.169364625802686, 26.602654901254674, 37.71733463997321, 623.4938595732378, 524.8599431099021, 488.67934144075633, 441.2349081853606, 422.96766315244747, 402.3952703242389, 374.22309938567497, 361.4546799395318, 310.0727436713463, 303.96174901892084, 286.23901090528915, 279.3329501014089, 277.1604644149618, 213.1855504027295, 212.89646096552576, 199.11425355323584, 196.66527683606284, 189.50258667942563, 165.36488225751705, 158.06067062543724, 135.1260110634221, 132.8296098469892, 121.40982828656686, 106.35417095232798, 105.22184003072614, 898.5226428262711, 822.5700370674114, 813.4557456123467, 763.9393698090795, 422.40933402569493, 410.42542873387225, 335.4569455977755, 222.03634049157088, 205.19684074061865, 183.71253040595755, 175.32880707007024, 170.51750957649546, 147.49665861829365, 125.43288756605538, 102.56052269089291, 102.09986743037106, 99.42190847978854, 97.65004644822213, 76.44351292789932, 61.64514518049668, 53.46773454691506, 52.89971306445029, 51.63034369898164, 47.726997126632966, 45.05372087961893, 400.8969767494528, 234.1210451295581, 377.6372363794789, 240.5011632018065, 442.3794176611346, 361.5473998761026, 346.12583387938344, 313.89940138388016, 302.4700487389241, 280.4368798689017, 262.97951611929227, 262.1821970799076, 251.97383565240673, 205.09845214406235, 184.45660745076339, 174.05170555860082, 149.72004667601192, 145.5918739771268, 144.93792075377965, 137.4763147919043, 120.59438461708588, 117.60126119239595, 110.10714114849729, 104.21985295323849, 99.0141972850303, 96.36822043917809, 91.44117777365676, 89.4666020827315, 83.50101579995248, 462.11591148450754, 225.81750749471945, 1889.4741132957317, 633.3820978624101, 364.09516430710653, 358.075647565436, 288.19326676900914, 270.9227554369518, 247.87697166357307, 195.8682063198411, 183.44406125790226, 159.47727829716877, 150.48316281085178, 148.3826646875172, 131.4464200252622, 122.61797944282922, 112.86146914227677, 111.72587326941732, 109.56169032499884, 83.38233167692846, 80.83547502144592, 77.1139522700209, 71.17030253793075, 70.44541227818632, 64.87093862107959, 62.62255988210505, 62.40542524066358, 330.71605068907564, 324.64141240022605, 311.91974236329673, 286.6111811407212, 279.85702028745567, 231.8432908330544, 229.21704308676075, 200.05758533397398, 185.06380014367755, 177.25382608555472, 163.9024959215887, 156.3124903474879, 150.81337387352872, 138.55162676242043, 137.79453737076085, 129.9701646800359, 128.08955845621722, 117.14010873128002, 105.42750415654726, 104.29959533161625, 100.57211857962491, 99.53232282896681, 79.78621020629298, 74.87129707570489, 73.85348861078975, 104.34511102874676, 1359.5170473168569, 790.4853550009564, 652.354543119484, 399.2531432162676, 307.1558888046123, 274.0928639433947, 236.73343689976946, 213.62069533674526, 143.30086917601952, 121.51549112820004, 112.22124046382393, 108.9527313842783, 106.81678572882446, 95.35690696009245, 80.697740073993, 70.12772741265134, 70.06661259737749, 49.68201041002136, 47.24550891231678, 35.65032123297996, 28.59649411450025, 24.275313461529006, 21.723795481059973, 15.877503121978844, 12.951101404396475, 1067.0540301556025, 239.79430236745947, 230.70153590574375, 170.93679950449152, 169.29740546384278, 162.40187220908874, 149.9176709921106, 147.43445964771385, 130.6251058249097, 126.92486998887365, 78.14438775283082, 74.23877103214429, 70.82077494566661, 68.92882207747138, 58.83056157147272, 39.73404412192878, 38.36766762056266, 36.20887664875311, 36.07528902808622, 27.52798792486553, 27.304926513989965, 24.26447559631302, 20.02526536977334, 14.93162546956287, 9.01980940288596, 295.69459539318063, 124.06973948144699, 503.003948563606, 290.92702872293916, 269.40676526882356, 188.66416087134155, 185.7724533918386, 170.1404954597972, 132.52104888600005, 113.4775859219567, 111.85102638304593, 75.59826063769817, 74.40817881044073, 58.97029718103859, 48.066126538301944, 46.724962703737624, 41.7654490046955, 40.56134788014236, 40.26033430404714, 39.60815940306162, 38.18015752613564, 18.684849801041608, 10.911598422586291, 5.555103171495387, 5.347602856607777, 4.040333691484765, 3.729441268060012, 55.521251364914995, 42.44259200101026, 355.51775927814975, 44.80548714076415, 42.87555948932527, 839.8458192979527, 709.5952581524003, 555.7144319027271, 169.78779049125248, 124.1006979318652, 79.9809423628962, 54.095243211270656, 36.05538997865695, 28.30578577185426, 6.388280939365656, 5.166792487739317, 5.056477098725796, 5.028775485700663, 4.671857029410468, 2.4473123469684444, 2.3370804303636588, 1.04368074576225, 1.0436807526158784, 1.0436807624440807, 1.0436807638304604, 1.0436807754106716, 1.0436807780837105, 1.0436807784421316, 1.0436808691445802, 1.0436807818858398, 1.043680886040887, 1.0436808980555994, 1.043682118945113, 1.0436813273686505, 1.0436817210052018, 1.0436809582486146, 1.0436813946577903, 1.0436811923319689, 1.0436817492568744, 1.0436812550095187, 1.0436812617976505, 1.0436817290925668, 644.6951881085557, 487.0530829490364, 273.98276461144496, 157.15496510720882, 114.48523895181704, 101.33325994013133, 90.38156628981531, 76.89925970329371, 76.1607630103071, 74.36002101383987, 57.41337164759766, 53.0643965321657, 44.40567854083624, 43.169758113460574, 40.915280519726835, 34.098844169284185, 21.37835940584831, 19.765543777773257, 7.660609324782574, 5.352629177555389, 4.753928325785811, 4.391200146126996, 3.9632919499081067, 3.4893641762139924, 2.4511773825532037, 156.52974432229703, 863.747739455251, 615.7287062993696, 175.64238105897965, 151.79614614457512, 136.4655942714165, 91.81897182433428, 86.7556869374555, 83.21510747086532, 57.84014286830742, 47.97967610675338, 25.862446605494263, 21.02340355272161, 18.62481729261059, 16.200397150895324, 15.207816389175438, 10.072414904379569, 9.204241354461233, 6.493328610704143, 6.17142628512607, 3.1344120407235643, 2.314629856778276, 1.9359536981744796, 1.570112434022811, 1.3597605743761194, 1.0436810500547349, 1.043682118945113, 544.5902481725923, 315.56770006895323, 277.03317981904735, 237.78082904389262, 237.2961363383929, 226.53066034865122, 192.3273167975714, 69.1555685172358, 32.10716289430781, 29.432340857254502, 20.10156971699538, 17.643533404918923, 13.731689371259138, 12.805927421588382, 11.170811600693181, 8.876688988731962, 8.53370210147258, 4.897369716521145, 4.423769622562942, 2.0097581156655213, 1.6828562326697636, 1.0436810000304984, 1.0436807701421145, 1.0436807733080828, 1.04368079327639, 1.0436810395202782, 1.04368107130705, 1.0436817290925668, 1.0436813273686505, 1.0436817492568744, 1.043682118945113, 684.0162606960286, 229.99981312656536, 164.86426954216358, 156.4369105417919, 147.33274597810032, 128.83599113879302, 102.8190890253507, 74.15992314505367, 42.05112165554071, 35.663668133042535, 34.11698176944677, 32.1218449781632, 28.8114841537835, 23.7444003462855, 23.574499488270103, 21.595383560927136, 20.810794563086066, 18.774386421139724, 15.385559869507365, 13.344670678819245, 10.811920900757341, 5.003758228750777, 3.5069947679847036, 2.3777616894656304, 2.3695129056917263, 22.482972043765745, 601.0121803020392, 538.262337542942, 153.65264408846193, 89.13677893008426, 87.6143506408935, 73.19814110440272, 46.873923734684524, 45.43139391475895, 41.19745557166213, 30.48677311471437, 21.06697616438744, 15.371171534807889, 5.85786119369733, 25.929953807516753, 1.043681077719997, 1.0436807763048104, 1.0436807775219918, 1.0436807843532583, 1.0436807877284358, 1.0436807308722933, 1.0436807308722933, 1.0436807308722933, 1.0436807308722933, 1.0436807308722933, 1.0436807308722933, 1.043680940647262, 1.0436812936072364, 1.043680813042757, 1.0436809897657229, 1.043680829374716, 1.0436808299715308, 1.0436810701122743, 1.04368107130705, 1.0436810783463744, 1.0436811473298593, 1.0436817290925668, 1.0436814037064888, 1.0436816669133955, 1.0436817492568744, 1.0436815627543485, 1.0436814391824065, 802.1740840192617, 155.02709789556113, 135.58249833065446, 92.19536006129623, 84.3163424283894, 68.44706189803632, 39.28276147388811, 36.74732387861123, 34.9332423694825, 28.1049393580482, 28.06198263362409, 27.151006839729046, 25.223440593557175, 25.223440593557175, 15.101646821170577, 12.975014644531614, 12.519248882727508, 8.376812405502324, 6.903221762015979, 6.5052412487483595, 1.963351426715209, 1.913884018320402, 1.547071073934073, 234.1210451295581, 312.7758045920812, 437.0260074191837, 280.21557242694905, 150.2223815267725, 55.26967066859226, 40.59627417269488, 29.345000440576037, 26.754528827205835, 26.743920991409826, 24.88640636756144, 23.118479610430715, 13.377456799784333, 5.019667457615365, 34.89361026031662, 3.5187488996666185, 2.8162980397215835, 1.432602369691234, 1.0436808135177218, 1.0436807328959476, 1.0436807328959476, 1.0436807361618636, 1.0436807361618636, 1.0436807361618636, 1.0436807427589219, 1.0436807427589219, 1.0436807427589219, 1.0436810482691985, 1.0436813946577903, 1.0436810932254013, 1.0436809469127233, 1.0436818358222761, 1.0436810486821837, 1.0436810619158805, 1.043681114572878, 1.043682118945113, 1.0436812874371137, 1.0436817492568744, 1.0436814037064888, 67.11251404346355, 59.04005657373264, 37.67122661877532, 26.414529374598526, 22.892085143133738, 10.540750467847559, 5.471767454265897, 4.89450957716886, 4.583158768288053, 1.773546795561642, 1.1215489199569812, 1.1215485368610705, 1.1215483305786569, 1.0436808245020872, 1.0436807498927965, 1.0436807744403622, 1.0436807766478131, 1.043680777608003, 1.0436808496926355, 1.0436807785092945, 1.0436807847088765, 1.0436807284593321, 1.0436807284593321, 1.0436807284593321, 1.0436807284593321, 1.0436808004643567, 1.043680803860127, 1.0436808068754775, 1.043680817661162, 1.043681229359097, 1.043682118945113, 1.0436817210052018, 1.0436814391824065, 1.0436810460743084, 1.0436816669133955, 1.0436811898896172, 1.043681340073452, 1.0436813419740385, 1.0436817290925668, 1.0436817492568744, 1.0436818358222761], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\"], \"logprob\": [25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.4955, -4.6617, -4.7874, -4.8167, -4.9399, -5.0309, -5.1153, -5.1821, -5.2484, -5.2741, -4.7882, -5.3616, -5.4206, -5.4231, -5.4391, -5.492, -5.5014, -5.5122, -5.5151, -5.5361, -5.4918, -4.8339, -5.8664, -5.9018, -5.9758, -4.6624, -4.7934, -4.2195, -4.2433, -5.1835, -5.3891, -3.9989, -4.6372, -5.0213, -4.6809, -5.2276, -4.4872, -5.0365, -4.9207, -4.7303, -4.8843, -4.8664, -5.0052, -4.9974, -4.6465, -5.0316, -4.5304, -4.7833, -4.7956, -4.9565, -5.0258, -5.0355, -5.1087, -5.1737, -5.2061, -5.2096, -5.2246, -5.2756, -5.2833, -5.3557, -5.3637, -5.3907, -5.4031, -5.4235, -5.4676, -5.4809, -5.5299, -5.5441, -5.5583, -5.5785, -5.5781, -4.8542, -4.9827, -4.6988, -4.5918, -5.2786, -4.274, -4.298, -4.7547, -5.0507, -4.9463, -4.0531, -4.8993, -4.9084, -4.0902, -4.6202, -4.3224, -5.0509, -5.1216, -4.9957, -4.9534, -5.0985, -4.4147, -4.7081, -4.8144, -4.8987, -4.9139, -4.987, -5.0199, -5.1155, -5.1462, -5.1636, -5.2394, -5.2948, -5.3002, -5.3128, -5.3419, -5.4326, -5.4574, -5.48, -5.5378, -5.5482, -5.5496, -5.5731, -5.6054, -5.6202, -5.6286, -4.7184, -5.0619, -4.7195, -5.3278, -4.8191, -4.8428, -4.196, -4.2422, -4.6836, -4.8827, -4.9437, -4.4973, -4.3284, -4.7182, -4.4787, -5.0496, -5.0568, -4.9792, -4.811, -5.0163, -4.9759, -5.0425, -3.853, -3.9517, -3.9712, -4.0444, -4.1094, -4.166, -4.3108, -4.3478, -4.3664, -4.4029, -4.4615, -4.5975, -4.723, -4.772, -4.7767, -4.8106, -4.8244, -4.8995, -4.9488, -4.9834, -5.1564, -5.2387, -5.3001, -5.3834, -5.3988, -3.9978, -5.0699, -3.9913, -4.3429, -3.7937, -4.3329, -4.3949, -4.3295, -4.2222, -4.006, -4.7585, -4.2499, -4.2977, -4.3744, -4.4247, -2.8966, -4.0735, -3.5343, -4.9027, -5.0779, -5.4193, -5.5938, -5.7331, -6.0292, -6.0319, -6.0886, -6.1429, -6.3439, -6.3559, -6.4448, -6.4491, -6.5144, -6.5667, -6.6402, -6.7094, -6.719, -6.7295, -6.9189, -1.4225, -7.0537, -3.1477, -3.4537, -2.6719, -4.2875, -3.1825, -2.6856, -3.7207, -5.2123, -2.4351, -4.6183, -4.4144, -5.396, -5.0882, -3.9033, -3.9526, -4.4522, -4.8215, -5.3823, -5.3351, -4.8365, -4.5155, -3.0216, -3.7716, -4.0214, -4.101, -4.272, -4.3702, -4.4119, -4.512, -4.5265, -4.548, -4.5711, -4.5812, -4.6521, -4.6775, -4.6811, -4.7517, -4.7926, -4.7949, -4.8144, -4.8809, -4.8993, -4.9317, -4.9358, -4.9711, -5.0037, -4.7938, -4.5227, -4.6726, -4.6496, -4.6963, -4.7462, -4.7472, -4.7485, -4.7269, -4.729, -4.7388, -2.6041, -2.6745, -3.0667, -3.3089, -3.4558, -3.5972, -3.6555, -3.6971, -3.7438, -3.8413, -3.8798, -3.9161, -3.9267, -3.9457, -4.0742, -4.1351, -4.1828, -4.2471, -4.3654, -4.3787, -4.3946, -4.4048, -4.4057, -4.4459, -4.4807, -4.4009, -1.4351, -1.5991, -2.525, -2.8187, -3.0057, -3.0672, -3.2145, -3.2604, -3.6818, -4.1984, -4.3925, -4.4873, -5.0262, -5.0342, -5.1077, -5.1722, -5.1967, -5.2932, -5.4279, -5.4468, -4.635, -5.84, -5.9335, -6.0482, -6.1076, -5.8666, -2.7816, -2.9541, -3.0257, -3.128, -3.1704, -3.2204, -3.2932, -3.328, -3.4817, -3.5017, -3.562, -3.5865, -3.5943, -3.8579, -3.8592, -3.9265, -3.9389, -3.9762, -4.1132, -4.1587, -4.3165, -4.3338, -4.4244, -4.558, -4.5688, -2.295, -2.3834, -2.3945, -2.4574, -3.051, -3.0798, -3.2821, -3.6963, -3.7755, -3.8867, -3.9337, -3.9616, -4.1076, -4.2708, -4.474, -4.4785, -4.5053, -4.5235, -4.7712, -4.9896, -5.1344, -5.1453, -5.1701, -5.2503, -5.3092, -3.127, -3.7787, -3.5978, -4.4439, -2.9575, -3.1598, -3.2035, -3.3015, -3.3387, -3.4146, -3.4791, -3.4822, -3.522, -3.7288, -3.8354, -3.8938, -4.0453, -4.0734, -4.078, -4.1312, -4.2632, -4.2886, -4.355, -4.4105, -4.4622, -4.4896, -4.5426, -4.5647, -4.6345, -2.9368, -3.6835, -1.4024, -2.4964, -3.0512, -3.0679, -3.2857, -3.3478, -3.437, -3.6736, -3.7394, -3.8803, -3.9387, -3.9529, -4.0749, -4.145, -4.2286, -4.2388, -4.2586, -4.5345, -4.5659, -4.6136, -4.695, -4.7053, -4.789, -4.8248, -4.8284, -3.1021, -3.1206, -3.1607, -3.2457, -3.2696, -3.4585, -3.47, -3.6067, -3.685, -3.7283, -3.8071, -3.8548, -3.8909, -3.9763, -3.9818, -4.0407, -4.0554, -4.1455, -4.2518, -4.2626, -4.2994, -4.3099, -4.5335, -4.5979, -4.6118, -4.4647, -1.5588, -2.1015, -2.2939, -2.7858, -3.0488, -3.1631, -3.3102, -3.4134, -3.815, -3.9811, -4.0614, -4.0912, -4.1112, -4.2258, -4.3947, -4.537, -4.5378, -4.8876, -4.939, -5.2276, -5.4552, -5.6255, -5.7416, -6.0731, -6.2921, -1.4582, -2.9543, -2.9932, -3.2945, -3.3042, -3.346, -3.4265, -3.4434, -3.5653, -3.5943, -4.0843, -4.1362, -4.184, -4.2115, -4.3724, -4.7732, -4.8091, -4.8686, -4.8724, -5.1517, -5.1601, -5.283, -5.4841, -5.7957, -6.3479, -3.402, -4.2175, -1.9676, -2.5166, -2.5937, -2.9515, -2.9671, -3.0555, -3.307, -3.4635, -3.478, -3.8741, -3.8902, -4.1263, -4.3347, -4.3636, -4.4785, -4.5084, -4.5161, -4.5328, -4.5705, -5.3138, -5.893, -6.6711, -6.7178, -7.076, -7.184, -4.5713, -5.0934, -4.0688, -5.3899, -5.8154, -1.4488, -1.6175, -1.8624, -3.0522, -3.3678, -3.8117, -4.2088, -4.624, -4.8739, -6.4975, -6.7548, -6.7817, -6.7886, -6.8815, -7.8145, -7.894, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -1.668, -1.9489, -2.5258, -3.0844, -3.4036, -3.5268, -3.6424, -3.8059, -3.8157, -3.8399, -4.1026, -4.1829, -4.3648, -4.3937, -4.4486, -4.6359, -5.1211, -5.2035, -6.2399, -6.6658, -6.814, -6.9158, -7.0509, -7.2256, -7.767, -4.0269, -1.356, -1.695, -2.9534, -3.1003, -3.2075, -3.6073, -3.6647, -3.7069, -4.076, -4.2666, -4.903, -5.1196, -5.2472, -5.3952, -5.4628, -5.9116, -6.0123, -6.4138, -6.4742, -7.361, -7.8475, -8.1892, -8.6894, -9.1565, -11.4061, -11.4061, -1.7864, -2.3334, -2.464, -2.6174, -2.6195, -2.6661, -2.8306, -3.8628, -4.6472, -4.7371, -5.135, -5.2727, -5.5407, -5.6162, -5.7654, -6.0211, -6.0656, -6.7251, -6.8548, -8.0782, -8.471, -11.2962, -11.2962, -11.2962, -11.2962, -11.2962, -11.2962, -11.2962, -11.2962, -11.2962, -11.2962, -1.4773, -2.5701, -2.9048, -2.9576, -3.0179, -3.1531, -3.3806, -3.7112, -4.289, -4.4582, -4.5038, -4.566, -4.6785, -4.8796, -4.8871, -4.9789, -5.0177, -5.1262, -5.3378, -5.4909, -5.7206, -6.6175, -7.0862, -7.686, -7.6921, -6.1119, -1.4954, -1.6058, -2.8642, -3.4135, -3.4309, -3.613, -4.0665, -4.0985, -4.1986, -4.5086, -4.8935, -5.2275, -6.313, -5.6295, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -1.1535, -2.8025, -2.9375, -3.3267, -3.4171, -3.6284, -4.1949, -4.2634, -4.3155, -4.5402, -4.5418, -4.5761, -4.6527, -4.6527, -5.194, -5.3576, -5.3964, -5.8426, -6.0657, -6.1356, -7.8869, -7.9401, -8.4604, -4.4519, -4.2168, -1.4691, -1.9148, -2.5414, -3.553, -3.8683, -4.2027, -4.2986, -4.299, -4.374, -4.4509, -5.0319, -6.1587, -4.2637, -6.6281, -6.9571, -8.4149, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -2.3238, -2.4541, -2.9135, -3.2804, -3.4298, -4.2619, -5.0227, -5.1618, -5.2457, -6.8053, -8.86, -8.86, -8.86, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346], \"loglift\": [25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.7233, 0.7233, 0.7233, 0.7233, 0.7232, 0.7232, 0.7232, 0.7232, 0.7232, 0.7232, 0.7231, 0.7231, 0.7231, 0.7231, 0.7231, 0.7231, 0.7231, 0.7231, 0.7231, 0.7231, 0.723, 0.7229, 0.7229, 0.7229, 0.7228, 0.7212, 0.7214, 0.7147, 0.7118, 0.7212, 0.7223, 0.698, 0.7081, 0.7144, 0.7055, 0.7185, 0.6747, 0.7024, 0.6947, 0.67, 0.6828, 0.6803, 0.6852, 0.6686, 0.5308, 0.5222, 2.1912, 2.191, 2.191, 2.1908, 2.1907, 2.1907, 2.1906, 2.1906, 2.1905, 2.1905, 2.1905, 2.1904, 2.1904, 2.1903, 2.1903, 2.1902, 2.1902, 2.1902, 2.1901, 2.1901, 2.19, 2.19, 2.1899, 2.1899, 2.1899, 2.189, 2.1857, 2.1738, 2.1529, 2.1796, 2.1297, 2.1192, 2.1157, 2.1483, 2.1156, 1.924, 2.0729, 2.0593, 1.7824, 1.9423, 1.7669, 2.0019, 2.0221, 1.5721, 1.2974, 1.8915, 2.2342, 2.234, 2.2339, 2.2338, 2.2338, 2.2337, 2.2336, 2.2335, 2.2335, 2.2335, 2.2333, 2.2333, 2.2333, 2.2332, 2.2332, 2.233, 2.233, 2.2329, 2.2328, 2.2328, 2.2328, 2.2328, 2.2327, 2.2327, 2.2326, 2.231, 2.2313, 2.2262, 2.2309, 2.2214, 2.2201, 2.2015, 2.2007, 2.1892, 2.1999, 2.2036, 2.148, 2.0485, 2.048, 1.9231, 2.108, 2.0591, 1.8635, 1.3486, 1.8507, 1.5902, 1.3466, 2.3026, 2.3026, 2.3026, 2.3026, 2.3025, 2.3025, 2.3024, 2.3024, 2.3024, 2.3023, 2.3023, 2.3022, 2.3021, 2.302, 2.302, 2.302, 2.302, 2.3019, 2.3018, 2.3018, 2.3016, 2.3014, 2.3013, 2.3012, 2.3012, 2.2974, 2.2983, 2.2817, 2.2882, 2.2558, 2.2729, 2.2635, 2.2441, 2.221, 2.1645, 2.2794, 2.0183, 1.937, 1.7449, 1.3392, 2.4034, 2.4033, 2.4033, 2.4026, 2.4024, 2.4018, 2.4014, 2.401, 2.4, 2.4, 2.3997, 2.3995, 2.3985, 2.3985, 2.398, 2.3979, 2.3975, 2.3972, 2.3967, 2.3962, 2.3961, 2.396, 2.3944, 2.3931, 2.393, 2.3886, 2.375, 2.3665, 2.3442, 2.3061, 2.2562, 2.2923, 2.3265, 2.1475, 2.2804, 2.2601, 2.2922, 2.2443, 2.0555, 2.0601, 2.1306, 1.9662, 2.1569, 2.0906, 1.3715, 0.6618, 4.1872, 4.1859, 4.1852, 4.1849, 4.1843, 4.1839, 4.1837, 4.1832, 4.1831, 4.183, 4.1829, 4.1828, 4.1824, 4.1823, 4.1822, 4.1818, 4.1815, 4.1815, 4.1814, 4.1809, 4.1807, 4.1805, 4.1805, 4.1802, 4.1799, 3.9965, 3.6217, 3.3769, 3.1086, 3.2842, 3.2968, 3.1015, 3.0742, 2.1472, 2.138, 2.2714, 4.5115, 4.5114, 4.5109, 4.5104, 4.5101, 4.5097, 4.5096, 4.5094, 4.5093, 4.5089, 4.5088, 4.5087, 4.5086, 4.5086, 4.508, 4.5077, 4.5075, 4.5071, 4.5065, 4.5064, 4.5063, 4.5062, 4.5062, 4.506, 4.5057, 4.1312, 4.6178, 4.6177, 4.6171, 4.6167, 4.6164, 4.6163, 4.616, 4.6159, 4.6147, 4.6124, 4.6111, 4.6104, 4.605, 4.6048, 4.6038, 4.6029, 4.6025, 4.6009, 4.5985, 4.5981, 4.5955, 4.5886, 4.5858, 4.5819, 4.5797, 4.4716, 4.7514, 4.7511, 4.751, 4.7508, 4.7507, 4.7505, 4.7504, 4.7503, 4.7498, 4.7497, 4.7495, 4.7494, 4.7494, 4.7483, 4.7483, 4.748, 4.7479, 4.7477, 4.747, 4.7467, 4.7456, 4.7455, 4.7448, 4.7436, 4.7435, 4.8726, 4.8725, 4.8725, 4.8724, 4.8714, 4.8713, 4.8708, 4.8692, 4.8689, 4.8683, 4.868, 4.8679, 4.8669, 4.8657, 4.864, 4.8639, 4.8636, 4.8635, 4.8606, 4.8574, 4.8549, 4.8547, 4.8542, 4.8526, 4.8513, 4.8476, 4.7339, 4.4366, 4.0418, 4.9187, 4.9182, 4.9181, 4.9178, 4.9177, 4.9174, 4.9172, 4.9172, 4.917, 4.9161, 4.9156, 4.9152, 4.9143, 4.9141, 4.9141, 4.9137, 4.9127, 4.9125, 4.9119, 4.9114, 4.9109, 4.9106, 4.91, 4.9098, 4.909, 4.8958, 4.8652, 5.0219, 5.0209, 5.0197, 5.0197, 5.019, 5.0188, 5.0184, 5.0173, 5.017, 5.0162, 5.0158, 5.0157, 5.0148, 5.0143, 5.0136, 5.0135, 5.0133, 5.0104, 5.01, 5.0094, 5.0083, 5.0082, 5.0069, 5.0064, 5.0063, 5.065, 5.065, 5.0649, 5.0646, 5.0645, 5.0638, 5.0637, 5.0631, 5.0627, 5.0624, 5.062, 5.0617, 5.0614, 5.0608, 5.0608, 5.0604, 5.0603, 5.0595, 5.0586, 5.0585, 5.0581, 5.058, 5.0555, 5.0547, 5.0545, 4.856, 5.1947, 5.1942, 5.1939, 5.1929, 5.1922, 5.1918, 5.1912, 5.1908, 5.1884, 5.1872, 5.1865, 5.1862, 5.186, 5.1849, 5.183, 5.1811, 5.1811, 5.1751, 5.174, 5.167, 5.1598, 5.1534, 5.1483, 5.1304, 5.1151, 5.5375, 5.5342, 5.5341, 5.5325, 5.5325, 5.5322, 5.5317, 5.5316, 5.5307, 5.5305, 5.5255, 5.5249, 5.5242, 5.5238, 5.5213, 5.5129, 5.512, 5.5104, 5.5103, 5.5015, 5.5011, 5.4964, 5.4872, 5.4692, 5.4211, 4.877, 4.93, 5.7802, 5.7787, 5.7785, 5.7769, 5.7768, 5.7763, 5.7746, 5.7733, 5.7732, 5.7688, 5.7686, 5.765, 5.7611, 5.7604, 5.7578, 5.7571, 5.7569, 5.7565, 5.7555, 5.7269, 5.6855, 5.5826, 5.5739, 5.4961, 5.4681, 5.3803, 5.1268, 4.026, 4.7761, 4.3947, 5.7864, 5.7861, 5.7857, 5.7816, 5.7794, 5.7749, 5.7688, 5.7593, 5.7514, 5.6164, 5.5713, 5.566, 5.5646, 5.5453, 5.2589, 5.2254, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 5.8316, 5.8311, 5.8294, 5.8267, 5.8243, 5.8231, 5.8219, 5.82, 5.8198, 5.8195, 5.8154, 5.814, 5.8102, 5.8095, 5.8082, 5.8032, 5.7849, 5.7809, 5.6923, 5.6249, 5.5954, 5.5729, 5.5403, 5.493, 5.3048, 4.8882, 5.851, 5.8506, 5.8465, 5.8456, 5.8448, 5.8412, 5.8405, 5.84, 5.8347, 5.831, 5.8125, 5.8032, 5.7967, 5.7881, 5.7837, 5.7469, 5.7364, 5.6838, 5.6742, 5.4649, 5.2816, 5.1185, 4.8278, 4.5046, 2.5195, 2.5195, 5.882, 5.8806, 5.8802, 5.8796, 5.8796, 5.8794, 5.8786, 5.8692, 5.8521, 5.8491, 5.8326, 5.8253, 5.8079, 5.8022, 5.7897, 5.7639, 5.7587, 5.6545, 5.6266, 5.1921, 4.9769, 2.6294, 2.6294, 2.6294, 2.6294, 2.6294, 2.6294, 2.6294, 2.6294, 2.6294, 2.6294, 5.9631, 5.9602, 5.9585, 5.9581, 5.9577, 5.9568, 5.9548, 5.951, 5.9404, 5.936, 5.9347, 5.9328, 5.9291, 5.9214, 5.9211, 5.917, 5.9152, 5.9096, 5.8971, 5.8864, 5.8672, 5.7407, 5.6274, 5.4162, 5.4136, 4.7437, 6.0744, 6.0742, 6.0695, 6.0647, 6.0645, 6.0622, 6.0544, 6.0537, 6.0513, 6.0425, 6.0271, 6.0084, 5.8876, 5.0835, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 6.1275, 6.1222, 6.1213, 6.1177, 6.1167, 6.1139, 6.1027, 6.1009, 6.0994, 6.0922, 6.0921, 6.0909, 6.0879, 6.0879, 6.0596, 6.0478, 6.0447, 6.0003, 5.9706, 5.9601, 5.4068, 5.3791, 5.0716, 4.0606, 4.0061, 6.4193, 6.418, 6.4148, 6.4031, 6.3963, 6.3865, 6.383, 6.383, 6.3801, 6.3768, 6.3429, 6.1962, 6.1524, 6.0821, 5.9759, 5.194, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 7.4382, 7.4361, 7.426, 7.414, 7.4078, 7.3512, 7.246, 7.2184, 7.2002, 6.59, 4.9936, 4.9936, 4.9936, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391]}, \"token.table\": {\"Topic\": [3, 5, 2, 1, 2, 4, 6, 1, 5, 6, 5, 19, 21, 12, 20, 21, 21, 5, 18, 1, 2, 6, 24, 24, 18, 1, 2, 4, 6, 16, 1, 2, 4, 6, 22, 2, 6, 21, 19, 5, 6, 11, 12, 1, 3, 1, 5, 7, 8, 21, 3, 3, 1, 4, 18, 10, 16, 13, 6, 9, 1, 2, 4, 25, 5, 13, 1, 3, 4, 6, 5, 12, 6, 8, 9, 20, 12, 16, 1, 3, 6, 1, 11, 3, 5, 8, 16, 16, 23, 21, 6, 7, 9, 1, 6, 14, 17, 3, 18, 12, 24, 19, 1, 3, 15, 13, 16, 12, 5, 6, 11, 21, 13, 1, 5, 1, 4, 1, 3, 18, 10, 5, 10, 19, 10, 9, 6, 21, 12, 6, 5, 17, 20, 12, 4, 6, 9, 11, 16, 6, 1, 2, 12, 3, 4, 10, 18, 13, 23, 22, 2, 7, 1, 3, 2, 1, 4, 1, 3, 5, 20, 8, 21, 16, 3, 11, 20, 17, 12, 22, 12, 6, 10, 9, 23, 1, 3, 5, 4, 5, 2, 4, 1, 16, 4, 6, 3, 5, 13, 1, 4, 14, 13, 2, 1, 6, 4, 18, 5, 19, 15, 15, 11, 16, 6, 20, 4, 10, 21, 18, 8, 2, 6, 20, 17, 16, 16, 21, 6, 12, 17, 10, 11, 21, 20, 14, 1, 5, 23, 5, 1, 3, 5, 7, 1, 4, 1, 4, 24, 3, 1, 6, 13, 8, 14, 9, 18, 10, 4, 6, 23, 10, 20, 6, 10, 1, 17, 12, 9, 20, 12, 19, 6, 17, 14, 16, 15, 21, 9, 6, 8, 24, 21, 21, 4, 1, 3, 4, 21, 15, 3, 22, 3, 3, 1, 3, 4, 1, 16, 1, 5, 9, 1, 2, 3, 3, 3, 19, 7, 14, 1, 2, 4, 16, 22, 9, 17, 2, 8, 13, 1, 20, 2, 9, 10, 5, 7, 11, 12, 5, 4, 1, 12, 14, 14, 1, 2, 3, 13, 23, 1, 1, 19, 4, 9, 16, 7, 1, 2, 10, 25, 11, 4, 12, 17, 7, 1, 5, 19, 9, 13, 1, 5, 1, 3, 4, 5, 6, 9, 7, 22, 19, 1, 3, 18, 1, 6, 22, 16, 1, 2, 5, 1, 2, 2, 16, 24, 1, 5, 6, 16, 1, 20, 24, 6, 4, 5, 6, 1, 5, 25, 2, 19, 18, 15, 8, 1, 2, 2, 15, 18, 20, 3, 17, 10, 12, 4, 9, 16, 23, 20, 1, 4, 21, 1, 24, 23, 9, 23, 11, 13, 15, 5, 22, 3, 3, 4, 23, 8, 13, 6, 11, 7, 15, 10, 5, 1, 4, 10, 14, 17, 5, 17, 20, 23, 11, 1, 7, 15, 1, 2, 16, 2, 23, 13, 17, 16, 19, 8, 6, 2, 6, 15, 8, 1, 8, 11, 8, 1, 5, 1, 2, 3, 4, 14, 1, 8, 19, 13, 12, 14, 1, 4, 5, 6, 15, 2, 1, 2, 1, 4, 5, 5, 6, 18, 25, 1, 1, 3, 1, 2, 1, 2, 3, 4, 7, 4, 6, 7, 14, 1, 2, 3, 23, 16, 6, 13, 10, 12, 2, 20, 15, 1, 4, 18, 1, 2, 6, 2, 4, 14, 19, 15, 6, 1, 6, 11, 4, 3, 5, 24, 11, 2, 6, 3, 6, 1, 2, 4, 6, 19, 1, 4, 18, 1, 2, 19, 13, 17, 10, 5, 12, 2, 2, 3, 2, 24, 1, 5, 4, 12, 14, 1, 2, 3, 4, 13, 16, 9, 5, 8, 12, 6, 7, 19, 5, 4, 11, 8, 12, 4, 11, 7, 11, 25, 11, 5, 22, 1, 5, 8, 1, 2, 4, 25, 3, 25, 18, 9, 18, 6, 4, 8, 3, 1, 11, 3, 1, 4, 13, 16, 13, 9, 24, 13, 9, 19, 9, 15, 1, 4, 15, 9, 6, 13, 1, 3, 23, 15, 23, 10, 2, 3, 4, 24, 19, 6, 1, 5, 1, 2, 18, 7, 1, 4, 5, 6, 11, 22, 4, 8, 7, 3, 1, 2, 3, 4, 6, 21, 7, 9, 15, 18, 15, 5, 7, 3, 15, 1, 3, 8, 8, 14, 10, 8, 14, 10, 11, 1, 22, 10, 10, 5, 2, 11, 20, 15, 23, 2, 1, 3, 6, 2, 22, 1, 3, 14, 14, 12, 21, 3, 20, 14, 17, 7, 21, 18, 1, 11, 19, 9, 8, 5, 6, 3, 5, 2, 6, 7, 1, 2, 9, 14, 25, 21, 4, 3, 6, 11, 4, 7, 13, 2, 13, 19, 4, 5, 23, 1, 4, 1, 2, 13, 1, 3, 5, 6, 7, 22, 18, 8, 10, 3, 5, 14, 4, 7, 15, 18, 11, 1, 3, 4, 6, 10, 15, 23, 6, 9, 8, 1, 23, 1, 20, 18, 1, 7, 14, 24, 1, 5, 6, 4, 11, 21, 21, 15, 13, 5, 7, 1, 15, 16, 14, 20, 1, 2, 3, 17, 7, 18, 14, 19, 3, 5, 11, 16, 4, 3, 21, 23, 2, 4, 25, 6, 8, 10, 23, 16, 24, 3, 11, 4, 16, 3, 4, 5, 3, 22, 6, 9, 24, 1, 2, 4, 2, 7, 1, 2, 3, 16, 1, 3, 5, 10, 25, 7, 2, 9, 21, 15, 2, 13, 19, 1, 2, 4, 1, 3, 5, 1, 6, 1, 5, 21, 23, 15, 18, 18, 20, 2, 23, 14, 23, 23, 13, 6, 10, 6, 10, 10, 10], \"Freq\": [0.07259292120849857, 0.9259630394150707, 0.9985147430605898, 0.29037037180089736, 0.6538065607563881, 0.011735960268354391, 0.043536626801959834, 0.004667636489019755, 0.9714518442772366, 0.023629909725662514, 0.9964214652500295, 0.43203452036685297, 0.4205635932441726, 0.9901439949616422, 0.9989161609584951, 0.7993991350374712, 0.9908147601687134, 0.9934264749465609, 0.6831845281855294, 0.19796890596337352, 0.7651413613387821, 0.03688651410257729, 0.9643819057492832, 0.9541659423962293, 0.9612687722442463, 0.0021019719237473657, 0.9295970832772724, 0.005254929809368414, 0.06253366473148413, 0.9923914298279972, 0.0020738327250026272, 0.9399646826074408, 0.0015553745437519703, 0.05599348357507093, 0.8535538543282092, 0.887898068722325, 0.1116737777155914, 0.9947576904045752, 0.6368970643954991, 0.7810643310456258, 0.21785885266768487, 0.9836072674205811, 0.9955673953616373, 0.008022812217855994, 0.9912630206950962, 0.23876874915848376, 0.760959104460934, 0.9976469280774621, 0.9994863072389824, 0.9610397113561323, 0.9987416448390072, 0.9980625166714707, 0.038125340135369265, 0.9611598907811514, 0.9978378476886746, 0.7230226350216624, 0.25655641887865444, 0.9946067523546631, 0.9950998623239676, 0.9952280957154356, 0.02273544131109824, 0.03379592627325414, 0.9425991073303973, 0.9610308481051011, 0.9967190408293364, 0.9946904337026105, 0.9716298564233479, 0.0016500264422521676, 0.003300052884504335, 0.023100370191530346, 0.9995395489989773, 0.9865742867362092, 0.10605203252513924, 0.8749292683323988, 0.9944388801187986, 0.9832903041358441, 0.996594027565298, 0.9835460015054736, 0.11789966615254566, 0.8803909650082615, 0.9960517704447311, 0.999456874399144, 0.9882953878875091, 0.14569025941740743, 0.8531411587505842, 0.4476635168640912, 0.5183472300531582, 0.9947782852913146, 0.9576072133706356, 0.4220276655163743, 0.9973593297844491, 0.9970929479973016, 0.9956714114495766, 0.5537584647613516, 0.4435444013865195, 0.9968612815263704, 0.9989928874103823, 0.9980950170300157, 0.9847140700639639, 0.9969960326183885, 0.9516196726913618, 0.9853979943330513, 0.03289071340642397, 0.9662321053165862, 0.8869367015050602, 0.9929263806979854, 0.9633473210470533, 0.9835563079515219, 0.8837162975752223, 0.11567412557288761, 0.9948762806462067, 0.9750037189459317, 0.9947135954269944, 0.0003252775532228367, 0.9992526435005542, 0.9909402629379709, 0.008771735160126888, 0.9989566536889735, 0.0007740849699255897, 0.9799414183195223, 0.9961214926146679, 0.9980329527774338, 0.9956568328159692, 0.8101854853311045, 0.996526948297844, 0.9957892162159007, 0.9903155721907608, 0.99095417675648, 0.9921280566511659, 0.9979735362803025, 0.9957662100572154, 0.7910645933723259, 0.9930986569163837, 0.9896645003790198, 0.9539057157442292, 0.04583986277031765, 0.048711900693781354, 0.9476678862244736, 0.9964796659403984, 0.9971185178850891, 0.005569276115937708, 0.9935588590832871, 0.9774791176369102, 0.029192462095715577, 0.9703320553119373, 0.9994183309341798, 0.9989216793899217, 0.9942517113403504, 0.9985363725372689, 0.9107958992128296, 0.9983943062642872, 0.9951195673167631, 0.014032243089497017, 0.9852639254982548, 0.9981102853474406, 0.30607364799924036, 0.6935917860240297, 0.13426893916103746, 0.8298426164689685, 0.03533393135816776, 0.9967161816743919, 0.9892550027350261, 0.9920336871964626, 0.9932967430433592, 0.9992237780877722, 0.9946442689714168, 0.9467152692231309, 0.97072864891264, 0.9948732962832842, 0.9929880135343175, 0.9958594911588772, 0.9955630212450556, 0.9766119011028098, 0.9883879617542389, 0.9933747202295365, 0.011850939471736611, 0.9671397125412877, 0.02061032951606367, 0.2679447052992168, 0.7303654063801234, 0.014318935136553704, 0.9854030816701049, 0.9994227732632464, 0.9958419379314041, 0.659719550872526, 0.3364569709449883, 0.9988708402178156, 0.9934723787275656, 0.9963626688095036, 0.4275510598467394, 0.5722786540101886, 0.9996196830941714, 0.9978675600222968, 0.9991870171859382, 0.9626802216045207, 0.036827364182186365, 0.9995655826476967, 0.9964130422114738, 0.996921331198742, 0.9947551623358287, 0.9701931971425355, 0.947887783880021, 0.3061890642245619, 0.6664114927240464, 0.9972773187161369, 0.9451998161086876, 0.9994677220851111, 0.9725491540018925, 0.9686494358489053, 0.9355254825835118, 0.9773460617561802, 0.5921324924604799, 0.40395479358532743, 0.9513344567392281, 0.98773529876098, 0.9844844669361367, 0.9816726738742964, 0.9718346979471053, 0.9950966582110026, 0.9794818110726864, 0.8561905843477303, 0.9941673529850709, 0.9921032514319, 0.8554332693584119, 0.9012369375738152, 0.9447329271336035, 0.05730409782414144, 0.9422616994492349, 0.5224977012335293, 0.9976454200093232, 0.2519965257096151, 0.7321870305525777, 0.015005953276342581, 0.9978127101724759, 0.02047124347749932, 0.9789803547461898, 0.1289371075345395, 0.8705307897236424, 0.8525757550600536, 0.9976847043977891, 0.8519164424678439, 0.14737393815012925, 0.9883627356579132, 0.9917330873879525, 0.9909221124512397, 0.6069134042945449, 0.3897022911786025, 0.9987703607822774, 0.9994937964190528, 0.9921398940692141, 0.6463827143100047, 0.9856982379283374, 0.997657445805197, 0.5613278464134512, 0.43658832498823985, 0.9991192599625333, 0.9991611300307301, 0.9906817640023584, 0.9973478637509269, 0.996270555679568, 0.9978179082309486, 0.9910805816264081, 0.9941889725356982, 0.782683173682792, 0.9791410054634009, 0.9861605220368632, 0.9376072302736278, 0.9724300538933528, 0.9965325876640782, 0.9945112118923798, 0.999521853042496, 0.9770277142376068, 0.924905027681023, 0.9672602407506319, 0.9988788904540623, 0.0010905303072077986, 0.969481443107733, 0.028353787987402766, 0.9843591646827234, 0.983313691553327, 0.9981947561337398, 0.9709337493045127, 0.9980515873748104, 0.9990653825679631, 0.0025238778578237035, 0.9969317538403628, 0.9978327398794603, 0.8241501088297613, 0.17158073938094007, 0.9465779281480722, 0.053303890663131355, 0.996835953793446, 0.05011974375050099, 0.8757286135314809, 0.07381271352346509, 0.9981342975181686, 0.9981599727033919, 0.9259032269570636, 0.9925096954314448, 0.9957578155392863, 0.9846343966381664, 0.004497690465184389, 0.010794457116442533, 0.9686953840340952, 0.9836315364526291, 0.9932894715602545, 0.991130606433257, 0.9991419871132161, 0.9974067145185961, 0.9978348474844698, 0.9996251884454466, 0.8951901041263172, 0.9983692788271018, 0.997201245498822, 0.9898529320439261, 0.9988448305841368, 0.9964617030464694, 0.9984859526681974, 0.9923670217229692, 0.9980951922346886, 0.9995127417322892, 0.9998390123107308, 0.9834217675480171, 0.9839189511159334, 0.9891175640299786, 0.02607540189526249, 0.1437872161653046, 0.8299427917520691, 0.9960186193745215, 0.9514958877628423, 0.999328904165914, 0.9996763533690329, 0.9666525515296359, 0.9996544386316244, 0.9944039488215848, 0.7479987028313801, 0.9957612674058245, 0.00196562822007186, 0.997556321686469, 0.9829920993455272, 0.9823838825013022, 0.9939575107567865, 0.9988435430923325, 0.9992198287950288, 0.9987143902304625, 0.9949481628094552, 0.03653282923921357, 0.9632224578759317, 0.9205792364750581, 0.9916669554990888, 0.987539785485416, 0.010677524877173694, 0.9892687829632533, 0.24489226581098142, 0.011297607920723702, 0.0009968477577109148, 0.7057682124593277, 0.03655108444940021, 0.9917462387486143, 0.9961826017031347, 0.968493286438787, 0.9963426762088591, 0.4744911164798658, 0.5251116340477693, 0.9677747385269437, 0.9912389004125564, 0.008587318112104928, 0.9872468026809011, 0.9778195869922576, 0.9949998163279123, 0.004592811549156206, 0.9969389471742724, 0.9977093575082858, 0.0020563618011225676, 0.9974913245485992, 0.7425129281580559, 0.9956620097290776, 0.13303467137317354, 0.8626222864648958, 0.004206154921464384, 0.9920863174277869, 0.9997144059300355, 0.9982010197215077, 0.9717980895092954, 0.9958991068007066, 0.06349020631596668, 0.8944752823520167, 0.04174217984309411, 0.0003438608626347055, 0.9994315972477715, 0.9834231505208841, 0.9982463537616072, 0.5165412793410074, 0.9817103196677857, 0.9522091182585175, 0.9956793798929282, 0.037913424682061665, 0.9616885991469103, 0.9989066923413908, 0.9643536418713741, 0.8414094041560484, 0.9374594876729428, 0.9988773598161185, 0.9797534284670253, 0.9924210568002143, 0.9935030870811191, 0.998927032098239, 0.9872673451336952, 0.9869790504446146, 0.924854447471276, 0.678154663547314, 0.24720637198843334, 0.752032571195582, 0.993511198762057, 0.9998219761697167, 0.7968655361684523, 0.9957037350850854, 0.9958130232700709, 0.9870344878473115, 0.18208807113891093, 0.8050209460878168, 0.9815260656661003, 0.9924558843787975, 0.9976547912515963, 0.9987357721072417, 0.998763609814308, 0.9992955341196939, 0.958523958778078, 0.9886480485913411, 0.9864598506057055, 0.9947038415812658, 0.9959347045892151, 0.9974809256241766, 0.9808199594424912, 0.9847675912921142, 0.9909992737984498, 0.005116920025962859, 0.9945431832280538, 0.9953325636277535, 0.9970944044734673, 0.42788428973511883, 0.9946337406058088, 0.9538685913057159, 0.9945378953134553, 0.9788586703664217, 0.9967473277947356, 0.9997361151696529, 0.4802252128118363, 0.5174257574662743, 0.0174007996653744, 0.981902266831841, 0.9810749458869549, 0.9976378702599198, 0.9606852253274197, 0.994651759209101, 0.774174695324403, 0.996068180184355, 0.9892603386279065, 0.997871667176201, 0.991344591104875, 0.8442695120358769, 0.15538036064495545, 0.9923365307324815, 0.953258304237046, 0.0175135070487093, 0.9807563947277209, 0.9951398535324303, 0.997667945501357, 0.9747914931598091, 0.025117673101106835, 0.06835592112476854, 0.07720198150562094, 0.6899927097064872, 0.1640542106994445, 0.9912555530074965, 0.999790751167402, 0.9911364634062999, 0.8691645179558937, 0.9942338979039278, 0.9964620688332302, 0.986272487679287, 0.9576469733786471, 0.003080665815096751, 0.01694366198303213, 0.02200475582211965, 0.9952145047388701, 0.9993035797375895, 0.010417367850482941, 0.9881617503886676, 0.9979731844871564, 0.0006394531297440132, 0.0010657552162400218, 0.9847219103630028, 0.01502561075078443, 0.4079672108259977, 0.8172422460175729, 0.9997777327894785, 0.08252035497442574, 0.9169661844758188, 0.99985961760897, 0.9990950777828015, 0.9975950233352583, 0.0006295961018209266, 0.0006295961018209266, 0.00094439415273139, 0.9972795938661707, 0.6707248785637554, 0.3278623208314843, 0.9952894732950694, 0.9817583345538454, 0.46062723484101487, 0.5382129892505757, 0.0006109114520437863, 0.8691593877244636, 0.9968135352462489, 0.9943733937702764, 0.994493702389862, 0.9933431014949203, 0.9949601237466373, 0.9992302097672615, 0.5942278256375781, 0.9913678814780912, 0.07820545201025181, 0.9212062898862765, 0.756946507579276, 0.5308641828053152, 0.40893896395480694, 0.06007264432415553, 0.7408006019536669, 0.25811542986310154, 0.9474645934624039, 0.6380782022322469, 0.9666138041099726, 0.9913084912522635, 0.9820677277256162, 0.017620608208858163, 0.989757051889127, 0.9986139063600872, 0.13321002748372024, 0.8658651786441816, 0.9976522966556551, 0.9951907129873525, 0.9979101388383802, 0.9961820977500976, 0.587611894084194, 0.4113283258589358, 0.9601233817284485, 0.003144263706245211, 0.014598367207567053, 0.02200984594371648, 0.9664524337181827, 0.9792798805585627, 0.0205350083886191, 0.9753825353391035, 0.9995701514321471, 0.9984814217345847, 0.9795814355942373, 0.9925354816436196, 0.7954222675826373, 0.9993070048242413, 0.989283336803327, 0.9900585366794794, 0.9992820386636753, 0.9577930484161215, 0.04134358482371747, 0.9976160711756259, 0.9853120961259065, 0.9523449807092325, 0.04747497387715469, 0.9992333977520463, 0.9969920932369681, 0.984777163361578, 0.10463109088963365, 0.05284398529779478, 0.8391624865289811, 0.0021137594119117913, 0.6026047639025741, 0.35709911934967353, 0.993754330469352, 0.9893221649006414, 0.980486812652153, 0.988996124618805, 0.5675059957351481, 0.428585257195815, 0.985474744240859, 0.9927330753804876, 0.9993241396068324, 0.996881822241126, 0.9991964205276657, 0.9907367474981859, 0.9989700613208028, 0.9961351715352299, 0.998659454132973, 0.9899448742656561, 0.9464488140394882, 0.9858021614081622, 0.9942626721510597, 0.9512321914451227, 0.22605962365061016, 0.7738388453251399, 0.9969627491127775, 0.29204139106880556, 0.6639521396115433, 0.04389766212307725, 0.7310252187127437, 0.9984275787396254, 0.9486990542564299, 0.9926507883068096, 0.9967316304426883, 0.5731703253083853, 0.9938055678560158, 0.9991842540655334, 0.9980334272685815, 0.9990775326856328, 0.9996558405425287, 0.9948872895894179, 0.9993442600920412, 0.9991848249276186, 0.9983490981208409, 0.9902671361361339, 0.9980040940702893, 0.9914937761567064, 0.20060979496755812, 0.7737806377320099, 0.9916034198894138, 0.9977122053604871, 0.9912894824059166, 0.45135905204648286, 0.5480788489135864, 0.002720348099828891, 0.995647404537374, 0.9966875678044999, 0.9983615760333951, 0.9985100641617178, 0.990146038967634, 0.012741897908527713, 0.9870070149144159, 0.9621558231473063, 0.9990122054499445, 0.984388051112305, 0.9885764603377978, 0.001645538302772404, 0.9955506731773044, 0.9982226964506807, 0.7101521116698707, 0.9991343080611446, 0.9977460078273629, 0.8247548575803262, 0.17520147444819026, 0.2205975351829324, 0.779080000287932, 0.9776298608221556, 0.9950396871999624, 0.07370674398098544, 0.0031143694639853, 0.9069043879125194, 0.01619472121272356, 0.9935288104803642, 0.9983158738953835, 0.9997444829842953, 0.957679849189688, 0.9941358690637292, 0.9977260886796503, 0.1672657496720716, 0.008239692102072493, 0.6814225368413952, 0.014007476573523239, 0.1285391967923309, 0.9813909177663179, 0.9984498219435498, 0.9965403483755304, 0.9865249100524677, 0.9870268082993465, 0.990270526638471, 0.9944569340902979, 0.9948244354326831, 0.9986611631368073, 0.948801409077909, 0.9996459427535377, 0.9986564597395118, 0.9812828647062005, 0.9584880723673985, 0.9923533953652376, 0.9877912162921729, 0.9882346963554748, 0.9960127968029827, 0.9982104181816911, 0.9962753140102062, 0.999800959630589, 0.9957524708257791, 0.9895345338451601, 0.9969650648911026, 0.9954768379022143, 0.9995917580340078, 0.9971347464190279, 0.816764963957307, 0.9853554709974759, 0.9514958877628423, 0.998467214370192, 0.8572514234812258, 0.0116181219058443, 0.13028893851553963, 0.998142456631095, 0.9493531413306906, 0.04392780522286993, 0.9556369702258307, 0.9969018449215521, 0.9981209582295746, 0.9855544653434399, 0.8992353793373473, 0.9993057104696711, 0.9655166388275278, 0.9736375172796143, 0.9953601463982001, 0.9954176286115562, 0.9650753255634648, 0.9868428200087609, 0.9994790541664013, 0.989261315346291, 0.7700210939205485, 0.9976040508654562, 0.9899258525705413, 0.6589151746278553, 0.3395020259515474, 0.10301699909288717, 0.8934383375874032, 0.9989848270479555, 0.9935504813548446, 0.9965500514876221, 0.07276105387699533, 0.9268766067739972, 0.9966172125208589, 0.9857702288869298, 0.5638419028483107, 0.9099441371481447, 0.9985114007808804, 0.9987348102737792, 0.0238035517207435, 0.9759456205504835, 0.9990138084764114, 0.6838713725775315, 0.31362254029033293, 0.9978093025228391, 0.9969376494948192, 0.9513207483195971, 0.9765352762289851, 0.02248279414741407, 0.9796631754442883, 0.8176033914302736, 0.18218215986871372, 0.17166263983791857, 0.8265605721675501, 0.9980242434399118, 0.023845827813415814, 0.23109412542707386, 0.7090627035106879, 0.03541806778169114, 0.5784815550134801, 0.3856543700089867, 0.9729032970167178, 0.9976363653265853, 0.9811166065947176, 0.3539281718473448, 0.6454432037130073, 0.9979236089734169, 0.9995106878512541, 0.9934272314130471, 0.9969591190496869, 0.988306003116761, 0.9950712081746328, 0.5369530373726306, 0.4122597072112103, 0.05036473270363554, 0.35218984567070444, 0.6461227243883599, 0.9884105342493597, 0.7686110028528188, 0.9917077564437242, 0.9883878570090783, 0.9898366360699132, 0.999779752144324, 0.9270512127441486, 0.9996487165504656, 0.9370660636238075, 0.9137654334302808, 0.9997009252725169, 0.9964094316392165, 0.966681905024791, 0.9721835481173918, 0.9883942618890299, 0.0020703181927197523, 0.009377323578789466, 0.998535963789381, 0.9954909330493279, 0.9587530370490418, 0.9985142740685807, 0.9945196148096426, 0.9943113599702889, 0.9893797582020487, 0.9919692357784556, 0.9995414173169477, 0.9938788337222841, 0.8044100401024805, 0.9962368007687863, 0.49757231589476875, 0.5564294761358535, 0.031679180459525176, 0.4113183914502866, 0.4086115126410932, 0.9937387968859017, 0.9683446219711843, 0.9913536602963945, 0.8935295145642508, 0.9958566435639731, 0.0026842497131104397, 0.9939998837720395, 0.9000732921858664, 0.9993716876243244, 0.6671715808212872, 0.3113467377166007, 0.9732849771111491, 0.926241098525836, 0.07209660442579481, 0.9821819813416752, 0.994890587006978, 0.9862847189278281, 0.8713441369062329, 0.1281388436626813, 0.9690897680207898, 0.8970314895872779, 0.9981183570496822, 0.9842392912170916, 0.9989594444810346, 0.9846456030215176, 0.9660788534217526, 0.007156139654975946, 0.026579947289910654, 0.9993695247365241, 0.9813558655846457, 0.993223206162807, 0.9986098421129213, 0.991862853495272, 0.9995726164915097, 0.00021240387090767313, 0.999560026194471, 0.99920089132086, 0.9951511749628519, 0.999455261047882, 0.00044459753605332827, 0.9985791916812047, 0.9164560142994869, 0.9623766677372247, 0.027184346211162367, 0.009908313105003105, 0.9966635821887184, 0.8727605134862304, 0.991163464398462, 0.9979279484993421, 0.9959754845620614, 0.9756304693316626, 0.9927132484834947, 0.9990217384886632, 0.9970513493107932, 0.9988165140070385, 0.61799182669681, 0.000273447710927792, 0.38145955674426985, 0.3849521851920965, 0.2583400046804103, 0.35596353780221546, 0.9613432754005179, 0.03825785081982322, 0.9479458289214473, 0.05189276344406559, 0.9956529828743157, 0.9673454353574206, 0.9858821410286271, 0.7472963038001539, 0.9847590417371475, 0.9635258204720201, 0.8760268408783979, 0.11829559530109753, 0.92656212203901, 0.5093331669476274, 0.835640057476044, 0.9884434895785673, 0.8232364644026258, 0.1691581776169779, 0.022449657946970996, 0.9753129174739622, 0.9945347130047077, 0.9892275332176984], \"Term\": [\"&\", \"&\", \"0\", \"1\", \"1\", \"1\", \"1\", \"10\", \"10\", \"10\", \"11\", \"123\", \"1300\", \"14\", \"15\", \"160\", \"200\", \"2021\", \"220\", \"3\", \"3\", \"3\", \"3070\", \"3080\", \"34\", \"4\", \"4\", \"4\", \"4\", \"45\", \"5\", \"5\", \"5\", \"5\", \"50fp\", \"6\", \"6\", \"70\", \"74\", \"9\", \"9\", \"95\", \">\", \"abil\", \"abil\", \"absolut\", \"absolut\", \"accept\", \"access\", \"accessori\", \"act\", \"action\", \"ad\", \"ad\", \"addict\", \"admir\", \"admir\", \"ador\", \"adult\", \"advanc\", \"ai\", \"ai\", \"ai\", \"aint\", \"aliv\", \"alright\", \"also\", \"also\", \"also\", \"also\", \"amaz\", \"ambiti\", \"amus\", \"amus\", \"anti\", \"app\", \"appar\", \"applic\", \"area\", \"area\", \"ark\", \"around\", \"arriv\", \"art\", \"art\", \"artist\", \"artist\", \"ass\", \"aswel\", \"asynchron\", \"audienc\", \"autom\", \"automat\", \"averag\", \"averag\", \"aw\", \"awesom\", \"b\", \"babi\", \"ball\", \"banger\", \"basi\", \"battl\", \"battl\", \"bb\", \"bear\", \"bearabl\", \"beaten\", \"beauti\", \"beauti\", \"becam\", \"bend\", \"benefit\", \"best\", \"best\", \"better\", \"better\", \"bit\", \"bit\", \"bite\", \"black\", \"blast\", \"block\", \"bloodborn\", \"board\", \"bodi\", \"boi\", \"bot\", \"boy\", \"brain\", \"brand\", \"brrrrrr\", \"buck\", \"buddi\", \"bug\", \"bug\", \"busi\", \"busi\", \"c\", \"ca\", \"call\", \"call\", \"calm\", \"campaign\", \"campaign\", \"can\", \"cant\", \"capabl\", \"car\", \"cathart\", \"caus\", \"center\", \"certain\", \"certain\", \"chanc\", \"chang\", \"chang\", \"charact\", \"charact\", \"charact\", \"cheap\", \"cheaper\", \"children\", \"chines\", \"choic\", \"chore\", \"christma\", \"clan\", \"closest\", \"club\", \"co\", \"coffe\", \"colleg\", \"colour\", \"com\", \"combat\", \"combat\", \"combat\", \"combin\", \"combin\", \"commun\", \"commun\", \"complet\", \"complic\", \"comput\", \"comput\", \"con\", \"consol\", \"construct\", \"content\", \"content\", \"cool\", \"copi\", \"cost\", \"could\", \"could\", \"crash\", \"crazi\", \"creativ\", \"cri\", \"crossbow\", \"crumb\", \"cultur\", \"cultur\", \"cup\", \"cure\", \"current\", \"d\", \"dad\", \"daddi\", \"dang\", \"dark\", \"dark\", \"darn\", \"data\", \"daughter\", \"daunt\", \"dc\", \"deaf\", \"dear\", \"decay\", \"dedic\", \"defeat\", \"defi\", \"defiantli\", \"deficit\", \"definit\", \"definit\", \"del\", \"deserv\", \"design\", \"design\", \"design\", \"destroy\", \"dev\", \"dev\", \"develop\", \"develop\", \"diagnos\", \"dialogu\", \"difficult\", \"difficult\", \"dimens\", \"dirti\", \"dive\", \"divers\", \"divers\", \"divis\", \"dlc\", \"do\", \"documentari\", \"doesn\", \"dollar\", \"don\", \"don\", \"done\", \"dont\", \"door\", \"doubl\", \"download\", \"dragon\", \"dread\", \"dri\", \"drifter\", \"drove\", \"drug\", \"drunkenli\", \"duck\", \"e\", \"eargasm\", \"earli\", \"east\", \"easter\", \"eastern\", \"edit\", \"effect\", \"effect\", \"effect\", \"egg\", \"elden\", \"element\", \"email\", \"encount\", \"enemi\", \"engag\", \"engag\", \"engin\", \"english\", \"english\", \"enjoy\", \"enjoy\", \"enter\", \"entir\", \"entir\", \"entir\", \"environ\", \"equip\", \"eras\", \"escap\", \"euro\", \"even\", \"even\", \"even\", \"everybodi\", \"everytim\", \"everywher\", \"excus\", \"exist\", \"expans\", \"expens\", \"experi\", \"export\", \"faction\", \"fail\", \"famili\", \"fantast\", \"farm\", \"faster\", \"favor\", \"favorit\", \"featur\", \"feel\", \"fellow\", \"fetch\", \"fi\", \"fight\", \"fight\", \"fight\", \"fighter\", \"filedetail\", \"finish\", \"first\", \"fist\", \"fix\", \"flat\", \"fold\", \"food\", \"forc\", \"forc\", \"forgotten\", \"format\", \"formula\", \"fp\", \"friend\", \"fuck\", \"fuel\", \"fun\", \"fun\", \"fusion\", \"g\", \"ga\", \"game\", \"game\", \"gameplay\", \"gameplay\", \"gameplay\", \"gameplay\", \"gameplay\", \"gap\", \"gather\", \"gay\", \"gem\", \"gener\", \"gener\", \"geniu\", \"get\", \"get\", \"girl\", \"girlfriend\", \"give\", \"give\", \"glad\", \"go\", \"go\", \"god\", \"goliath\", \"gon\", \"good\", \"good\", \"good\", \"googl\", \"got\", \"grab\", \"grade\", \"grandma\", \"graphic\", \"graphic\", \"graphic\", \"great\", \"great\", \"greatest\", \"group\", \"gunsling\", \"h\", \"hail\", \"hang\", \"happen\", \"happen\", \"happi\", \"hassl\", \"heartfelt\", \"heartwarm\", \"heavi\", \"heck\", \"hero\", \"hesit\", \"high\", \"hilari\", \"hill\", \"hog\", \"hollywood\", \"hope\", \"hope\", \"hot\", \"hour\", \"hq\", \"http\", \"hurt\", \"id\", \"ident\", \"ident\", \"idiot\", \"ill\", \"im\", \"impact\", \"import\", \"improv\", \"in\", \"incorpor\", \"inde\", \"infin\", \"influenc\", \"instantli\", \"investig\", \"invit\", \"irl\", \"issu\", \"issu\", \"it\", \"itch\", \"itu\", \"ive\", \"joe\", \"joke\", \"juic\", \"justifi\", \"keep\", \"kick\", \"kick\", \"kill\", \"kill\", \"killer\", \"kinda\", \"l\", \"lake\", \"lama\", \"languag\", \"laugh\", \"launch\", \"leaderboard\", \"left\", \"left\", \"legend\", \"legitim\", \"licens\", \"licens\", \"lie\", \"light\", \"like\", \"like\", \"limit\", \"limit\", \"limit\", \"limit\", \"link\", \"littl\", \"local\", \"lockdown\", \"logist\", \"lol\", \"lone\", \"look\", \"look\", \"look\", \"look\", \"lord\", \"lose\", \"lost\", \"lost\", \"lot\", \"lot\", \"lot\", \"love\", \"love\", \"lowlif\", \"mach\", \"made\", \"main\", \"main\", \"make\", \"man\", \"mani\", \"mani\", \"mani\", \"mani\", \"market\", \"master\", \"master\", \"materi\", \"mc\", \"mean\", \"mean\", \"mean\", \"medit\", \"meet\", \"mehh\", \"men\", \"mental\", \"mile\", \"minut\", \"mixtap\", \"mmo\", \"mode\", \"mode\", \"mommi\", \"money\", \"money\", \"money\", \"month\", \"month\", \"monument\", \"mop\", \"moron\", \"ms\", \"much\", \"much\", \"multi\", \"multiplay\", \"music\", \"music\", \"na\", \"nail\", \"name\", \"nasa\", \"necessari\", \"necessari\", \"need\", \"need\", \"need\", \"need\", \"netflix\", \"new\", \"new\", \"newli\", \"nice\", \"non\", \"nonetheless\", \"nonsens\", \"noon\", \"not\", \"notch\", \"oddli\", \"offer\", \"often\", \"often\", \"ok\", \"omg\", \"one\", \"one\", \"onlin\", \"op\", \"orang\", \"order\", \"order\", \"order\", \"order\", \"outpost\", \"outpost\", \"outright\", \"outstand\", \"overpr\", \"own\", \"paint\", \"paint\", \"pandem\", \"passion\", \"patch\", \"path\", \"pay\", \"peac\", \"perform\", \"period\", \"physic\", \"piss\", \"pl\", \"plagu\", \"platform\", \"platinum\", \"play\", \"play\", \"playabl\", \"player\", \"player\", \"player\", \"pleb\", \"plenti\", \"pod\", \"polici\", \"polit\", \"pong\", \"potato\", \"potenti\", \"prefer\", \"present\", \"pretti\", \"primari\", \"pro\", \"probabl\", \"product\", \"profit\", \"promis\", \"protect\", \"proud\", \"proud\", \"publish\", \"pull\", \"pump\", \"punch\", \"punch\", \"purchas\", \"purchas\", \"pure\", \"push\", \"puzzl\", \"pve\", \"quest\", \"quest\", \"quot\", \"race\", \"radar\", \"rage\", \"rang\", \"rang\", \"rate\", \"rdr2\", \"realist\", \"realiti\", \"realli\", \"realli\", \"reason\", \"reason\", \"rebel\", \"receiv\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"record\", \"relax\", \"releas\", \"reliev\", \"repair\", \"repetit\", \"requir\", \"requir\", \"requir\", \"requir\", \"requir\", \"resist\", \"resourc\", \"respons\", \"revisit\", \"rid\", \"ring\", \"rip\", \"risk\", \"rpg\", \"rx\", \"say\", \"scale\", \"scam\", \"scaveng\", \"sci\", \"scienc\", \"scout\", \"scratch\", \"screen\", \"season\", \"see\", \"self\", \"sent\", \"separ\", \"sequel\", \"seri\", \"session\", \"sfw\", \"shadow\", \"sharedfil\", \"shit\", \"short\", \"short\", \"short\", \"shot\", \"shower\", \"side\", \"side\", \"sight\", \"sim\", \"six\", \"skeleton\", \"skill\", \"slave\", \"smile\", \"smoke\", \"social\", \"solar\", \"sold\", \"someth\", \"song\", \"soni\", \"soon\", \"sooner\", \"soul\", \"soul\", \"soundtrack\", \"soundtrack\", \"space\", \"spare\", \"spawn\", \"spend\", \"spend\", \"sport\", \"spread\", \"squid\", \"stabilis\", \"stage\", \"standard\", \"star\", \"star\", \"state\", \"station\", \"station\", \"stay\", \"steal\", \"stealthi\", \"steam\", \"steam\", \"steamcommun\", \"still\", \"still\", \"stop\", \"stop\", \"store\", \"stori\", \"stori\", \"stori\", \"stori\", \"strand\", \"strand\", \"struck\", \"stuck\", \"stumbl\", \"style\", \"style\", \"suck\", \"support\", \"surfac\", \"surprisingli\", \"swarm\", \"sweet\", \"system\", \"system\", \"system\", \"t\", \"t\", \"tap\", \"tedious\", \"teen\", \"teleport\", \"tens\", \"thing\", \"thingi\", \"think\", \"thorough\", \"thou\", \"though\", \"thousand\", \"threaten\", \"ti\", \"time\", \"time\", \"time\", \"titl\", \"today\", \"tomorrow\", \"ton\", \"tough\", \"tradit\", \"trailer\", \"transport\", \"tri\", \"trial\", \"trike\", \"truck\", \"tsushima\", \"turn\", \"turn\", \"turn\", \"typewrit\", \"unabl\", \"understood\", \"underwhelm\", \"unholi\", \"uniqu\", \"uniqu\", \"unreal\", \"unsuspect\", \"updat\", \"urg\", \"urg\", \"url\", \"us\", \"us\", \"usa\", \"usag\", \"usd\", \"v\", \"v\", \"va\", \"van\", \"varieti\", \"verdict\", \"version\", \"videogam\", \"visual\", \"visual\", \"visual\", \"voic\", \"void\", \"w\", \"wall\", \"wan\", \"want\", \"want\", \"war\", \"wast\", \"wave\", \"way\", \"way\", \"weapon\", \"weav\", \"well\", \"well\", \"well\", \"went\", \"whale\", \"what\", \"whatev\", \"wheel\", \"wholesom\", \"wife\", \"win\", \"wont\", \"word\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"worth\", \"worth\", \"would\", \"would\", \"wow\", \"www\", \"yard\", \"yearn\", \"yell\", \"yellow\", \"youtub\", \"youtub\", \"zip\", \"ziplin\", \"|\", \"\\u2013\", \"\\u2018\", \"\\u2018\", \"\\u2019\", \"\\u2019\", \"\\u201c\", \"\\u201d\"]}, \"R\": 25, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [14, 23, 4, 8, 20, 22, 21, 16, 11, 13, 6, 24, 15, 12, 17, 25, 5, 1, 18, 3, 19, 7, 9, 2, 10]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el257223554182800007690155034\", ldavis_el257223554182800007690155034_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el257223554182800007690155034\", ldavis_el257223554182800007690155034_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el257223554182800007690155034\", ldavis_el257223554182800007690155034_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "13    -0.420470 -0.313089       1        1  48.505229\n",
       "22    -0.111985  0.515828       2        1  11.170353\n",
       "3     -0.502573  0.050162       3        1  10.700845\n",
       "7     -0.495161 -0.152324       4        1   9.995363\n",
       "19    -0.450056  0.290528       5        1   9.036553\n",
       "21    -0.239951  0.375147       6        1   1.517139\n",
       "20    -0.024988 -0.478811       7        1   1.097022\n",
       "15     0.314437 -0.386843       8        1   0.987106\n",
       "10     0.447322 -0.150161       9        1   0.862563\n",
       "12     0.179098  0.421481      10        1   0.764457\n",
       "5      0.444791  0.081935      11        1   0.729215\n",
       "23     0.357921  0.272776      12        1   0.658834\n",
       "14    -0.228462  0.151977      13        1   0.629464\n",
       "11    -0.185209 -0.311399      14        1   0.554172\n",
       "16     0.141129 -0.319475      15        1   0.393279\n",
       "24     0.014561 -0.260934      16        1   0.308195\n",
       "4     -0.097703 -0.149916      17        1   0.306547\n",
       "0     -0.142248 -0.000651      18        1   0.292892\n",
       "17     0.135342  0.257561      19        1   0.287354\n",
       "2      0.212884 -0.165319      20        1   0.278417\n",
       "18     0.270108 -0.034286      21        1   0.256811\n",
       "6      0.012057  0.182278      22        1   0.229723\n",
       "8      0.228235  0.111369      23        1   0.217930\n",
       "1      0.041428  0.044444      24        1   0.162603\n",
       "9      0.099491 -0.032275      25        1   0.057935, topic_info=       Term          Freq         Total Category  logprob  loglift\n",
       "57     game  25661.000000  25661.000000  Default  25.0000   25.000\n",
       "144    play  11917.000000  11917.000000  Default  24.0000   24.000\n",
       "60     good   8321.000000   8321.000000  Default  23.0000   23.000\n",
       "174     fun   7554.000000   7554.000000  Default  22.0000   22.000\n",
       "249   great   5816.000000   5816.000000  Default  21.0000   21.000\n",
       "...     ...           ...           ...      ...      ...      ...\n",
       "7332     rt      0.017955      1.043681  Topic25 -10.5346    3.391\n",
       "4172  steel      0.017955      1.043681  Topic25 -10.5346    3.391\n",
       "3102     20      0.017955      1.043682  Topic25 -10.5346    3.391\n",
       "3086    mod      0.017955      1.043682  Topic25 -10.5346    3.391\n",
       "3244     ui      0.017955      1.043682  Topic25 -10.5346    3.391\n",
       "\n",
       "[842 rows x 6 columns], token_table=      Topic      Freq Term\n",
       "term                      \n",
       "1222      3  0.072593    &\n",
       "1222      5  0.925963    &\n",
       "1092      2  0.998515    0\n",
       "0         1  0.290370    1\n",
       "0         2  0.653807    1\n",
       "...     ...       ...  ...\n",
       "130      10  0.169158    \n",
       "1351      6  0.022450    \n",
       "1351     10  0.975313    \n",
       "1352     10  0.994535    \n",
       "1353     10  0.989228    \n",
       "\n",
       "[892 rows x 3 columns], R=25, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[14, 23, 4, 8, 20, 22, 21, 16, 11, 13, 6, 24, 15, 12, 17, 25, 5, 1, 18, 3, 19, 7, 9, 2, 10])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis = pyLDAvis.gensim_models.prepare(lda_model2, corpus, id2word, mds=\"mmds\", R=25)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "3bd83b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(0.018335398, 'like'),\n",
       "   (0.014705912, 'get'),\n",
       "   (0.014360783, 'time'),\n",
       "   (0.011251795, 'one'),\n",
       "   (0.011159561, 'make'),\n",
       "   (0.009684668, 'even'),\n",
       "   (0.00959506, 'realli'),\n",
       "   (0.009450252, 'feel'),\n",
       "   (0.009443683, 'go'),\n",
       "   (0.009270748, 'much'),\n",
       "   (0.008823673, 'would'),\n",
       "   (0.0083337, 'hour'),\n",
       "   (0.008327535, 'want'),\n",
       "   (0.008284028, 'lot'),\n",
       "   (0.0080933655, 'thing'),\n",
       "   (0.007955168, 'way'),\n",
       "   (0.007700883, 'look'),\n",
       "   (0.007564078, 'need'),\n",
       "   (0.007293762, 'also'),\n",
       "   (0.0071551627, 'see')],\n",
       "  -1.1897604930678212),\n",
       " ([(0.015055558, 'combat'),\n",
       "   (0.0143759195, 'battl'),\n",
       "   (0.013188115, 'charact'),\n",
       "   (0.012098297, 'enemi'),\n",
       "   (0.011348516, 'design'),\n",
       "   (0.011139194, 'main'),\n",
       "   (0.009245404, 'side'),\n",
       "   (0.009022133, 'choic'),\n",
       "   (0.008930867, 'fight'),\n",
       "   (0.008929173, 'uniqu'),\n",
       "   (0.0089199785, 'abil'),\n",
       "   (0.008139824, 'system'),\n",
       "   (0.008112323, 'weapon'),\n",
       "   (0.008073769, 'quest'),\n",
       "   (0.007884642, 'certain'),\n",
       "   (0.007576329, 'visual'),\n",
       "   (0.0074563045, 'skill'),\n",
       "   (0.007343471, 'voic'),\n",
       "   (0.007128181, 'effect'),\n",
       "   (0.006902093, 'gener')],\n",
       "  -1.848314244360834),\n",
       " ([(0.24111256, 'game'),\n",
       "   (0.087591745, 'play'),\n",
       "   (0.06911887, 'fun'),\n",
       "   (0.06817898, 'good'),\n",
       "   (0.05521197, 'great'),\n",
       "   (0.04295174, 'love'),\n",
       "   (0.04148377, 'recommend'),\n",
       "   (0.031629674, '10'),\n",
       "   (0.029180096, 'best'),\n",
       "   (0.024216479, 'graphic'),\n",
       "   (0.020175058, 'gameplay'),\n",
       "   (0.01920488, 'stori'),\n",
       "   (0.017018227, 'amaz'),\n",
       "   (0.013739459, 'definit'),\n",
       "   (0.012102385, 'music'),\n",
       "   (0.011652655, 'absolut'),\n",
       "   (0.010937872, 'realli'),\n",
       "   (0.009869755, 'beauti'),\n",
       "   (0.008055024, 'style'),\n",
       "   (0.007934576, 'world')],\n",
       "  -1.9612821020791429),\n",
       " ([(0.022511853, 'bug'),\n",
       "   (0.021216271, 'fix'),\n",
       "   (0.019221757, 'releas'),\n",
       "   (0.018850882, 'updat'),\n",
       "   (0.018476417, 'dev'),\n",
       "   (0.018355321, 'issu'),\n",
       "   (0.018205734, 'develop'),\n",
       "   (0.017520456, 'improv'),\n",
       "   (0.016417764, 'current'),\n",
       "   (0.01551417, 'crash'),\n",
       "   (0.014666023, 'mode'),\n",
       "   (0.014265439, 'hope'),\n",
       "   (0.01359946, 'chang'),\n",
       "   (0.01342325, 'potenti'),\n",
       "   (0.0131745655, 'ai'),\n",
       "   (0.013128787, 'campaign'),\n",
       "   (0.01299836, 'commun'),\n",
       "   (0.012935622, 'state'),\n",
       "   (0.012697209, 'featur'),\n",
       "   (0.01259587, 'content')],\n",
       "  -2.084334192959508),\n",
       " ([(0.017368473, '3'),\n",
       "   (0.016735576, 'player'),\n",
       "   (0.013926259, '5'),\n",
       "   (0.013595482, '4'),\n",
       "   (0.013268455, '1'),\n",
       "   (0.010776576, 'seri'),\n",
       "   (0.010134875, 'happen'),\n",
       "   (0.009850523, 'reason'),\n",
       "   (0.009105998, 'kill'),\n",
       "   (0.008610803, 'spend'),\n",
       "   (0.008368088, 'space'),\n",
       "   (0.00826607, 'minut'),\n",
       "   (0.0077951783, 'forc'),\n",
       "   (0.0074521275, '6'),\n",
       "   (0.007384304, 'entir'),\n",
       "   (0.007109391, 'us'),\n",
       "   (0.007059086, 'money'),\n",
       "   (0.0070372485, 'win'),\n",
       "   (0.0068555344, 'call'),\n",
       "   (0.006766942, 'mean')],\n",
       "  -2.171800993596319),\n",
       " ([(0.04872363, 'puzzl'),\n",
       "   (0.023014396, 'brain'),\n",
       "   (0.017928062, 'ca'),\n",
       "   (0.016555976, 'realiti'),\n",
       "   (0.013954406, 'cup'),\n",
       "   (0.012648672, 'audienc'),\n",
       "   (0.01213201, 'w'),\n",
       "   (0.01097642, 'infin'),\n",
       "   (0.0108596645, 'paint'),\n",
       "   (0.0108184125, 'spare'),\n",
       "   (0.010587926, 'potato'),\n",
       "   (0.010346682, 'adult'),\n",
       "   (0.01024241, 'dri'),\n",
       "   (0.009565684, 'soul'),\n",
       "   (0.009541604, 'ark'),\n",
       "   (0.009348372, 'averag'),\n",
       "   (0.009301854, 'leaderboard'),\n",
       "   (0.009268498, 'usag'),\n",
       "   (0.009128936, 'dark'),\n",
       "   (0.0088538565, 'short')],\n",
       "  -2.5496649302546603),\n",
       " ([(0.06193947, 'soon'),\n",
       "   (0.05212525, 'push'),\n",
       "   (0.048525225, 'wall'),\n",
       "   (0.043804433, 'fail'),\n",
       "   (0.041986816, 'pull'),\n",
       "   (0.039939832, 'e'),\n",
       "   (0.03713666, 'polit'),\n",
       "   (0.035866182, 'wheel'),\n",
       "   (0.030753603, 'respons'),\n",
       "   (0.03014555, 'enter'),\n",
       "   (0.028382111, 'automat'),\n",
       "   (0.027694948, 'advanc'),\n",
       "   (0.027478782, 'hurt'),\n",
       "   (0.021113183, 'anti'),\n",
       "   (0.021084419, 'bodi'),\n",
       "   (0.019713068, 'flat'),\n",
       "   (0.019469392, 'sport'),\n",
       "   (0.018756693, 'doubl'),\n",
       "   (0.016354956, 'gap'),\n",
       "   (0.015628176, 'everywher')],\n",
       "  -4.11809540998704),\n",
       " ([(0.07396964, 'resourc'),\n",
       "   (0.06894029, 'physic'),\n",
       "   (0.046574395, 'destroy'),\n",
       "   (0.036557846, 'market'),\n",
       "   (0.03156305, 'farm'),\n",
       "   (0.027399398, 'spawn'),\n",
       "   (0.02584979, 'food'),\n",
       "   (0.024795132, 'autom'),\n",
       "   (0.023664411, 'wave'),\n",
       "   (0.021465551, 'receiv'),\n",
       "   (0.02065548, 'gather'),\n",
       "   (0.019919265, 'accept'),\n",
       "   (0.019708548, 'thousand'),\n",
       "   (0.019337783, 'materi'),\n",
       "   (0.01700622, 'unabl'),\n",
       "   (0.016000928, 'instantli'),\n",
       "   (0.015255529, 'fuel'),\n",
       "   (0.014305147, 'social'),\n",
       "   (0.012709283, 'what'),\n",
       "   (0.012541158, 'transport')],\n",
       "  -4.164562163884084),\n",
       " ([(0.10076559, 'can'),\n",
       "   (0.09223832, 'not'),\n",
       "   (0.09121505, 'screen'),\n",
       "   (0.0856558, 'divis'),\n",
       "   (0.0473119, 'went'),\n",
       "   (0.045966454, 'board'),\n",
       "   (0.043848135, ''),\n",
       "   (0.03754967, 'block'),\n",
       "   (0.02738286, 't'),\n",
       "   (0.024815831, 'it'),\n",
       "   (0.022925245, 'dedic'),\n",
       "   (0.022853417, 'v'),\n",
       "   (0.02051318, 'black'),\n",
       "   (0.019571932, 'hero'),\n",
       "   (0.019031763, 'separ'),\n",
       "   (0.01644719, 'famili'),\n",
       "   (0.01397007, 'rage'),\n",
       "   (0.011750299, 'don'),\n",
       "   (0.011402167, ''),\n",
       "   (0.011350449, '')],\n",
       "  -4.3960608258711655),\n",
       " ([(0.31551942, 'car'),\n",
       "   (0.06065612, 'com'),\n",
       "   (0.05299833, 'http'),\n",
       "   (0.035911348, 'id'),\n",
       "   (0.032808386, 'radar'),\n",
       "   (0.02655865, 'juic'),\n",
       "   (0.015072991, 'www'),\n",
       "   (0.014746412, 'youtub'),\n",
       "   (0.01407447, 'steamcommun'),\n",
       "   (0.0133600375, 'url'),\n",
       "   (0.011656508, 'v'),\n",
       "   (0.010670874, 'l'),\n",
       "   (0.010653957, 'quot'),\n",
       "   (0.010295191, 'aswel'),\n",
       "   (0.009536065, 'sharedfil'),\n",
       "   (0.009536065, 'filedetail'),\n",
       "   (0.0055498397, 'thingi'),\n",
       "   (0.0047123167, 'hog'),\n",
       "   (0.0045328243, 'in'),\n",
       "   (0.0029014251, '|')],\n",
       "  -5.702449620673292),\n",
       " ([(0.05303743, 'star'),\n",
       "   (0.051949665, 'path'),\n",
       "   (0.042435993, 'faster'),\n",
       "   (0.040620923, 'justifi'),\n",
       "   (0.036827974, 'session'),\n",
       "   (0.035482775, 'lie'),\n",
       "   (0.03288954, 'becam'),\n",
       "   (0.030834865, 'season'),\n",
       "   (0.030741023, 'today'),\n",
       "   (0.029539531, 'period'),\n",
       "   (0.025135674, 'busi'),\n",
       "   (0.024022447, 'chore'),\n",
       "   (0.021592967, 'defeat'),\n",
       "   (0.020368343, 'formula'),\n",
       "   (0.017504584, 'nail'),\n",
       "   (0.017018711, 'influenc'),\n",
       "   (0.016941743, 'record'),\n",
       "   (0.016063536, 'song'),\n",
       "   (0.014076586, 'sweet'),\n",
       "   (0.013724305, 'primari')],\n",
       "  -5.997850423763918),\n",
       " ([(0.2380886, 'earli'),\n",
       "   (0.20208235, 'access'),\n",
       "   (0.080055736, 'pay'),\n",
       "   (0.05968592, 'launch'),\n",
       "   (0.049503073, 'stuck'),\n",
       "   (0.046550594, 'expans'),\n",
       "   (0.040176388, 'light'),\n",
       "   (0.03837341, 'playabl'),\n",
       "   (0.025177227, 'prefer'),\n",
       "   (0.015019973, 'hang'),\n",
       "   (0.0123697445, 'local'),\n",
       "   (0.011250843, 'cheaper'),\n",
       "   (0.009706469, 'licens'),\n",
       "   (0.006563637, 'overpr'),\n",
       "   (0.0065114936, 'scout'),\n",
       "   (0.0060498556, 'dirti'),\n",
       "   (0.0056721834, 'scam'),\n",
       "   (0.005534573, 'tens'),\n",
       "   (0.00502588, 'usd'),\n",
       "   (0.0043922476, 'sooner')],\n",
       "  -8.037068228380974),\n",
       " ([(0.044956792, 'expens'),\n",
       "   (0.044128526, 'store'),\n",
       "   (0.04239395, 'wont'),\n",
       "   (0.03894317, 'copi'),\n",
       "   (0.038022254, 'steal'),\n",
       "   (0.031475667, 'construct'),\n",
       "   (0.031117583, 'alright'),\n",
       "   (0.027141742, 'benefit'),\n",
       "   (0.025097366, 'capabl'),\n",
       "   (0.02403249, 'bear'),\n",
       "   (0.02221206, 'men'),\n",
       "   (0.021177176, 'publish'),\n",
       "   (0.020427382, 'ador'),\n",
       "   (0.018755514, 'fighter'),\n",
       "   (0.018652286, 'logist'),\n",
       "   (0.017585447, 'nonsens'),\n",
       "   (0.01732903, 'protect'),\n",
       "   (0.015836092, 'profit'),\n",
       "   (0.014239099, 'inde'),\n",
       "   (0.0140853105, 'ga')],\n",
       "  -8.593030318374751),\n",
       " ([(0.23264815, 'race'),\n",
       "   (0.052113004, 'pure'),\n",
       "   (0.050128665, 'surprisingli'),\n",
       "   (0.037086044, 'tough'),\n",
       "   (0.036728274, 'legend'),\n",
       "   (0.035223443, 'mmo'),\n",
       "   (0.03330722, 'kick'),\n",
       "   (0.03249898, 'trial'),\n",
       "   (0.031957064, 'ring'),\n",
       "   (0.028288713, 'lord'),\n",
       "   (0.0274812, 'wife'),\n",
       "   (0.016835703, 'shadow'),\n",
       "   (0.01598337, 'elden'),\n",
       "   (0.015237451, 'tap'),\n",
       "   (0.014824565, 'revisit'),\n",
       "   (0.014735844, 'punch'),\n",
       "   (0.012620794, 'yard'),\n",
       "   (0.008453309, 'idiot'),\n",
       "   (0.008155121, 'hassl'),\n",
       "   (0.0076840017, 'moron')],\n",
       "  -8.95809036205522),\n",
       " ([(0.24601173, 'friend'),\n",
       "   (0.08238063, 'dragon'),\n",
       "   (0.047300622, 'op'),\n",
       "   (0.04651646, 'ball'),\n",
       "   (0.03741288, 'co'),\n",
       "   (0.035163052, 'appar'),\n",
       "   (0.03216088, 'lol'),\n",
       "   (0.025385696, '>'),\n",
       "   (0.023767203, 'boy'),\n",
       "   (0.02064505, 'peac'),\n",
       "   (0.019473387, '14'),\n",
       "   (0.019199755, 'door'),\n",
       "   (0.01699347, 'own'),\n",
       "   (0.01584339, 'mile'),\n",
       "   (0.014572409, 'favor'),\n",
       "   (0.014424475, 'hesit'),\n",
       "   (0.014142547, 'closest'),\n",
       "   (0.010732162, 'fellow'),\n",
       "   (0.010400383, 'buddi'),\n",
       "   (0.00991558, 'six')],\n",
       "  -9.730958957698626),\n",
       " ([(0.21039747, 'cool'),\n",
       "   (0.122269854, 'sim'),\n",
       "   (0.10087713, 'suck'),\n",
       "   (0.06167857, 'aw'),\n",
       "   (0.047415197, 'truck'),\n",
       "   (0.04229463, 'scratch'),\n",
       "   (0.036508664, 'sight'),\n",
       "   (0.032929126, 'itch'),\n",
       "   (0.022038488, 'dive'),\n",
       "   (0.018664522, 'euro'),\n",
       "   (0.017225094, 'fi'),\n",
       "   (0.01671889, 'link'),\n",
       "   (0.01638809, 'sci'),\n",
       "   (0.014613265, 'spread'),\n",
       "   (0.012342957, 'underwhelm'),\n",
       "   (0.010705948, 'fetch'),\n",
       "   (0.010696483, 'orang'),\n",
       "   (0.0075394595, 'lone'),\n",
       "   (0.0071621113, 'smile'),\n",
       "   (0.0053663305, 'mc')],\n",
       "  -11.462034267044318),\n",
       " ([(0.22826281, 'ton'),\n",
       "   (0.076530494, 'wow'),\n",
       "   (0.054762192, '70'),\n",
       "   (0.05194577, '200'),\n",
       "   (0.048903156, 'bot'),\n",
       "   (0.04272154, 'hot'),\n",
       "   (0.03402669, 'children'),\n",
       "   (0.024448795, 'egg'),\n",
       "   (0.013718032, 'bend'),\n",
       "   (0.011583345, 'resist'),\n",
       "   (0.011066442, 'eastern'),\n",
       "   (0.010399668, 'solar'),\n",
       "   (0.009293345, 'dc'),\n",
       "   (0.007599925, 'dad'),\n",
       "   (0.0075431443, 'wholesom'),\n",
       "   (0.0068817236, 'duck'),\n",
       "   (0.006619514, 'accessori'),\n",
       "   (0.0059389463, 'tomorrow'),\n",
       "   (0.0048064003, 'stabilis'),\n",
       "   (0.004124335, 'skeleton')],\n",
       "  -12.247098969288531),\n",
       " ([(0.2241674, 'relax'),\n",
       "   (0.20072353, 'im'),\n",
       "   (0.05703006, 'self'),\n",
       "   (0.03292638, 'girl'),\n",
       "   (0.03235759, 'club'),\n",
       "   (0.02697157, 'everytim'),\n",
       "   (0.017136617, 'void'),\n",
       "   (0.016597675, 'gay'),\n",
       "   (0.01501584, 'email'),\n",
       "   (0.011014237, 'platinum'),\n",
       "   (0.0074949204, 'shower'),\n",
       "   (0.005366919, 'cathart'),\n",
       "   (0.0035905181, 'strand'),\n",
       "   (0.001812665, '50fp'),\n",
       "   (1.4046055e-05, 'builder'),\n",
       "   (1.4046053e-05, 'chill'),\n",
       "   (1.4046053e-05, '20'),\n",
       "   (1.4046052e-05, 'mod'),\n",
       "   (1.4046052e-05, 'low'),\n",
       "   (1.4046052e-05, 'ship')],\n",
       "  -12.445152259517704),\n",
       " ([(0.13979694, 'promis'),\n",
       "   (0.080737606, 'meet'),\n",
       "   (0.07474463, 'ass'),\n",
       "   (0.052259367, 'c'),\n",
       "   (0.051454082, 'complic'),\n",
       "   (0.047100883, 'chines'),\n",
       "   (0.03662459, 'languag'),\n",
       "   (0.031321354, 'hill'),\n",
       "   (0.030868389, '45'),\n",
       "   (0.020772692, 'googl'),\n",
       "   (0.020441277, 'killer'),\n",
       "   (0.01709796, 'english'),\n",
       "   (0.016142124, 'applic'),\n",
       "   (0.013105523, 'girlfriend'),\n",
       "   (0.012732035, 'daughter'),\n",
       "   (0.0113509055, 'daunt'),\n",
       "   (0.011015587, 'drug'),\n",
       "   (0.01093176, 'everybodi'),\n",
       "   (0.010750142, 'videogam'),\n",
       "   (0.010352471, 'va')],\n",
       "  -12.520744659568566),\n",
       " ([(0.23013988, 'na'),\n",
       "   (0.14737096, 'gon'),\n",
       "   (0.078756936, 'wan'),\n",
       "   (0.02863825, 'east'),\n",
       "   (0.020893222, 'omg'),\n",
       "   (0.014954486, '3080'),\n",
       "   (0.014070704, 'proud'),\n",
       "   (0.013587163, 'grade'),\n",
       "   (0.013581564, 'ti'),\n",
       "   (0.012601116, '3070'),\n",
       "   (0.011667955, 'banger'),\n",
       "   (0.0065263715, 'van'),\n",
       "   (0.0021148974, 'hq'),\n",
       "   (0.0013226707, 'diagnos'),\n",
       "   (0.0009518976, 'rdr2'),\n",
       "   (0.00022154445, 'bachelor'),\n",
       "   (1.6260785e-05, 'ui'),\n",
       "   (1.6260785e-05, 'ea'),\n",
       "   (1.6260781e-05, 'unit'),\n",
       "   (1.6260781e-05, 'ignor')],\n",
       "  -13.056573493192326),\n",
       " ([(0.23485637, 'awesom'),\n",
       "   (0.19838917, 'dont'),\n",
       "   (0.15530604, 'fuck'),\n",
       "   (0.04725536, 'smoke'),\n",
       "   (0.034464013, 'excus'),\n",
       "   (0.022111485, 'data'),\n",
       "   (0.014864078, 'heck'),\n",
       "   (0.009813329, 'clan'),\n",
       "   (0.0076436163, 'joe'),\n",
       "   (0.0015072135, 'drifter'),\n",
       "   (0.0011652245, 'lama'),\n",
       "   (0.0011343387, 'brrrrrr'),\n",
       "   (0.0011265829, 'noon'),\n",
       "   (0.0010266538, 'decay'),\n",
       "   (0.0004038319, 'typewrit'),\n",
       "   (0.00037296946, 'itu'),\n",
       "   (1.0846969e-05, 'unit'),\n",
       "   (1.0846967e-05, 'wargam'),\n",
       "   (1.0846966e-05, 'mod'),\n",
       "   (1.0846965e-05, '20')],\n",
       "  -14.07173603917544),\n",
       " ([(0.097902074, 'greatest'),\n",
       "   (0.0859434, 'format'),\n",
       "   (0.05428726, 'usa'),\n",
       "   (0.0376114, 'pl'),\n",
       "   (0.03239319, 'aint'),\n",
       "   (0.014095717, 'pod'),\n",
       "   (0.006586441, 'pleb'),\n",
       "   (0.0057312814, 'mach'),\n",
       "   (0.005270041, 'whale'),\n",
       "   (0.0011078349, 'squid'),\n",
       "   (0.0001419543, 'bpm'),\n",
       "   (0.00014195373, 'slooooow'),\n",
       "   (0.00014195342, 'breeeeeaaaatherrrrrrr'),\n",
       "   (2.6599071e-05, 'unit'),\n",
       "   (2.6599066e-05, 'wargam'),\n",
       "   (2.6599062e-05, 'warno'),\n",
       "   (2.6599062e-05, 'depth'),\n",
       "   (2.6599062e-05, 'ship'),\n",
       "   (2.6599062e-05, 'ussr'),\n",
       "   (2.6599058e-05, 'fleet')],\n",
       "  -14.202770482258796),\n",
       " ([(0.2576817, 'realist'),\n",
       "   (0.18360391, 'word'),\n",
       "   (0.052159887, 'gem'),\n",
       "   (0.045037545, 'cri'),\n",
       "   (0.04045865, 'laugh'),\n",
       "   (0.027123693, 'dread'),\n",
       "   (0.025611402, 'pump'),\n",
       "   (0.02455391, 'basi'),\n",
       "   (0.01697497, 'pandem'),\n",
       "   (0.014029868, 'nonetheless'),\n",
       "   (0.007423942, 'fist'),\n",
       "   (0.005978627, 'stealthi'),\n",
       "   (0.0052622226, 'netflix'),\n",
       "   (0.004538102, 'eras'),\n",
       "   (0.00424164, 'fusion'),\n",
       "   (0.0027078097, 'unholi'),\n",
       "   (0.0024485055, 'lockdown'),\n",
       "   (0.001638816, 'soni'),\n",
       "   (0.0015426709, 'bloodborn'),\n",
       "   (0.0006355821, 'mop')],\n",
       "  -14.303957777015759),\n",
       " ([(0.18862063, 'cant'),\n",
       "   (0.14242667, 'addict'),\n",
       "   (0.07999055, 'crazi'),\n",
       "   (0.045756433, 'polici'),\n",
       "   (0.0332529, 'rid'),\n",
       "   (0.029398967, 'sold'),\n",
       "   (0.026189785, 'babi'),\n",
       "   (0.022239057, 'swarm'),\n",
       "   (0.022022655, 'yell'),\n",
       "   (0.021494983, 'h'),\n",
       "   (0.017829128, 'divers'),\n",
       "   (0.016529096, 'newli'),\n",
       "   (0.015254714, 'bite'),\n",
       "   (0.012717444, 'understood'),\n",
       "   (0.012355282, 'struck'),\n",
       "   (0.011694651, 'rebel'),\n",
       "   (0.009697227, 'geniu'),\n",
       "   (0.005969736, 'daddi'),\n",
       "   (0.0054971315, '34'),\n",
       "   (0.0019500155, 'thou')],\n",
       "  -15.941569993743144),\n",
       " ([(0.16756897, '15'),\n",
       "   (0.09696926, 'grab'),\n",
       "   (0.085090406, 'download'),\n",
       "   (0.07299027, 'cheap'),\n",
       "   (0.072840855, 'joke'),\n",
       "   (0.06952223, 'dollar'),\n",
       "   (0.058978528, 'buck'),\n",
       "   (0.021008952, 'app'),\n",
       "   (0.009588214, 'slave'),\n",
       "   (0.008763659, 'darn'),\n",
       "   (0.005887306, 'cure'),\n",
       "   (0.0051295785, 'yellow'),\n",
       "   (0.0039236927, 'christma'),\n",
       "   (0.0036383125, 'thorough'),\n",
       "   (0.003134263, 'export'),\n",
       "   (0.0024270646, 'defiantli'),\n",
       "   (0.0023213336, 'heartwarm'),\n",
       "   (0.0012003785, 'sfw'),\n",
       "   (0.001054384, 'hollywood'),\n",
       "   (0.00031022806, 'tsushima')],\n",
       "  -16.230074854288773)]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model2.top_topics(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f53af0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_vis(fd):\n",
    "    def make_lda(fd):\n",
    "        id2word = corpora.Dictionary(fd)\n",
    "        corpus = [id2word.doc2bow(text) for text in fd]\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=25,\n",
    "                                               random_state=100,\n",
    "                                               update_every=5,\n",
    "                                               chunksize=100,\n",
    "                                               passes=20,\n",
    "                                               alpha=\"auto\")\n",
    "        return model, corpus, id2word\n",
    "    \n",
    "    model, corpus, id2word = make_lda(fd)\n",
    "    vis = pyLDAvis.gensim_models.prepare(model, corpus, id2word, mds=\"mmds\", R=25)\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "b854905b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el257223554179110722206257735\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el257223554179110722206257735_data = {\"mdsDat\": {\"x\": [-0.42046959626937686, -0.11198504331101936, -0.5025725523774819, -0.49516074052345466, -0.45005577794129453, -0.2399509654056908, -0.024987553231065024, 0.3144366925013235, 0.44732245482012856, 0.17909771209819503, 0.44479055983150995, 0.3579212483897264, -0.22846237540641748, -0.18520891443548712, 0.14112928103008934, 0.01456081011596003, -0.09770312769223384, -0.14224785120934802, 0.1353422667262102, 0.21288372782899623, 0.2701081558820108, 0.012057254883505513, 0.22823517936774626, 0.041428372156102324, 0.09949078217136523], \"y\": [-0.3130885061013926, 0.5158277411755309, 0.050162245797717935, -0.15232407895958938, 0.2905275871225756, 0.3751466649149533, -0.47881130976791825, -0.3868434141201516, -0.15016078775931005, 0.4214808303622908, 0.08193466532569751, 0.2727755893639237, 0.1519766134698386, -0.3113987339973937, -0.3194750341981275, -0.260934073198788, -0.14991633574505578, -0.0006513367114385266, 0.25756059139432247, -0.16531941727136257, -0.03428580175404564, 0.18227763845121128, 0.11136932720948904, 0.044444289130770614, -0.03227495413374811], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [48.505229275117316, 11.170352848177895, 10.700844977269783, 9.99536257420767, 9.036552560818757, 1.5171387583162763, 1.0970220330233058, 0.9871063378996683, 0.8625630043102883, 0.7644568747877594, 0.7292150550691658, 0.6588339876655633, 0.629464024919201, 0.554172343907536, 0.3932787891474747, 0.3081947541318336, 0.3065474089211477, 0.29289196497695785, 0.2873541163207501, 0.27841700462487146, 0.25681116092215894, 0.2297225325687218, 0.21792950185324594, 0.162602825206703, 0.05793528583595212]}, \"tinfo\": {\"Term\": [\"game\", \"play\", \"good\", \"fun\", \"great\", \"earli\", \"access\", \"love\", \"recommend\", \"friend\", \"get\", \"10\", \"time\", \"best\", \"cool\", \"much\", \"worth\", \"also\", \"bug\", \"race\", \"realli\", \"graphic\", \"could\", \"gameplay\", \"look\", \"make\", \"feel\", \"hour\", \"thing\", \"see\", \"first\", \"think\", \"say\", \"tri\", \"pretti\", \"want\", \"littl\", \"made\", \"someth\", \"got\", \"around\", \"complet\", \"keep\", \"nice\", \"though\", \"experi\", \"way\", \"finish\", \"done\", \"probabl\", \"go\", \"lot\", \"get\", \"time\", \"mani\", \"bit\", \"like\", \"even\", \"better\", \"much\", \"give\", \"one\", \"new\", \"also\", \"would\", \"need\", \"look\", \"well\", \"enjoy\", \"realli\", \"still\", \"seri\", \"space\", \"minut\", \"win\", \"lose\", \"faction\", \"wast\", \"shit\", \"cost\", \"caus\", \"offer\", \"happi\", \"exist\", \"kinda\", \"man\", \"stay\", \"group\", \"name\", \"god\", \"whatev\", \"ok\", \"shot\", \"0\", \"non\", \"chanc\", \"forc\", \"call\", \"kill\", \"happen\", \"lost\", \"5\", \"4\", \"spend\", \"often\", \"us\", \"3\", \"6\", \"entir\", \"player\", \"reason\", \"1\", \"stop\", \"left\", \"mean\", \"money\", \"month\", \"enemi\", \"choic\", \"weapon\", \"skill\", \"voic\", \"act\", \"pro\", \"b\", \"con\", \"element\", \"varieti\", \"present\", \"action\", \"repetit\", \"rpg\", \"encount\", \"scale\", \"plenti\", \"standard\", \"heavi\", \"equip\", \"impact\", \"import\", \"environ\", \"dialogu\", \"uniqu\", \"engag\", \"abil\", \"rang\", \"quest\", \"certain\", \"combat\", \"battl\", \"side\", \"visual\", \"effect\", \"main\", \"charact\", \"fight\", \"design\", \"area\", \"order\", \"limit\", \"system\", \"requir\", \"gener\", \"turn\", \"fix\", \"releas\", \"updat\", \"improv\", \"current\", \"crash\", \"potenti\", \"state\", \"featur\", \"support\", \"war\", \"dlc\", \"high\", \"patch\", \"version\", \"multiplay\", \"titl\", \"onlin\", \"edit\", \"perform\", \"product\", \"rate\", \"engin\", \"stage\", \"fp\", \"issu\", \"purchas\", \"dev\", \"commun\", \"bug\", \"campaign\", \"ad\", \"ai\", \"mode\", \"develop\", \"steam\", \"hope\", \"chang\", \"content\", \"work\", \"great\", \"amaz\", \"best\", \"fantast\", \"favorit\", \"creativ\", \"blast\", \"deserv\", \"platform\", \"glad\", \"11\", \"ive\", \"rip\", \"sequel\", \"ill\", \"aliv\", \"consol\", \"passion\", \"brand\", \"outstand\", \"irl\", \"2021\", \"trailer\", \"game\", \"notch\", \"love\", \"10\", \"fun\", \"definit\", \"recommend\", \"good\", \"graphic\", \"&\", \"play\", \"beauti\", \"music\", \"soundtrack\", \"art\", \"gameplay\", \"stori\", \"absolut\", \"style\", \"9\", \"combin\", \"world\", \"realli\", \"puzzl\", \"brain\", \"ca\", \"realiti\", \"cup\", \"audienc\", \"w\", \"infin\", \"spare\", \"potato\", \"adult\", \"dri\", \"ark\", \"leaderboard\", \"usag\", \"coffe\", \"nasa\", \"teen\", \"boi\", \"ms\", \"eargasm\", \"grandma\", \"do\", \"deaf\", \"mehh\", \"\\u2018\", \"paint\", \"averag\", \"soul\", \"dark\", \"necessari\", \"comput\", \"master\", \"short\", \"requir\", \"difficult\", \"resourc\", \"physic\", \"destroy\", \"market\", \"farm\", \"spawn\", \"food\", \"autom\", \"wave\", \"receiv\", \"gather\", \"accept\", \"thousand\", \"materi\", \"unabl\", \"instantli\", \"fuel\", \"social\", \"what\", \"transport\", \"center\", \"escap\", \"surfac\", \"repair\", \"risk\", \"station\", \"earli\", \"access\", \"pay\", \"launch\", \"stuck\", \"expans\", \"light\", \"playabl\", \"prefer\", \"hang\", \"local\", \"cheaper\", \"overpr\", \"scout\", \"dirti\", \"scam\", \"tens\", \"usd\", \"sooner\", \"incorpor\", \"licens\", \"reliev\", \"legitim\", \"scaveng\", \"dang\", \"amus\", \"soon\", \"push\", \"wall\", \"fail\", \"pull\", \"e\", \"polit\", \"wheel\", \"respons\", \"enter\", \"automat\", \"advanc\", \"hurt\", \"anti\", \"bodi\", \"flat\", \"sport\", \"doubl\", \"gap\", \"everywher\", \"g\", \"outright\", \"teleport\", \"hilari\", \"colour\", \"can\", \"not\", \"screen\", \"divis\", \"went\", \"board\", \"block\", \"it\", \"dedic\", \"black\", \"hero\", \"separ\", \"famili\", \"rage\", \"\\u201c\", \"\\u201d\", \"doesn\", \"mental\", \"stumbl\", \"sent\", \"d\", \"forgotten\", \"scienc\", \"invit\", \"colleg\", \"\\u2019\", \"v\", \"t\", \"don\", \"path\", \"faster\", \"justifi\", \"session\", \"lie\", \"becam\", \"season\", \"today\", \"period\", \"chore\", \"defeat\", \"formula\", \"nail\", \"influenc\", \"record\", \"song\", \"sweet\", \"primari\", \"piss\", \"arriv\", \"multi\", \"plagu\", \"verdict\", \"95\", \"unreal\", \"star\", \"busi\", \"friend\", \"dragon\", \"op\", \"ball\", \"co\", \"appar\", \"lol\", \">\", \"boy\", \"peac\", \"14\", \"door\", \"own\", \"mile\", \"favor\", \"hesit\", \"closest\", \"fellow\", \"buddi\", \"six\", \"beaten\", \"dear\", \"ambiti\", \"oddli\", \"calm\", \"expens\", \"store\", \"wont\", \"copi\", \"steal\", \"construct\", \"alright\", \"benefit\", \"capabl\", \"bear\", \"men\", \"publish\", \"ador\", \"fighter\", \"logist\", \"nonsens\", \"protect\", \"profit\", \"inde\", \"ga\", \"tradit\", \"lake\", \"pve\", \"dimens\", \"\\u2013\", \"ident\", \"cool\", \"sim\", \"suck\", \"aw\", \"truck\", \"scratch\", \"sight\", \"itch\", \"dive\", \"euro\", \"fi\", \"link\", \"sci\", \"spread\", \"underwhelm\", \"fetch\", \"orang\", \"lone\", \"smile\", \"mc\", \"drove\", \"monument\", \"threaten\", \"deficit\", \"zip\", \"race\", \"pure\", \"surprisingli\", \"tough\", \"legend\", \"mmo\", \"trial\", \"ring\", \"lord\", \"wife\", \"shadow\", \"elden\", \"tap\", \"revisit\", \"yard\", \"idiot\", \"hassl\", \"moron\", \"crossbow\", \"investig\", \"hail\", \"crumb\", \"rx\", \"drunkenli\", \"bb\", \"kick\", \"punch\", \"promis\", \"meet\", \"ass\", \"c\", \"complic\", \"chines\", \"languag\", \"hill\", \"45\", \"googl\", \"killer\", \"applic\", \"girlfriend\", \"daughter\", \"daunt\", \"drug\", \"everybodi\", \"videogam\", \"va\", \"bearabl\", \"weav\", \"unsuspect\", \"fold\", \"goliath\", \"trike\", \"cultur\", \"artist\", \"english\", \"outpost\", \"admir\", \"awesom\", \"dont\", \"fuck\", \"smoke\", \"excus\", \"data\", \"heck\", \"clan\", \"joe\", \"drifter\", \"lama\", \"brrrrrr\", \"noon\", \"decay\", \"typewrit\", \"itu\", \"retreat\", \"apc\", \"ventur\", \"recon\", \"gamemod\", \"heli\", \"unarm\", \"didnt\", \"waypoint\", \"deploy\", \"german\", \"unit\", \"server\", \"wargam\", \"isnt\", \"ea\", \"beta\", \"mod\", \"terribl\", \"command\", \"20\", \"cant\", \"addict\", \"crazi\", \"polici\", \"rid\", \"sold\", \"babi\", \"swarm\", \"yell\", \"h\", \"newli\", \"bite\", \"understood\", \"struck\", \"rebel\", \"geniu\", \"daddi\", \"34\", \"thou\", \"yearn\", \"heartfelt\", \"220\", \"mommi\", \"pong\", \"lowlif\", \"divers\", \"realist\", \"word\", \"gem\", \"cri\", \"laugh\", \"dread\", \"pump\", \"basi\", \"pandem\", \"nonetheless\", \"fist\", \"stealthi\", \"netflix\", \"eras\", \"fusion\", \"unholi\", \"lockdown\", \"soni\", \"bloodborn\", \"mop\", \"123\", \"gunsling\", \"74\", \"uncertain\", \"snowboard\", \"unit\", \"15\", \"grab\", \"download\", \"cheap\", \"joke\", \"dollar\", \"buck\", \"app\", \"slave\", \"darn\", \"cure\", \"yellow\", \"christma\", \"thorough\", \"export\", \"defiantli\", \"heartwarm\", \"sfw\", \"hollywood\", \"tsushima\", \"mixtap\", \"cash\", \"greedi\", \"artwork\", \"jeep\", \"2k\", \"fish\", \"20\", \"server\", \"mod\", \"unit\", \"ton\", \"wow\", \"70\", \"200\", \"bot\", \"hot\", \"children\", \"egg\", \"bend\", \"resist\", \"eastern\", \"solar\", \"dc\", \"dad\", \"wholesom\", \"duck\", \"accessori\", \"tomorrow\", \"stabilis\", \"skeleton\", \"easter\", \"160\", \"defi\", \"1300\", \"asynchron\", \"urg\", \"relax\", \"im\", \"self\", \"girl\", \"club\", \"everytim\", \"void\", \"gay\", \"email\", \"platinum\", \"shower\", \"cathart\", \"50fp\", \"strand\", \"chill\", \"nake\", \"sexi\", \"everyday\", \"laid\", \"hallway\", \"muscular\", \"handsom\", \"445\", \"dawg\", \"cackl\", \"kingdom\", \"builder\", \"youth\", \"process\", \"coal\", \"tilt\", \"solv\", \"fish\", \"research\", \"spot\", \"20\", \"low\", \"ship\", \"mod\", \"tactic\", \"depth\", \"car\", \"com\", \"http\", \"id\", \"radar\", \"juic\", \"www\", \"steamcommun\", \"url\", \"l\", \"quot\", \"aswel\", \"filedetail\", \"sharedfil\", \"thingi\", \"hog\", \"in\", \"|\", \"medit\", \"tedious\", \"ziplin\", \"del\", \"documentari\", \"v\", \"youtub\", \"na\", \"gon\", \"wan\", \"east\", \"omg\", \"3080\", \"grade\", \"ti\", \"3070\", \"banger\", \"van\", \"hq\", \"proud\", \"diagnos\", \"rdr2\", \"bachelor\", \"wreck\", \"brillianc\", \"homerun\", \"dumbass\", \"egocentr\", \"\\u56fd\\u4eba\\u73a9\\u5bb6\\u7d20\\u8d28\\u4f4e\\u4e0b\\uff0c\\u5bf9\\u840c\\u65b0\\u53cd\\u6b63\\u4e2a\\u4eba\\u4f53\\u9a8c\\u975e\\u5e38\\u6076\\u52a3\\uff0c\\u770b\\u7740\\u8bc4\\u8bba\\u533a\\u90fd\\u662f\\u7edf\\u4e00\\u7684\\u56fd\\u5916\\u73a9\\u5bb6\\u5bf9\\u4e2d\\u56fd\\u73a9\\u5bb6\\u7684\\u62b1\\u6028\\u3002\\u3002\\u3002\\u3002\\u3002\", \"feloni\", \"misstak\", \"numeri\", \"2022\", \"ea\", \"ignor\", \"rule\", \"ui\", \"report\", \"destruct\", \"casual\", \"unit\", \"fli\", \"mod\", \"low\", \"greatest\", \"format\", \"usa\", \"pl\", \"aint\", \"pod\", \"pleb\", \"mach\", \"whale\", \"squid\", \"bpm\", \"slooooow\", \"breeeeeaaaatherrrrrrr\", \"ussr\", \"halah\", \"unfold\", \"mega\", \"unrespons\", \"atm\", \"cmon\", \"nato\", \"impli\", \"rli\", \"mushroom\", \"copiou\", \"crime\", \"germani\", \"west\", \"union\", \"warno\", \"unit\", \"wargam\", \"depth\", \"skirmish\", \"ship\", \"fleet\", \"rt\", \"steel\", \"20\", \"mod\", \"ui\"], \"Freq\": [25661.0, 11917.0, 8321.0, 7554.0, 5816.0, 2739.0, 2325.0, 4592.0, 4816.0, 1889.0, 8384.0, 3427.0, 8211.0, 3074.0, 1359.0, 5334.0, 3267.0, 4242.0, 2748.0, 1067.0, 6575.0, 2850.0, 3095.0, 3009.0, 4544.0, 6306.890017638296, 5340.864480935227, 4709.83846218333, 4574.012141880901, 4043.7814670933053, 3692.1997974935903, 3393.196830868222, 3174.1286830032413, 2970.3415685695577, 2895.0011919282365, 4706.354602361346, 2652.5597606194046, 2500.5605198229273, 2494.304414996805, 2454.705839997976, 2328.269466700261, 2306.3270448557546, 2281.6068514127405, 2274.9828333043147, 2227.6678167100486, 2328.7908087691044, 4495.909257568378, 1601.0796532367528, 1545.3664501559142, 1435.17523783579, 5337.151662203654, 4681.765994498555, 8311.131045724058, 8116.079108447898, 3169.475290544235, 2580.539024995949, 10362.356021084244, 5473.345895788387, 3727.885543678908, 5239.41673566418, 3032.9578827744754, 6359.016855730157, 3671.5524409296386, 4122.1117312884535, 4986.749788916219, 4274.882326756198, 4352.198329298988, 3788.370434655373, 3817.904818136426, 5422.703343041441, 3689.4146571025904, 1402.5788096809124, 1089.112459063698, 1075.8347446755279, 915.9027772917724, 854.6016665704022, 846.3876557947882, 786.6352137717179, 737.1372094434357, 713.5867543061634, 711.1492965631109, 700.5094644233459, 665.7347521372019, 660.5729271838035, 614.4595939623952, 609.5583148053412, 593.3077480323631, 586.0352536433998, 574.2079047942666, 549.386538898831, 542.1311960475196, 516.2388590501525, 508.9509100954605, 501.75220368208437, 491.7538269292723, 491.9292514018285, 1014.5478763970842, 892.2525644478745, 1185.1519789932847, 1319.0610031477302, 663.7326889047191, 1812.512317152349, 1769.4614727969754, 1120.701955050079, 833.629396633524, 925.2921835487322, 2260.5188442671424, 969.8995754006202, 961.0722910932058, 2178.1468574189307, 1282.0524697493122, 1726.898629081789, 833.433093345699, 776.5730793595071, 880.7221856852937, 918.7449628384975, 794.7246183232602, 1508.4186635142132, 1124.881870078178, 1011.4464194563201, 929.6539256285533, 915.5858219859765, 851.0801517681043, 823.5486265381757, 748.4348068207005, 725.8286390053613, 713.2974089762131, 661.2540530314111, 625.58594000048, 622.2153963582307, 614.407305059241, 596.8082994607778, 545.0719002155413, 531.7236672213306, 519.8263228070596, 490.6282787376516, 485.5541457333824, 484.9020006363768, 473.6079448669297, 458.5759205501453, 451.85265730702486, 448.0476018547608, 1113.2915984810056, 789.6434098994528, 1112.1451700736634, 605.2897563360034, 1006.6395019258073, 983.0591660639373, 1877.1307314599817, 1792.393264749908, 1152.7193069498715, 944.6186308053441, 888.7433962580608, 1388.8374602016422, 1644.2975432283383, 1113.5027001567423, 1414.935804775476, 799.4476196565017, 793.6687403125928, 857.7314801992783, 1014.8752541003405, 826.5178099261984, 860.5547038478064, 805.1274507444132, 2470.8547440531484, 2238.5728154422295, 2195.3805769791247, 2040.4386820138466, 1912.018824110388, 1806.785900059244, 1563.276618570875, 1506.4872373176565, 1478.7215730354335, 1425.69864903459, 1344.5925919852473, 1173.5951507257855, 1035.1122813203372, 985.6674157857323, 981.0224149710629, 948.3164073902037, 935.3714476173835, 867.666487989161, 825.9276425797285, 797.8233051773492, 671.110153721358, 618.1008959537057, 581.2624797650172, 534.7981546121224, 526.610725153463, 2137.667423439479, 731.6943550839316, 2151.770325319837, 1513.7937912774055, 2621.7386354579717, 1528.983375620804, 1437.0984567594537, 1534.314725035745, 1708.0103796531325, 2120.246446910021, 999.0841859901551, 1661.358198024942, 1583.7980946400553, 1466.9195875909415, 1394.896152505369, 5813.204640636264, 1791.8295239745198, 3072.338771863708, 781.9084028782077, 656.2559542709693, 466.4431058178774, 391.776612895279, 340.80882243327596, 253.46393565993586, 252.78083993958526, 238.86235708436993, 226.22333347324354, 185.03519878379902, 182.83551534625877, 167.27345981668833, 166.5537394774178, 156.02901540309665, 148.08739334209693, 137.59076285096333, 128.38553015104986, 127.15749000882477, 125.8377534897121, 104.12031053454426, 25386.46393888633, 90.98979630445828, 4522.339303715292, 3330.251985860653, 7277.446510919485, 1446.6118475660817, 4367.778508979098, 7178.486496424195, 2549.7251695106174, 573.7579950605417, 9222.434013570579, 1039.1751702881922, 1274.2461929091758, 477.45402538553117, 649.5345277216362, 2124.208678010941, 2022.0596250778622, 1226.8946300478115, 848.1041901803422, 484.0594494604806, 507.45292224893933, 835.4223643037311, 1151.6359900904, 861.2803613867272, 406.82205793219237, 316.9116819046775, 292.6575240867234, 246.6699507218047, 223.58868868352658, 214.45574130997284, 194.0285377055563, 191.23546696257011, 187.16119134685596, 182.89675334569668, 181.05355614746094, 168.6655081332916, 164.42749300709028, 163.83785955569195, 152.67749568746657, 146.55899500678044, 146.2164030891012, 143.39402847232986, 134.17048603279875, 131.7241311515273, 127.5226882475809, 127.00175427107325, 122.60169530798021, 118.6689672803586, 146.38386320378805, 191.96467246000807, 165.24977946092224, 169.09117159429664, 161.3708355656917, 153.5103512912817, 153.36748667515386, 153.1577993490856, 156.50830220800734, 156.18286458048772, 154.66003445183574, 945.4715249910229, 881.1869385645581, 595.3086226260912, 467.2782274945417, 403.43532928226347, 350.2159944691596, 330.4090683743779, 316.92855456478014, 302.47581242861247, 274.370233321232, 264.0159873583922, 254.60578090983265, 251.9124160815392, 247.17333867217724, 217.37157397124838, 204.52204672593405, 194.99444244511088, 182.84676662875913, 162.44861352756868, 160.29965704067763, 157.77922248102067, 156.18163821018973, 156.03646820771007, 149.88914058979182, 144.75869342466737, 156.79016181467995, 2738.3069458467216, 2324.1915887532086, 920.7378451025339, 686.460303591832, 569.345247735861, 535.3881682353458, 462.077080832166, 441.34066141648515, 289.568583054182, 172.74786401847297, 142.26703100840555, 129.39831331242627, 75.48976792550168, 74.89005625254269, 69.58066089363304, 65.23697287024407, 63.6542871870578, 57.80370154306722, 50.516163306908055, 49.5712680985306, 111.63614288297549, 33.455432792573305, 30.468161498790792, 27.16651782951135, 25.599808104963337, 32.576463183544725, 622.4979343426766, 523.8640178793408, 487.68341621019505, 440.2389780403061, 421.9717379218862, 401.3993450936776, 373.2271741551137, 360.4587547089705, 309.076818440785, 302.96582378835956, 285.24308567472787, 278.33702487084764, 276.1645391844005, 212.18962517216812, 211.9005357349644, 198.11832832267447, 195.66935160550148, 188.50666144886426, 164.36895702695568, 157.06474539487587, 134.13008583286074, 131.83368461642783, 120.41390305600547, 105.35824572176658, 104.22591480016474, 897.5214910221616, 821.5688852633019, 812.4545938082373, 762.93821800497, 421.4081822215856, 409.4242769297629, 334.45579379366615, 221.0351886874614, 204.19568893650919, 182.71137860184808, 174.32765526596077, 169.516357772386, 146.49550681418418, 124.43173576194593, 101.55937088678346, 101.09871562626161, 98.42075667567909, 96.64889464411267, 75.44236112378987, 60.64399337638722, 52.466582742805606, 51.898561260340834, 50.629191894872186, 46.72584532252351, 44.052569075509474, 390.55635774075034, 203.55592130140712, 243.89977256006014, 104.66018336155206, 441.38541316498026, 360.55339537994826, 345.1318293832291, 312.90539688772583, 301.4760442427698, 279.4428753727474, 261.98551162313794, 261.1881925837533, 250.97983115625232, 204.10444764790793, 183.46260295460897, 173.0577010624464, 148.7260421798575, 144.59786948097238, 143.94391625762523, 136.48231029574987, 119.60038012093146, 116.60725669624154, 109.11313665234287, 103.22584845708407, 98.02019278887589, 95.37421594302367, 90.44717327750234, 88.47259758657708, 82.50701130379807, 450.6275152631969, 213.5628724461375, 1888.475768752028, 632.3837533187063, 363.09681976340283, 357.0773030217323, 287.19492222530545, 269.9244108932481, 246.87862711986932, 194.86986177613736, 182.4457167141985, 158.47893375346501, 149.48481826714803, 147.38432014381345, 130.44807548155845, 121.61963489912547, 111.86312459857302, 110.72752872571357, 108.56334578129508, 82.38398713322471, 79.83713047774216, 76.11560772631715, 70.171957994227, 69.44706773448257, 63.87259407737583, 61.6242153384013, 61.40708069695983, 329.7204080577233, 323.6457697688737, 310.9240997319444, 285.6155385093689, 278.86137765610334, 230.847648201702, 228.22140045540834, 199.06194270262156, 184.06815751232514, 176.2581834542023, 162.90685329023628, 155.3168477161355, 149.8177312421763, 137.55598413106802, 136.79889473940844, 128.9745220486835, 127.09391582486481, 116.1444660999276, 104.43186152519483, 103.30395270026382, 99.57647594827249, 98.53668019761439, 78.79056757494055, 73.87565444435246, 72.85784597943733, 84.4044974338186, 1358.5166163968258, 789.4849240809253, 651.354112199453, 398.2527122962366, 306.1554578845813, 273.09243302336375, 235.73300597973838, 212.62026441671418, 142.30043825598844, 120.51506020816898, 111.22080954379287, 107.95230046424723, 105.81635480879339, 94.35647604006138, 79.69730915396194, 69.12729649262027, 69.06618167734642, 48.68157948999031, 46.24507799228573, 34.649890312948905, 27.596063194469192, 23.27488254149795, 20.723364561028916, 14.877072201947788, 11.95067048436542, 1066.0552792153555, 238.7955514272125, 229.7027849654968, 169.93804856424455, 168.29865452359581, 161.40312126884177, 148.91892005186364, 146.4357087074669, 129.62635488466273, 125.92611904862669, 77.14563681258386, 73.24002009189732, 69.82202400541965, 67.93007113722442, 57.831810631225764, 38.735293181681826, 37.36891668031571, 35.210125708506155, 35.07653808783926, 26.529236984618574, 26.30617557374301, 23.265724656066062, 19.026514429526383, 13.932874529315914, 8.021058462639003, 152.62248458943372, 67.52352999076592, 501.99860984534195, 289.9216900046751, 268.4014265505595, 187.6588221530774, 184.76711467357447, 169.13515674153305, 131.5157101677359, 112.47224720369256, 110.84568766478179, 74.59292191943403, 73.40284009217659, 57.964958462774455, 47.06078782003781, 45.71962398547349, 40.76011028643136, 39.55600916187822, 39.25499558578301, 38.602820684797486, 37.1748188078715, 17.679511082777477, 9.906259704322162, 4.549764453231257, 4.342264138343647, 3.034994973220635, 2.7241025497958815, 37.14718821763842, 22.03683710332425, 61.3972784056467, 16.382946806594695, 10.70576765453525, 838.8408808598424, 708.59031971429, 554.7094934646168, 168.78285205314214, 123.09575949375485, 78.97600392478586, 53.09030477316033, 35.05045154054662, 27.300847333743935, 5.38334250125533, 4.1618540496289915, 4.05153866061547, 4.023837047590337, 3.666918591300142, 1.4423739088581176, 1.3321419922533317, 0.038742297638640476, 0.038742297638640476, 0.038742297638640476, 0.038742297638640476, 0.038742297638640476, 0.038742297638640476, 0.038742297638640476, 0.03874230088709972, 0.038742297638640476, 0.03874230088709972, 0.03874230088709972, 0.038742323626314426, 0.03874230738401821, 0.0387423138809367, 0.03874230088709972, 0.03874230738401821, 0.038742304135558965, 0.038742310632477454, 0.038742304135558965, 0.038742304135558965, 0.03874230738401821, 643.6892274361081, 486.0471222765888, 272.97680393899736, 156.14900443476117, 113.47927827936938, 100.32729926768367, 89.37560561736765, 75.89329903084605, 75.15480233785944, 73.35406034139221, 56.407410975150015, 52.05843585971806, 43.3997178683886, 42.16379744101293, 39.90931984727919, 33.09288349683654, 20.37239873340066, 18.759583105325607, 6.654648652334926, 4.346668505107742, 3.7479676533381636, 3.385239473679349, 2.957331277460458, 2.483403503766344, 1.445216710105555, 60.84391492052195, 862.7413129244028, 614.7222797685214, 174.63595452813138, 150.78971961372685, 135.45916774056823, 90.81254529348601, 85.74926040660722, 82.20868094001705, 56.833716337459165, 46.97324957590513, 24.856020074646004, 20.016977021873352, 17.61839076176233, 15.193970620047065, 14.201389858327182, 9.065988373531313, 8.197814823612978, 5.486902079855887, 5.164999754277813, 2.127985509875307, 1.3082033259300188, 0.9295271673262228, 0.5636859031745542, 0.35333404352786263, 0.03725421342574892, 0.03725422256095713, 543.5868571653107, 314.56430906167174, 276.02978881176585, 236.77743803661104, 236.29274533111132, 225.52726934136965, 191.32392579028982, 68.15217750995421, 31.103771887026234, 28.428949849972923, 19.098178709713803, 16.640142397637344, 12.72829836397756, 11.802536414306804, 10.167420593411602, 7.873297981450383, 7.530311094191002, 3.8939787092395663, 3.4203786152813636, 1.0063671083839423, 0.6794652253881848, 0.040289739374554996, 0.040289724622737395, 0.040289724622737395, 0.040289724622737395, 0.04028973347382795, 0.04028973347382795, 0.040289751176009074, 0.04028973642419147, 0.040289739374554996, 0.040289739374554996, 683.01250693422, 228.99605936475683, 163.86051578035506, 155.43315677998336, 146.3289922162918, 127.83223737698451, 101.81533526354217, 73.15616938324513, 41.04736789373219, 34.65991437123402, 33.11322800763825, 31.118091216354685, 27.807730391974978, 22.740646584476977, 22.57074572646158, 20.591629799118614, 19.807040801277545, 17.770632659331202, 14.381806107698845, 12.340916917010725, 9.808167138948821, 4.000004466942256, 2.5032410061761823, 1.374007927657109, 1.365759143883205, 6.632018322476444, 600.0060951988636, 537.2562524397664, 152.6465589852863, 88.13069382690864, 86.60826553771788, 72.1920560012271, 45.86783863150891, 44.425308811583335, 40.19137046848652, 29.480688011538756, 20.060891061211827, 14.365086431632275, 4.851776090521716, 9.610374691880093, 0.03759564230279105, 0.03759563013103127, 0.03759563013103127, 0.03759563013103127, 0.03759563013103127, 0.03759562769667931, 0.03759562769667931, 0.03759562769667931, 0.03759562769667931, 0.03759562769667931, 0.03759562769667931, 0.03759563499973518, 0.03759564717149496, 0.03759563013103127, 0.03759563499973518, 0.03759563013103127, 0.03759563013103127, 0.03759563499973518, 0.03759563499973518, 0.03759563499973518, 0.03759563499973518, 0.03759564230279105, 0.037595637434087135, 0.037595637434087135, 0.037595637434087135, 0.03759563499973518, 0.03759563499973518, 801.1645484963062, 154.0175623726056, 134.57296280769893, 91.18582453834067, 83.30680690543385, 67.43752637508076, 38.273225950932556, 35.73778835565568, 33.92370684652695, 27.09540383509265, 27.052447110668542, 26.141471316773497, 24.213905070601626, 24.213905070601626, 14.092111298215029, 11.965479121576067, 11.509713359771961, 7.3672768825467765, 5.89368623906043, 5.495705725792811, 0.9538159037596603, 0.9043484953648533, 0.5375355509785241, 29.598117227089542, 37.443977896982304, 436.0131336569693, 279.2026986647346, 149.20950776455808, 54.25679690637782, 39.583400410480444, 28.332126678361604, 25.741655064991402, 25.731047229195394, 23.87353260534701, 22.105605848216282, 12.3645830375699, 4.006793695400932, 26.6577507286002, 2.5058751374521853, 1.8034242775071505, 0.4197286074768009, 0.030806974127693704, 0.030806970681514514, 0.030806970681514514, 0.030806970681514514, 0.030806970681514514, 0.030806970681514514, 0.030806970681514514, 0.030806970681514514, 0.030806970681514514, 0.030806977573872893, 0.030806984466231275, 0.030806977573872893, 0.030806974127693704, 0.030806984466231275, 0.030806974127693704, 0.030806974127693704, 0.030806974127693704, 0.030806977573872893, 0.030806974127693704, 0.030806974127693704, 0.030806974127693704, 66.08678846166664, 58.01433099193573, 36.64550103697841, 25.388803792801614, 21.866359561336825, 9.515024886050647, 4.446041872468985, 3.868783995371948, 3.5574331864911413, 0.7478212137647301, 0.09582333816006958, 0.09582295506415868, 0.09582274878174511, 0.017955151573906498, 0.01795514789029197, 0.01795514789029197, 0.01795514789029197, 0.01795514789029197, 0.01795514911816348, 0.01795514789029197, 0.01795514789029197, 0.01795514666242046, 0.01795514666242046, 0.01795514666242046, 0.01795514666242046, 0.01795514789029197, 0.01795514789029197, 0.01795514789029197, 0.01795514789029197, 0.017955151573906498, 0.01795515771326404, 0.017955154029649516, 0.017955151573906498, 0.01795514911816348, 0.017955151573906498, 0.01795514911816348, 0.01795514911816348, 0.01795514911816348, 0.01795514911816348, 0.01795514911816348, 0.01795514911816348], \"Total\": [25661.0, 11917.0, 8321.0, 7554.0, 5816.0, 2739.0, 2325.0, 4592.0, 4816.0, 1889.0, 8384.0, 3427.0, 8211.0, 3074.0, 1359.0, 5334.0, 3267.0, 4242.0, 2748.0, 1067.0, 6575.0, 2850.0, 3095.0, 3009.0, 4544.0, 6307.885516051087, 5341.859973693565, 4710.838641538813, 4575.007635621447, 4044.8050794972187, 3693.195290213181, 3394.192323587813, 3175.124175722832, 2971.36261544051, 2895.9966846478274, 4708.012126740741, 2653.5552533389955, 2501.556013877168, 2495.299916094869, 2455.7013337385197, 2329.2650834980277, 2307.3318536364823, 2282.6023441323314, 2275.9783260239055, 2228.6665378374546, 2329.873263419775, 4498.450481201285, 1602.075145956344, 1546.3619428755053, 1436.170730579252, 5349.25322674011, 4691.5088228608165, 8384.457063318378, 8211.298176183876, 3176.639744457712, 2583.6956893663155, 10629.965559518107, 5558.4082972181795, 3762.0834871994284, 5334.66262264118, 3048.241768720532, 6677.202199631809, 3749.6941098244156, 4242.35625608854, 5260.849141215277, 4452.552746193924, 4544.472149946689, 3936.0887758287868, 4033.4766810691517, 6575.287129450865, 4511.96758558774, 1403.572997399872, 1090.1066467826577, 1076.8289323944875, 916.8969650107316, 855.5958542893615, 847.3818435137474, 787.6294014906771, 738.131397198536, 714.5809420251226, 712.1434843317202, 701.5036524997854, 666.7289398561611, 661.5676335550694, 615.4537816813544, 610.5525025243004, 594.3019357513223, 587.0294419727413, 575.2020925132258, 550.3807266177902, 543.1253837664788, 517.2330467691118, 509.94724913111486, 502.7467080368774, 492.748276823505, 492.9314998780276, 1017.486409473142, 897.7827451742608, 1206.841087986755, 1371.5458425628126, 671.9547682743569, 1928.7958723840336, 1902.9749897271972, 1209.4382270598576, 870.7517781415986, 998.6600696861634, 2955.009510979626, 1092.4677439560362, 1097.3719313848294, 3280.3569264409275, 1645.530625258252, 2641.454068619372, 1007.7906302929056, 920.3222299551446, 1636.898435369855, 2247.279132104324, 1073.1632748453449, 1509.410721572485, 1125.87392801333, 1012.4384810160961, 930.6461378699642, 916.5778796801876, 852.0722094623154, 824.5406842323868, 749.4276469045886, 726.8206966995724, 714.2894666784556, 662.2461107256222, 626.5779977228009, 623.2074540524418, 615.3993635793792, 597.8003571549889, 546.0639579097524, 532.7157250239649, 520.8189467847302, 491.6219950973814, 486.5462163337718, 485.894058330588, 474.6000025611409, 459.5682056190835, 452.8448738049425, 449.039659548972, 1117.6307425301638, 792.4313745216521, 1121.8011534620148, 607.7038731430313, 1020.2561732424138, 997.7022141583942, 1940.7744048354025, 1854.6268439433097, 1206.525109349348, 978.1810218212587, 916.9850607457274, 1514.7777786309732, 1981.0985449208688, 1342.26119085662, 1932.5663265738358, 907.5513399805304, 946.181475871513, 1243.491399155479, 2462.0402679323493, 1213.6375820990622, 1639.6513506339018, 1957.1213364946148, 2471.854177311937, 2239.572248817473, 2196.3800127436934, 2041.4381235047654, 1913.0182573691768, 1807.7853333180328, 1564.2760518296636, 1507.4866705764452, 1479.7210062942222, 1426.6981007132717, 1345.592025244036, 1174.5945839845742, 1036.1117146124177, 986.6668490445206, 982.0218482539452, 949.3158406489921, 936.3708808761719, 868.6659212479494, 826.9270758385169, 798.8227384361376, 672.1095869801463, 619.1003292124941, 582.2619130238056, 535.7975878709108, 527.6101584122514, 2149.73068646507, 735.200028307333, 2198.205499800787, 1536.4271009118477, 2748.699328166136, 1575.7492413341586, 1495.06862883357, 1627.4150782346396, 1854.0906838693575, 2435.2958275869973, 1023.0045184417355, 2208.6809316773883, 2283.764069756619, 2563.436517717612, 3657.0062942090785, 5816.30600434067, 1792.8255082999556, 3074.2975962898176, 782.9043872036428, 657.2519385964044, 467.4390901433126, 392.7726022563685, 341.8048067587112, 254.4599199853712, 253.7768242650206, 239.85834140980526, 227.21931779867887, 186.03118310923435, 183.8314996716941, 168.26944414212366, 167.54972380285312, 157.02499972853198, 149.08337766753226, 138.58674717639866, 129.3815144764852, 128.15347433426007, 126.83374479905811, 105.1163611725735, 25661.377814792497, 91.98578062989361, 4592.159423296505, 3427.859054071312, 7554.848768836863, 1535.6667907077108, 4816.384238755431, 8321.139057763154, 2850.833388369092, 619.8951530102053, 11917.209966534103, 1175.7166896784088, 1471.3607053639666, 533.8924690517168, 761.8903311990221, 3009.486631026759, 2851.6518919818236, 1612.438819388607, 1313.8259030721135, 619.6672678063021, 694.1730749719115, 2345.745873735956, 6575.287129450865, 862.2847489502651, 407.8264454957303, 317.9160694682154, 293.66191165026135, 247.67433828534269, 224.59307624706457, 215.46012887351083, 195.03292526909428, 192.2398545261081, 188.16557891039395, 183.90114090923467, 182.05794371099893, 169.6698956968296, 165.43188057062827, 164.84224711922994, 153.68188325100456, 147.56338257031842, 147.22079065263918, 144.39841603586785, 135.17487359633674, 132.7285187150653, 128.5270758111189, 128.00614183461124, 123.6060828715182, 119.67335484389659, 177.34880111991103, 338.3224167548801, 372.0033428089952, 497.78789839716336, 398.55944911813987, 374.3967782389341, 454.7386834348454, 466.65929653636357, 1205.0140387111567, 1213.6375820990622, 1051.746339587547, 946.467192673232, 882.1826062467673, 596.3042903083003, 468.27389517675095, 404.43099696447274, 351.2116621513689, 331.4047360565872, 317.9242222469894, 303.47148011082174, 275.36590100344125, 265.01165504060145, 255.60144859204195, 252.9080837637485, 248.1690067335545, 218.36724165345768, 205.51771440814335, 195.99011012732018, 183.84243431096843, 163.44428120977798, 161.29532472288693, 158.77489016322997, 157.17730589239903, 157.03213588991937, 150.88480827200112, 145.75436110687667, 229.5753357948913, 2739.309792643013, 2325.1944355495, 921.7406918988252, 687.4631503881233, 570.3480945321522, 536.391015031637, 463.07992762845726, 442.3435082127764, 290.57142985047324, 173.7507108147643, 143.26987780469688, 130.4011601087176, 76.49261472179303, 75.89290304883404, 70.58350768992439, 66.23981966653542, 64.65713398334915, 58.806548339358564, 51.5190101031994, 50.574114894821946, 114.1975730182148, 34.45827958886465, 31.47100829508213, 28.169364625802686, 26.602654901254674, 37.71733463997321, 623.4938595732378, 524.8599431099021, 488.67934144075633, 441.2349081853606, 422.96766315244747, 402.3952703242389, 374.22309938567497, 361.4546799395318, 310.0727436713463, 303.96174901892084, 286.23901090528915, 279.3329501014089, 277.1604644149618, 213.1855504027295, 212.89646096552576, 199.11425355323584, 196.66527683606284, 189.50258667942563, 165.36488225751705, 158.06067062543724, 135.1260110634221, 132.8296098469892, 121.40982828656686, 106.35417095232798, 105.22184003072614, 898.5226428262711, 822.5700370674114, 813.4557456123467, 763.9393698090795, 422.40933402569493, 410.42542873387225, 335.4569455977755, 222.03634049157088, 205.19684074061865, 183.71253040595755, 175.32880707007024, 170.51750957649546, 147.49665861829365, 125.43288756605538, 102.56052269089291, 102.09986743037106, 99.42190847978854, 97.65004644822213, 76.44351292789932, 61.64514518049668, 53.46773454691506, 52.89971306445029, 51.63034369898164, 47.726997126632966, 45.05372087961893, 400.8969767494528, 234.1210451295581, 377.6372363794789, 240.5011632018065, 442.3794176611346, 361.5473998761026, 346.12583387938344, 313.89940138388016, 302.4700487389241, 280.4368798689017, 262.97951611929227, 262.1821970799076, 251.97383565240673, 205.09845214406235, 184.45660745076339, 174.05170555860082, 149.72004667601192, 145.5918739771268, 144.93792075377965, 137.4763147919043, 120.59438461708588, 117.60126119239595, 110.10714114849729, 104.21985295323849, 99.0141972850303, 96.36822043917809, 91.44117777365676, 89.4666020827315, 83.50101579995248, 462.11591148450754, 225.81750749471945, 1889.4741132957317, 633.3820978624101, 364.09516430710653, 358.075647565436, 288.19326676900914, 270.9227554369518, 247.87697166357307, 195.8682063198411, 183.44406125790226, 159.47727829716877, 150.48316281085178, 148.3826646875172, 131.4464200252622, 122.61797944282922, 112.86146914227677, 111.72587326941732, 109.56169032499884, 83.38233167692846, 80.83547502144592, 77.1139522700209, 71.17030253793075, 70.44541227818632, 64.87093862107959, 62.62255988210505, 62.40542524066358, 330.71605068907564, 324.64141240022605, 311.91974236329673, 286.6111811407212, 279.85702028745567, 231.8432908330544, 229.21704308676075, 200.05758533397398, 185.06380014367755, 177.25382608555472, 163.9024959215887, 156.3124903474879, 150.81337387352872, 138.55162676242043, 137.79453737076085, 129.9701646800359, 128.08955845621722, 117.14010873128002, 105.42750415654726, 104.29959533161625, 100.57211857962491, 99.53232282896681, 79.78621020629298, 74.87129707570489, 73.85348861078975, 104.34511102874676, 1359.5170473168569, 790.4853550009564, 652.354543119484, 399.2531432162676, 307.1558888046123, 274.0928639433947, 236.73343689976946, 213.62069533674526, 143.30086917601952, 121.51549112820004, 112.22124046382393, 108.9527313842783, 106.81678572882446, 95.35690696009245, 80.697740073993, 70.12772741265134, 70.06661259737749, 49.68201041002136, 47.24550891231678, 35.65032123297996, 28.59649411450025, 24.275313461529006, 21.723795481059973, 15.877503121978844, 12.951101404396475, 1067.0540301556025, 239.79430236745947, 230.70153590574375, 170.93679950449152, 169.29740546384278, 162.40187220908874, 149.9176709921106, 147.43445964771385, 130.6251058249097, 126.92486998887365, 78.14438775283082, 74.23877103214429, 70.82077494566661, 68.92882207747138, 58.83056157147272, 39.73404412192878, 38.36766762056266, 36.20887664875311, 36.07528902808622, 27.52798792486553, 27.304926513989965, 24.26447559631302, 20.02526536977334, 14.93162546956287, 9.01980940288596, 295.69459539318063, 124.06973948144699, 503.003948563606, 290.92702872293916, 269.40676526882356, 188.66416087134155, 185.7724533918386, 170.1404954597972, 132.52104888600005, 113.4775859219567, 111.85102638304593, 75.59826063769817, 74.40817881044073, 58.97029718103859, 48.066126538301944, 46.724962703737624, 41.7654490046955, 40.56134788014236, 40.26033430404714, 39.60815940306162, 38.18015752613564, 18.684849801041608, 10.911598422586291, 5.555103171495387, 5.347602856607777, 4.040333691484765, 3.729441268060012, 55.521251364914995, 42.44259200101026, 355.51775927814975, 44.80548714076415, 42.87555948932527, 839.8458192979527, 709.5952581524003, 555.7144319027271, 169.78779049125248, 124.1006979318652, 79.9809423628962, 54.095243211270656, 36.05538997865695, 28.30578577185426, 6.388280939365656, 5.166792487739317, 5.056477098725796, 5.028775485700663, 4.671857029410468, 2.4473123469684444, 2.3370804303636588, 1.04368074576225, 1.0436807526158784, 1.0436807624440807, 1.0436807638304604, 1.0436807754106716, 1.0436807780837105, 1.0436807784421316, 1.0436808691445802, 1.0436807818858398, 1.043680886040887, 1.0436808980555994, 1.043682118945113, 1.0436813273686505, 1.0436817210052018, 1.0436809582486146, 1.0436813946577903, 1.0436811923319689, 1.0436817492568744, 1.0436812550095187, 1.0436812617976505, 1.0436817290925668, 644.6951881085557, 487.0530829490364, 273.98276461144496, 157.15496510720882, 114.48523895181704, 101.33325994013133, 90.38156628981531, 76.89925970329371, 76.1607630103071, 74.36002101383987, 57.41337164759766, 53.0643965321657, 44.40567854083624, 43.169758113460574, 40.915280519726835, 34.098844169284185, 21.37835940584831, 19.765543777773257, 7.660609324782574, 5.352629177555389, 4.753928325785811, 4.391200146126996, 3.9632919499081067, 3.4893641762139924, 2.4511773825532037, 156.52974432229703, 863.747739455251, 615.7287062993696, 175.64238105897965, 151.79614614457512, 136.4655942714165, 91.81897182433428, 86.7556869374555, 83.21510747086532, 57.84014286830742, 47.97967610675338, 25.862446605494263, 21.02340355272161, 18.62481729261059, 16.200397150895324, 15.207816389175438, 10.072414904379569, 9.204241354461233, 6.493328610704143, 6.17142628512607, 3.1344120407235643, 2.314629856778276, 1.9359536981744796, 1.570112434022811, 1.3597605743761194, 1.0436810500547349, 1.043682118945113, 544.5902481725923, 315.56770006895323, 277.03317981904735, 237.78082904389262, 237.2961363383929, 226.53066034865122, 192.3273167975714, 69.1555685172358, 32.10716289430781, 29.432340857254502, 20.10156971699538, 17.643533404918923, 13.731689371259138, 12.805927421588382, 11.170811600693181, 8.876688988731962, 8.53370210147258, 4.897369716521145, 4.423769622562942, 2.0097581156655213, 1.6828562326697636, 1.0436810000304984, 1.0436807701421145, 1.0436807733080828, 1.04368079327639, 1.0436810395202782, 1.04368107130705, 1.0436817290925668, 1.0436813273686505, 1.0436817492568744, 1.043682118945113, 684.0162606960286, 229.99981312656536, 164.86426954216358, 156.4369105417919, 147.33274597810032, 128.83599113879302, 102.8190890253507, 74.15992314505367, 42.05112165554071, 35.663668133042535, 34.11698176944677, 32.1218449781632, 28.8114841537835, 23.7444003462855, 23.574499488270103, 21.595383560927136, 20.810794563086066, 18.774386421139724, 15.385559869507365, 13.344670678819245, 10.811920900757341, 5.003758228750777, 3.5069947679847036, 2.3777616894656304, 2.3695129056917263, 22.482972043765745, 601.0121803020392, 538.262337542942, 153.65264408846193, 89.13677893008426, 87.6143506408935, 73.19814110440272, 46.873923734684524, 45.43139391475895, 41.19745557166213, 30.48677311471437, 21.06697616438744, 15.371171534807889, 5.85786119369733, 25.929953807516753, 1.043681077719997, 1.0436807763048104, 1.0436807775219918, 1.0436807843532583, 1.0436807877284358, 1.0436807308722933, 1.0436807308722933, 1.0436807308722933, 1.0436807308722933, 1.0436807308722933, 1.0436807308722933, 1.043680940647262, 1.0436812936072364, 1.043680813042757, 1.0436809897657229, 1.043680829374716, 1.0436808299715308, 1.0436810701122743, 1.04368107130705, 1.0436810783463744, 1.0436811473298593, 1.0436817290925668, 1.0436814037064888, 1.0436816669133955, 1.0436817492568744, 1.0436815627543485, 1.0436814391824065, 802.1740840192617, 155.02709789556113, 135.58249833065446, 92.19536006129623, 84.3163424283894, 68.44706189803632, 39.28276147388811, 36.74732387861123, 34.9332423694825, 28.1049393580482, 28.06198263362409, 27.151006839729046, 25.223440593557175, 25.223440593557175, 15.101646821170577, 12.975014644531614, 12.519248882727508, 8.376812405502324, 6.903221762015979, 6.5052412487483595, 1.963351426715209, 1.913884018320402, 1.547071073934073, 234.1210451295581, 312.7758045920812, 437.0260074191837, 280.21557242694905, 150.2223815267725, 55.26967066859226, 40.59627417269488, 29.345000440576037, 26.754528827205835, 26.743920991409826, 24.88640636756144, 23.118479610430715, 13.377456799784333, 5.019667457615365, 34.89361026031662, 3.5187488996666185, 2.8162980397215835, 1.432602369691234, 1.0436808135177218, 1.0436807328959476, 1.0436807328959476, 1.0436807361618636, 1.0436807361618636, 1.0436807361618636, 1.0436807427589219, 1.0436807427589219, 1.0436807427589219, 1.0436810482691985, 1.0436813946577903, 1.0436810932254013, 1.0436809469127233, 1.0436818358222761, 1.0436810486821837, 1.0436810619158805, 1.043681114572878, 1.043682118945113, 1.0436812874371137, 1.0436817492568744, 1.0436814037064888, 67.11251404346355, 59.04005657373264, 37.67122661877532, 26.414529374598526, 22.892085143133738, 10.540750467847559, 5.471767454265897, 4.89450957716886, 4.583158768288053, 1.773546795561642, 1.1215489199569812, 1.1215485368610705, 1.1215483305786569, 1.0436808245020872, 1.0436807498927965, 1.0436807744403622, 1.0436807766478131, 1.043680777608003, 1.0436808496926355, 1.0436807785092945, 1.0436807847088765, 1.0436807284593321, 1.0436807284593321, 1.0436807284593321, 1.0436807284593321, 1.0436808004643567, 1.043680803860127, 1.0436808068754775, 1.043680817661162, 1.043681229359097, 1.043682118945113, 1.0436817210052018, 1.0436814391824065, 1.0436810460743084, 1.0436816669133955, 1.0436811898896172, 1.043681340073452, 1.0436813419740385, 1.0436817290925668, 1.0436817492568744, 1.0436818358222761], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\"], \"logprob\": [25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.4955, -4.6617, -4.7874, -4.8167, -4.9399, -5.0309, -5.1153, -5.1821, -5.2484, -5.2741, -4.7882, -5.3616, -5.4206, -5.4231, -5.4391, -5.492, -5.5014, -5.5122, -5.5151, -5.5361, -5.4918, -4.8339, -5.8664, -5.9018, -5.9758, -4.6624, -4.7934, -4.2195, -4.2433, -5.1835, -5.3891, -3.9989, -4.6372, -5.0213, -4.6809, -5.2276, -4.4872, -5.0365, -4.9207, -4.7303, -4.8843, -4.8664, -5.0052, -4.9974, -4.6465, -5.0316, -4.5304, -4.7833, -4.7956, -4.9565, -5.0258, -5.0355, -5.1087, -5.1737, -5.2061, -5.2096, -5.2246, -5.2756, -5.2833, -5.3557, -5.3637, -5.3907, -5.4031, -5.4235, -5.4676, -5.4809, -5.5299, -5.5441, -5.5583, -5.5785, -5.5781, -4.8542, -4.9827, -4.6988, -4.5918, -5.2786, -4.274, -4.298, -4.7547, -5.0507, -4.9463, -4.0531, -4.8993, -4.9084, -4.0902, -4.6202, -4.3224, -5.0509, -5.1216, -4.9957, -4.9534, -5.0985, -4.4147, -4.7081, -4.8144, -4.8987, -4.9139, -4.987, -5.0199, -5.1155, -5.1462, -5.1636, -5.2394, -5.2948, -5.3002, -5.3128, -5.3419, -5.4326, -5.4574, -5.48, -5.5378, -5.5482, -5.5496, -5.5731, -5.6054, -5.6202, -5.6286, -4.7184, -5.0619, -4.7195, -5.3278, -4.8191, -4.8428, -4.196, -4.2422, -4.6836, -4.8827, -4.9437, -4.4973, -4.3284, -4.7182, -4.4787, -5.0496, -5.0568, -4.9792, -4.811, -5.0163, -4.9759, -5.0425, -3.853, -3.9517, -3.9712, -4.0444, -4.1094, -4.166, -4.3108, -4.3478, -4.3664, -4.4029, -4.4615, -4.5975, -4.723, -4.772, -4.7767, -4.8106, -4.8244, -4.8995, -4.9488, -4.9834, -5.1564, -5.2387, -5.3001, -5.3834, -5.3988, -3.9978, -5.0699, -3.9913, -4.3429, -3.7937, -4.3329, -4.3949, -4.3295, -4.2222, -4.006, -4.7585, -4.2499, -4.2977, -4.3744, -4.4247, -2.8966, -4.0735, -3.5343, -4.9027, -5.0779, -5.4193, -5.5938, -5.7331, -6.0292, -6.0319, -6.0886, -6.1429, -6.3439, -6.3559, -6.4448, -6.4491, -6.5144, -6.5667, -6.6402, -6.7094, -6.719, -6.7295, -6.9189, -1.4225, -7.0537, -3.1477, -3.4537, -2.6719, -4.2875, -3.1825, -2.6856, -3.7207, -5.2123, -2.4351, -4.6183, -4.4144, -5.396, -5.0882, -3.9033, -3.9526, -4.4522, -4.8215, -5.3823, -5.3351, -4.8365, -4.5155, -3.0216, -3.7716, -4.0214, -4.101, -4.272, -4.3702, -4.4119, -4.512, -4.5265, -4.548, -4.5711, -4.5812, -4.6521, -4.6775, -4.6811, -4.7517, -4.7926, -4.7949, -4.8144, -4.8809, -4.8993, -4.9317, -4.9358, -4.9711, -5.0037, -4.7938, -4.5227, -4.6726, -4.6496, -4.6963, -4.7462, -4.7472, -4.7485, -4.7269, -4.729, -4.7388, -2.6041, -2.6745, -3.0667, -3.3089, -3.4558, -3.5972, -3.6555, -3.6971, -3.7438, -3.8413, -3.8798, -3.9161, -3.9267, -3.9457, -4.0742, -4.1351, -4.1828, -4.2471, -4.3654, -4.3787, -4.3946, -4.4048, -4.4057, -4.4459, -4.4807, -4.4009, -1.4351, -1.5991, -2.525, -2.8187, -3.0057, -3.0672, -3.2145, -3.2604, -3.6818, -4.1984, -4.3925, -4.4873, -5.0262, -5.0342, -5.1077, -5.1722, -5.1967, -5.2932, -5.4279, -5.4468, -4.635, -5.84, -5.9335, -6.0482, -6.1076, -5.8666, -2.7816, -2.9541, -3.0257, -3.128, -3.1704, -3.2204, -3.2932, -3.328, -3.4817, -3.5017, -3.562, -3.5865, -3.5943, -3.8579, -3.8592, -3.9265, -3.9389, -3.9762, -4.1132, -4.1587, -4.3165, -4.3338, -4.4244, -4.558, -4.5688, -2.295, -2.3834, -2.3945, -2.4574, -3.051, -3.0798, -3.2821, -3.6963, -3.7755, -3.8867, -3.9337, -3.9616, -4.1076, -4.2708, -4.474, -4.4785, -4.5053, -4.5235, -4.7712, -4.9896, -5.1344, -5.1453, -5.1701, -5.2503, -5.3092, -3.127, -3.7787, -3.5978, -4.4439, -2.9575, -3.1598, -3.2035, -3.3015, -3.3387, -3.4146, -3.4791, -3.4822, -3.522, -3.7288, -3.8354, -3.8938, -4.0453, -4.0734, -4.078, -4.1312, -4.2632, -4.2886, -4.355, -4.4105, -4.4622, -4.4896, -4.5426, -4.5647, -4.6345, -2.9368, -3.6835, -1.4024, -2.4964, -3.0512, -3.0679, -3.2857, -3.3478, -3.437, -3.6736, -3.7394, -3.8803, -3.9387, -3.9529, -4.0749, -4.145, -4.2286, -4.2388, -4.2586, -4.5345, -4.5659, -4.6136, -4.695, -4.7053, -4.789, -4.8248, -4.8284, -3.1021, -3.1206, -3.1607, -3.2457, -3.2696, -3.4585, -3.47, -3.6067, -3.685, -3.7283, -3.8071, -3.8548, -3.8909, -3.9763, -3.9818, -4.0407, -4.0554, -4.1455, -4.2518, -4.2626, -4.2994, -4.3099, -4.5335, -4.5979, -4.6118, -4.4647, -1.5588, -2.1015, -2.2939, -2.7858, -3.0488, -3.1631, -3.3102, -3.4134, -3.815, -3.9811, -4.0614, -4.0912, -4.1112, -4.2258, -4.3947, -4.537, -4.5378, -4.8876, -4.939, -5.2276, -5.4552, -5.6255, -5.7416, -6.0731, -6.2921, -1.4582, -2.9543, -2.9932, -3.2945, -3.3042, -3.346, -3.4265, -3.4434, -3.5653, -3.5943, -4.0843, -4.1362, -4.184, -4.2115, -4.3724, -4.7732, -4.8091, -4.8686, -4.8724, -5.1517, -5.1601, -5.283, -5.4841, -5.7957, -6.3479, -3.402, -4.2175, -1.9676, -2.5166, -2.5937, -2.9515, -2.9671, -3.0555, -3.307, -3.4635, -3.478, -3.8741, -3.8902, -4.1263, -4.3347, -4.3636, -4.4785, -4.5084, -4.5161, -4.5328, -4.5705, -5.3138, -5.893, -6.6711, -6.7178, -7.076, -7.184, -4.5713, -5.0934, -4.0688, -5.3899, -5.8154, -1.4488, -1.6175, -1.8624, -3.0522, -3.3678, -3.8117, -4.2088, -4.624, -4.8739, -6.4975, -6.7548, -6.7817, -6.7886, -6.8815, -7.8145, -7.894, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -11.4316, -1.668, -1.9489, -2.5258, -3.0844, -3.4036, -3.5268, -3.6424, -3.8059, -3.8157, -3.8399, -4.1026, -4.1829, -4.3648, -4.3937, -4.4486, -4.6359, -5.1211, -5.2035, -6.2399, -6.6658, -6.814, -6.9158, -7.0509, -7.2256, -7.767, -4.0269, -1.356, -1.695, -2.9534, -3.1003, -3.2075, -3.6073, -3.6647, -3.7069, -4.076, -4.2666, -4.903, -5.1196, -5.2472, -5.3952, -5.4628, -5.9116, -6.0123, -6.4138, -6.4742, -7.361, -7.8475, -8.1892, -8.6894, -9.1565, -11.4061, -11.4061, -1.7864, -2.3334, -2.464, -2.6174, -2.6195, -2.6661, -2.8306, -3.8628, -4.6472, -4.7371, -5.135, -5.2727, -5.5407, -5.6162, -5.7654, -6.0211, -6.0656, -6.7251, -6.8548, -8.0782, -8.471, -11.2962, -11.2962, -11.2962, -11.2962, -11.2962, -11.2962, -11.2962, -11.2962, -11.2962, -11.2962, -1.4773, -2.5701, -2.9048, -2.9576, -3.0179, -3.1531, -3.3806, -3.7112, -4.289, -4.4582, -4.5038, -4.566, -4.6785, -4.8796, -4.8871, -4.9789, -5.0177, -5.1262, -5.3378, -5.4909, -5.7206, -6.6175, -7.0862, -7.686, -7.6921, -6.1119, -1.4954, -1.6058, -2.8642, -3.4135, -3.4309, -3.613, -4.0665, -4.0985, -4.1986, -4.5086, -4.8935, -5.2275, -6.313, -5.6295, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -11.1732, -1.1535, -2.8025, -2.9375, -3.3267, -3.4171, -3.6284, -4.1949, -4.2634, -4.3155, -4.5402, -4.5418, -4.5761, -4.6527, -4.6527, -5.194, -5.3576, -5.3964, -5.8426, -6.0657, -6.1356, -7.8869, -7.9401, -8.4604, -4.4519, -4.2168, -1.4691, -1.9148, -2.5414, -3.553, -3.8683, -4.2027, -4.2986, -4.299, -4.374, -4.4509, -5.0319, -6.1587, -4.2637, -6.6281, -6.9571, -8.4149, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -11.0268, -2.3238, -2.4541, -2.9135, -3.2804, -3.4298, -4.2619, -5.0227, -5.1618, -5.2457, -6.8053, -8.86, -8.86, -8.86, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346, -10.5346], \"loglift\": [25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.7233, 0.7233, 0.7233, 0.7233, 0.7232, 0.7232, 0.7232, 0.7232, 0.7232, 0.7232, 0.7231, 0.7231, 0.7231, 0.7231, 0.7231, 0.7231, 0.7231, 0.7231, 0.7231, 0.7231, 0.723, 0.7229, 0.7229, 0.7229, 0.7228, 0.7212, 0.7214, 0.7147, 0.7118, 0.7212, 0.7223, 0.698, 0.7081, 0.7144, 0.7055, 0.7185, 0.6747, 0.7024, 0.6947, 0.67, 0.6828, 0.6803, 0.6852, 0.6686, 0.5308, 0.5222, 2.1912, 2.191, 2.191, 2.1908, 2.1907, 2.1907, 2.1906, 2.1906, 2.1905, 2.1905, 2.1905, 2.1904, 2.1904, 2.1903, 2.1903, 2.1902, 2.1902, 2.1902, 2.1901, 2.1901, 2.19, 2.19, 2.1899, 2.1899, 2.1899, 2.189, 2.1857, 2.1738, 2.1529, 2.1796, 2.1297, 2.1192, 2.1157, 2.1483, 2.1156, 1.924, 2.0729, 2.0593, 1.7824, 1.9423, 1.7669, 2.0019, 2.0221, 1.5721, 1.2974, 1.8915, 2.2342, 2.234, 2.2339, 2.2338, 2.2338, 2.2337, 2.2336, 2.2335, 2.2335, 2.2335, 2.2333, 2.2333, 2.2333, 2.2332, 2.2332, 2.233, 2.233, 2.2329, 2.2328, 2.2328, 2.2328, 2.2328, 2.2327, 2.2327, 2.2326, 2.231, 2.2313, 2.2262, 2.2309, 2.2214, 2.2201, 2.2015, 2.2007, 2.1892, 2.1999, 2.2036, 2.148, 2.0485, 2.048, 1.9231, 2.108, 2.0591, 1.8635, 1.3486, 1.8507, 1.5902, 1.3466, 2.3026, 2.3026, 2.3026, 2.3026, 2.3025, 2.3025, 2.3024, 2.3024, 2.3024, 2.3023, 2.3023, 2.3022, 2.3021, 2.302, 2.302, 2.302, 2.302, 2.3019, 2.3018, 2.3018, 2.3016, 2.3014, 2.3013, 2.3012, 2.3012, 2.2974, 2.2983, 2.2817, 2.2882, 2.2558, 2.2729, 2.2635, 2.2441, 2.221, 2.1645, 2.2794, 2.0183, 1.937, 1.7449, 1.3392, 2.4034, 2.4033, 2.4033, 2.4026, 2.4024, 2.4018, 2.4014, 2.401, 2.4, 2.4, 2.3997, 2.3995, 2.3985, 2.3985, 2.398, 2.3979, 2.3975, 2.3972, 2.3967, 2.3962, 2.3961, 2.396, 2.3944, 2.3931, 2.393, 2.3886, 2.375, 2.3665, 2.3442, 2.3061, 2.2562, 2.2923, 2.3265, 2.1475, 2.2804, 2.2601, 2.2922, 2.2443, 2.0555, 2.0601, 2.1306, 1.9662, 2.1569, 2.0906, 1.3715, 0.6618, 4.1872, 4.1859, 4.1852, 4.1849, 4.1843, 4.1839, 4.1837, 4.1832, 4.1831, 4.183, 4.1829, 4.1828, 4.1824, 4.1823, 4.1822, 4.1818, 4.1815, 4.1815, 4.1814, 4.1809, 4.1807, 4.1805, 4.1805, 4.1802, 4.1799, 3.9965, 3.6217, 3.3769, 3.1086, 3.2842, 3.2968, 3.1015, 3.0742, 2.1472, 2.138, 2.2714, 4.5115, 4.5114, 4.5109, 4.5104, 4.5101, 4.5097, 4.5096, 4.5094, 4.5093, 4.5089, 4.5088, 4.5087, 4.5086, 4.5086, 4.508, 4.5077, 4.5075, 4.5071, 4.5065, 4.5064, 4.5063, 4.5062, 4.5062, 4.506, 4.5057, 4.1312, 4.6178, 4.6177, 4.6171, 4.6167, 4.6164, 4.6163, 4.616, 4.6159, 4.6147, 4.6124, 4.6111, 4.6104, 4.605, 4.6048, 4.6038, 4.6029, 4.6025, 4.6009, 4.5985, 4.5981, 4.5955, 4.5886, 4.5858, 4.5819, 4.5797, 4.4716, 4.7514, 4.7511, 4.751, 4.7508, 4.7507, 4.7505, 4.7504, 4.7503, 4.7498, 4.7497, 4.7495, 4.7494, 4.7494, 4.7483, 4.7483, 4.748, 4.7479, 4.7477, 4.747, 4.7467, 4.7456, 4.7455, 4.7448, 4.7436, 4.7435, 4.8726, 4.8725, 4.8725, 4.8724, 4.8714, 4.8713, 4.8708, 4.8692, 4.8689, 4.8683, 4.868, 4.8679, 4.8669, 4.8657, 4.864, 4.8639, 4.8636, 4.8635, 4.8606, 4.8574, 4.8549, 4.8547, 4.8542, 4.8526, 4.8513, 4.8476, 4.7339, 4.4366, 4.0418, 4.9187, 4.9182, 4.9181, 4.9178, 4.9177, 4.9174, 4.9172, 4.9172, 4.917, 4.9161, 4.9156, 4.9152, 4.9143, 4.9141, 4.9141, 4.9137, 4.9127, 4.9125, 4.9119, 4.9114, 4.9109, 4.9106, 4.91, 4.9098, 4.909, 4.8958, 4.8652, 5.0219, 5.0209, 5.0197, 5.0197, 5.019, 5.0188, 5.0184, 5.0173, 5.017, 5.0162, 5.0158, 5.0157, 5.0148, 5.0143, 5.0136, 5.0135, 5.0133, 5.0104, 5.01, 5.0094, 5.0083, 5.0082, 5.0069, 5.0064, 5.0063, 5.065, 5.065, 5.0649, 5.0646, 5.0645, 5.0638, 5.0637, 5.0631, 5.0627, 5.0624, 5.062, 5.0617, 5.0614, 5.0608, 5.0608, 5.0604, 5.0603, 5.0595, 5.0586, 5.0585, 5.0581, 5.058, 5.0555, 5.0547, 5.0545, 4.856, 5.1947, 5.1942, 5.1939, 5.1929, 5.1922, 5.1918, 5.1912, 5.1908, 5.1884, 5.1872, 5.1865, 5.1862, 5.186, 5.1849, 5.183, 5.1811, 5.1811, 5.1751, 5.174, 5.167, 5.1598, 5.1534, 5.1483, 5.1304, 5.1151, 5.5375, 5.5342, 5.5341, 5.5325, 5.5325, 5.5322, 5.5317, 5.5316, 5.5307, 5.5305, 5.5255, 5.5249, 5.5242, 5.5238, 5.5213, 5.5129, 5.512, 5.5104, 5.5103, 5.5015, 5.5011, 5.4964, 5.4872, 5.4692, 5.4211, 4.877, 4.93, 5.7802, 5.7787, 5.7785, 5.7769, 5.7768, 5.7763, 5.7746, 5.7733, 5.7732, 5.7688, 5.7686, 5.765, 5.7611, 5.7604, 5.7578, 5.7571, 5.7569, 5.7565, 5.7555, 5.7269, 5.6855, 5.5826, 5.5739, 5.4961, 5.4681, 5.3803, 5.1268, 4.026, 4.7761, 4.3947, 5.7864, 5.7861, 5.7857, 5.7816, 5.7794, 5.7749, 5.7688, 5.7593, 5.7514, 5.6164, 5.5713, 5.566, 5.5646, 5.5453, 5.2589, 5.2254, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 2.494, 5.8316, 5.8311, 5.8294, 5.8267, 5.8243, 5.8231, 5.8219, 5.82, 5.8198, 5.8195, 5.8154, 5.814, 5.8102, 5.8095, 5.8082, 5.8032, 5.7849, 5.7809, 5.6923, 5.6249, 5.5954, 5.5729, 5.5403, 5.493, 5.3048, 4.8882, 5.851, 5.8506, 5.8465, 5.8456, 5.8448, 5.8412, 5.8405, 5.84, 5.8347, 5.831, 5.8125, 5.8032, 5.7967, 5.7881, 5.7837, 5.7469, 5.7364, 5.6838, 5.6742, 5.4649, 5.2816, 5.1185, 4.8278, 4.5046, 2.5195, 2.5195, 5.882, 5.8806, 5.8802, 5.8796, 5.8796, 5.8794, 5.8786, 5.8692, 5.8521, 5.8491, 5.8326, 5.8253, 5.8079, 5.8022, 5.7897, 5.7639, 5.7587, 5.6545, 5.6266, 5.1921, 4.9769, 2.6294, 2.6294, 2.6294, 2.6294, 2.6294, 2.6294, 2.6294, 2.6294, 2.6294, 2.6294, 5.9631, 5.9602, 5.9585, 5.9581, 5.9577, 5.9568, 5.9548, 5.951, 5.9404, 5.936, 5.9347, 5.9328, 5.9291, 5.9214, 5.9211, 5.917, 5.9152, 5.9096, 5.8971, 5.8864, 5.8672, 5.7407, 5.6274, 5.4162, 5.4136, 4.7437, 6.0744, 6.0742, 6.0695, 6.0647, 6.0645, 6.0622, 6.0544, 6.0537, 6.0513, 6.0425, 6.0271, 6.0084, 5.8876, 5.0835, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 2.7524, 6.1275, 6.1222, 6.1213, 6.1177, 6.1167, 6.1139, 6.1027, 6.1009, 6.0994, 6.0922, 6.0921, 6.0909, 6.0879, 6.0879, 6.0596, 6.0478, 6.0447, 6.0003, 5.9706, 5.9601, 5.4068, 5.3791, 5.0716, 4.0606, 4.0061, 6.4193, 6.418, 6.4148, 6.4031, 6.3963, 6.3865, 6.383, 6.383, 6.3801, 6.3768, 6.3429, 6.1962, 6.1524, 6.0821, 5.9759, 5.194, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 2.8988, 7.4382, 7.4361, 7.426, 7.414, 7.4078, 7.3512, 7.246, 7.2184, 7.2002, 6.59, 4.9936, 4.9936, 4.9936, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391, 3.391]}, \"token.table\": {\"Topic\": [3, 5, 2, 1, 2, 4, 6, 1, 5, 6, 5, 19, 21, 12, 20, 21, 21, 5, 18, 1, 2, 6, 24, 24, 18, 1, 2, 4, 6, 16, 1, 2, 4, 6, 22, 2, 6, 21, 19, 5, 6, 11, 12, 1, 3, 1, 5, 7, 8, 21, 3, 3, 1, 4, 18, 10, 16, 13, 6, 9, 1, 2, 4, 25, 5, 13, 1, 3, 4, 6, 5, 12, 6, 8, 9, 20, 12, 16, 1, 3, 6, 1, 11, 3, 5, 8, 16, 16, 23, 21, 6, 7, 9, 1, 6, 14, 17, 3, 18, 12, 24, 19, 1, 3, 15, 13, 16, 12, 5, 6, 11, 21, 13, 1, 5, 1, 4, 1, 3, 18, 10, 5, 10, 19, 10, 9, 6, 21, 12, 6, 5, 17, 20, 12, 4, 6, 9, 11, 16, 6, 1, 2, 12, 3, 4, 10, 18, 13, 23, 22, 2, 7, 1, 3, 2, 1, 4, 1, 3, 5, 20, 8, 21, 16, 3, 11, 20, 17, 12, 22, 12, 6, 10, 9, 23, 1, 3, 5, 4, 5, 2, 4, 1, 16, 4, 6, 3, 5, 13, 1, 4, 14, 13, 2, 1, 6, 4, 18, 5, 19, 15, 15, 11, 16, 6, 20, 4, 10, 21, 18, 8, 2, 6, 20, 17, 16, 16, 21, 6, 12, 17, 10, 11, 21, 20, 14, 1, 5, 23, 5, 1, 3, 5, 7, 1, 4, 1, 4, 24, 3, 1, 6, 13, 8, 14, 9, 18, 10, 4, 6, 23, 10, 20, 6, 10, 1, 17, 12, 9, 20, 12, 19, 6, 17, 14, 16, 15, 21, 9, 6, 8, 24, 21, 21, 4, 1, 3, 4, 21, 15, 3, 22, 3, 3, 1, 3, 4, 1, 16, 1, 5, 9, 1, 2, 3, 3, 3, 19, 7, 14, 1, 2, 4, 16, 22, 9, 17, 2, 8, 13, 1, 20, 2, 9, 10, 5, 7, 11, 12, 5, 4, 1, 12, 14, 14, 1, 2, 3, 13, 23, 1, 1, 19, 4, 9, 16, 7, 1, 2, 10, 25, 11, 4, 12, 17, 7, 1, 5, 19, 9, 13, 1, 5, 1, 3, 4, 5, 6, 9, 7, 22, 19, 1, 3, 18, 1, 6, 22, 16, 1, 2, 5, 1, 2, 2, 16, 24, 1, 5, 6, 16, 1, 20, 24, 6, 4, 5, 6, 1, 5, 25, 2, 19, 18, 15, 8, 1, 2, 2, 15, 18, 20, 3, 17, 10, 12, 4, 9, 16, 23, 20, 1, 4, 21, 1, 24, 23, 9, 23, 11, 13, 15, 5, 22, 3, 3, 4, 23, 8, 13, 6, 11, 7, 15, 10, 5, 1, 4, 10, 14, 17, 5, 17, 20, 23, 11, 1, 7, 15, 1, 2, 16, 2, 23, 13, 17, 16, 19, 8, 6, 2, 6, 15, 8, 1, 8, 11, 8, 1, 5, 1, 2, 3, 4, 14, 1, 8, 19, 13, 12, 14, 1, 4, 5, 6, 15, 2, 1, 2, 1, 4, 5, 5, 6, 18, 25, 1, 1, 3, 1, 2, 1, 2, 3, 4, 7, 4, 6, 7, 14, 1, 2, 3, 23, 16, 6, 13, 10, 12, 2, 20, 15, 1, 4, 18, 1, 2, 6, 2, 4, 14, 19, 15, 6, 1, 6, 11, 4, 3, 5, 24, 11, 2, 6, 3, 6, 1, 2, 4, 6, 19, 1, 4, 18, 1, 2, 19, 13, 17, 10, 5, 12, 2, 2, 3, 2, 24, 1, 5, 4, 12, 14, 1, 2, 3, 4, 13, 16, 9, 5, 8, 12, 6, 7, 19, 5, 4, 11, 8, 12, 4, 11, 7, 11, 25, 11, 5, 22, 1, 5, 8, 1, 2, 4, 25, 3, 25, 18, 9, 18, 6, 4, 8, 3, 1, 11, 3, 1, 4, 13, 16, 13, 9, 24, 13, 9, 19, 9, 15, 1, 4, 15, 9, 6, 13, 1, 3, 23, 15, 23, 10, 2, 3, 4, 24, 19, 6, 1, 5, 1, 2, 18, 7, 1, 4, 5, 6, 11, 22, 4, 8, 7, 3, 1, 2, 3, 4, 6, 21, 7, 9, 15, 18, 15, 5, 7, 3, 15, 1, 3, 8, 8, 14, 10, 8, 14, 10, 11, 1, 22, 10, 10, 5, 2, 11, 20, 15, 23, 2, 1, 3, 6, 2, 22, 1, 3, 14, 14, 12, 21, 3, 20, 14, 17, 7, 21, 18, 1, 11, 19, 9, 8, 5, 6, 3, 5, 2, 6, 7, 1, 2, 9, 14, 25, 21, 4, 3, 6, 11, 4, 7, 13, 2, 13, 19, 4, 5, 23, 1, 4, 1, 2, 13, 1, 3, 5, 6, 7, 22, 18, 8, 10, 3, 5, 14, 4, 7, 15, 18, 11, 1, 3, 4, 6, 10, 15, 23, 6, 9, 8, 1, 23, 1, 20, 18, 1, 7, 14, 24, 1, 5, 6, 4, 11, 21, 21, 15, 13, 5, 7, 1, 15, 16, 14, 20, 1, 2, 3, 17, 7, 18, 14, 19, 3, 5, 11, 16, 4, 3, 21, 23, 2, 4, 25, 6, 8, 10, 23, 16, 24, 3, 11, 4, 16, 3, 4, 5, 3, 22, 6, 9, 24, 1, 2, 4, 2, 7, 1, 2, 3, 16, 1, 3, 5, 10, 25, 7, 2, 9, 21, 15, 2, 13, 19, 1, 2, 4, 1, 3, 5, 1, 6, 1, 5, 21, 23, 15, 18, 18, 20, 2, 23, 14, 23, 23, 13, 6, 10, 6, 10, 10, 10], \"Freq\": [0.07259292120849857, 0.9259630394150707, 0.9985147430605898, 0.29037037180089736, 0.6538065607563881, 0.011735960268354391, 0.043536626801959834, 0.004667636489019755, 0.9714518442772366, 0.023629909725662514, 0.9964214652500295, 0.43203452036685297, 0.4205635932441726, 0.9901439949616422, 0.9989161609584951, 0.7993991350374712, 0.9908147601687134, 0.9934264749465609, 0.6831845281855294, 0.19796890596337352, 0.7651413613387821, 0.03688651410257729, 0.9643819057492832, 0.9541659423962293, 0.9612687722442463, 0.0021019719237473657, 0.9295970832772724, 0.005254929809368414, 0.06253366473148413, 0.9923914298279972, 0.0020738327250026272, 0.9399646826074408, 0.0015553745437519703, 0.05599348357507093, 0.8535538543282092, 0.887898068722325, 0.1116737777155914, 0.9947576904045752, 0.6368970643954991, 0.7810643310456258, 0.21785885266768487, 0.9836072674205811, 0.9955673953616373, 0.008022812217855994, 0.9912630206950962, 0.23876874915848376, 0.760959104460934, 0.9976469280774621, 0.9994863072389824, 0.9610397113561323, 0.9987416448390072, 0.9980625166714707, 0.038125340135369265, 0.9611598907811514, 0.9978378476886746, 0.7230226350216624, 0.25655641887865444, 0.9946067523546631, 0.9950998623239676, 0.9952280957154356, 0.02273544131109824, 0.03379592627325414, 0.9425991073303973, 0.9610308481051011, 0.9967190408293364, 0.9946904337026105, 0.9716298564233479, 0.0016500264422521676, 0.003300052884504335, 0.023100370191530346, 0.9995395489989773, 0.9865742867362092, 0.10605203252513924, 0.8749292683323988, 0.9944388801187986, 0.9832903041358441, 0.996594027565298, 0.9835460015054736, 0.11789966615254566, 0.8803909650082615, 0.9960517704447311, 0.999456874399144, 0.9882953878875091, 0.14569025941740743, 0.8531411587505842, 0.4476635168640912, 0.5183472300531582, 0.9947782852913146, 0.9576072133706356, 0.4220276655163743, 0.9973593297844491, 0.9970929479973016, 0.9956714114495766, 0.5537584647613516, 0.4435444013865195, 0.9968612815263704, 0.9989928874103823, 0.9980950170300157, 0.9847140700639639, 0.9969960326183885, 0.9516196726913618, 0.9853979943330513, 0.03289071340642397, 0.9662321053165862, 0.8869367015050602, 0.9929263806979854, 0.9633473210470533, 0.9835563079515219, 0.8837162975752223, 0.11567412557288761, 0.9948762806462067, 0.9750037189459317, 0.9947135954269944, 0.0003252775532228367, 0.9992526435005542, 0.9909402629379709, 0.008771735160126888, 0.9989566536889735, 0.0007740849699255897, 0.9799414183195223, 0.9961214926146679, 0.9980329527774338, 0.9956568328159692, 0.8101854853311045, 0.996526948297844, 0.9957892162159007, 0.9903155721907608, 0.99095417675648, 0.9921280566511659, 0.9979735362803025, 0.9957662100572154, 0.7910645933723259, 0.9930986569163837, 0.9896645003790198, 0.9539057157442292, 0.04583986277031765, 0.048711900693781354, 0.9476678862244736, 0.9964796659403984, 0.9971185178850891, 0.005569276115937708, 0.9935588590832871, 0.9774791176369102, 0.029192462095715577, 0.9703320553119373, 0.9994183309341798, 0.9989216793899217, 0.9942517113403504, 0.9985363725372689, 0.9107958992128296, 0.9983943062642872, 0.9951195673167631, 0.014032243089497017, 0.9852639254982548, 0.9981102853474406, 0.30607364799924036, 0.6935917860240297, 0.13426893916103746, 0.8298426164689685, 0.03533393135816776, 0.9967161816743919, 0.9892550027350261, 0.9920336871964626, 0.9932967430433592, 0.9992237780877722, 0.9946442689714168, 0.9467152692231309, 0.97072864891264, 0.9948732962832842, 0.9929880135343175, 0.9958594911588772, 0.9955630212450556, 0.9766119011028098, 0.9883879617542389, 0.9933747202295365, 0.011850939471736611, 0.9671397125412877, 0.02061032951606367, 0.2679447052992168, 0.7303654063801234, 0.014318935136553704, 0.9854030816701049, 0.9994227732632464, 0.9958419379314041, 0.659719550872526, 0.3364569709449883, 0.9988708402178156, 0.9934723787275656, 0.9963626688095036, 0.4275510598467394, 0.5722786540101886, 0.9996196830941714, 0.9978675600222968, 0.9991870171859382, 0.9626802216045207, 0.036827364182186365, 0.9995655826476967, 0.9964130422114738, 0.996921331198742, 0.9947551623358287, 0.9701931971425355, 0.947887783880021, 0.3061890642245619, 0.6664114927240464, 0.9972773187161369, 0.9451998161086876, 0.9994677220851111, 0.9725491540018925, 0.9686494358489053, 0.9355254825835118, 0.9773460617561802, 0.5921324924604799, 0.40395479358532743, 0.9513344567392281, 0.98773529876098, 0.9844844669361367, 0.9816726738742964, 0.9718346979471053, 0.9950966582110026, 0.9794818110726864, 0.8561905843477303, 0.9941673529850709, 0.9921032514319, 0.8554332693584119, 0.9012369375738152, 0.9447329271336035, 0.05730409782414144, 0.9422616994492349, 0.5224977012335293, 0.9976454200093232, 0.2519965257096151, 0.7321870305525777, 0.015005953276342581, 0.9978127101724759, 0.02047124347749932, 0.9789803547461898, 0.1289371075345395, 0.8705307897236424, 0.8525757550600536, 0.9976847043977891, 0.8519164424678439, 0.14737393815012925, 0.9883627356579132, 0.9917330873879525, 0.9909221124512397, 0.6069134042945449, 0.3897022911786025, 0.9987703607822774, 0.9994937964190528, 0.9921398940692141, 0.6463827143100047, 0.9856982379283374, 0.997657445805197, 0.5613278464134512, 0.43658832498823985, 0.9991192599625333, 0.9991611300307301, 0.9906817640023584, 0.9973478637509269, 0.996270555679568, 0.9978179082309486, 0.9910805816264081, 0.9941889725356982, 0.782683173682792, 0.9791410054634009, 0.9861605220368632, 0.9376072302736278, 0.9724300538933528, 0.9965325876640782, 0.9945112118923798, 0.999521853042496, 0.9770277142376068, 0.924905027681023, 0.9672602407506319, 0.9988788904540623, 0.0010905303072077986, 0.969481443107733, 0.028353787987402766, 0.9843591646827234, 0.983313691553327, 0.9981947561337398, 0.9709337493045127, 0.9980515873748104, 0.9990653825679631, 0.0025238778578237035, 0.9969317538403628, 0.9978327398794603, 0.8241501088297613, 0.17158073938094007, 0.9465779281480722, 0.053303890663131355, 0.996835953793446, 0.05011974375050099, 0.8757286135314809, 0.07381271352346509, 0.9981342975181686, 0.9981599727033919, 0.9259032269570636, 0.9925096954314448, 0.9957578155392863, 0.9846343966381664, 0.004497690465184389, 0.010794457116442533, 0.9686953840340952, 0.9836315364526291, 0.9932894715602545, 0.991130606433257, 0.9991419871132161, 0.9974067145185961, 0.9978348474844698, 0.9996251884454466, 0.8951901041263172, 0.9983692788271018, 0.997201245498822, 0.9898529320439261, 0.9988448305841368, 0.9964617030464694, 0.9984859526681974, 0.9923670217229692, 0.9980951922346886, 0.9995127417322892, 0.9998390123107308, 0.9834217675480171, 0.9839189511159334, 0.9891175640299786, 0.02607540189526249, 0.1437872161653046, 0.8299427917520691, 0.9960186193745215, 0.9514958877628423, 0.999328904165914, 0.9996763533690329, 0.9666525515296359, 0.9996544386316244, 0.9944039488215848, 0.7479987028313801, 0.9957612674058245, 0.00196562822007186, 0.997556321686469, 0.9829920993455272, 0.9823838825013022, 0.9939575107567865, 0.9988435430923325, 0.9992198287950288, 0.9987143902304625, 0.9949481628094552, 0.03653282923921357, 0.9632224578759317, 0.9205792364750581, 0.9916669554990888, 0.987539785485416, 0.010677524877173694, 0.9892687829632533, 0.24489226581098142, 0.011297607920723702, 0.0009968477577109148, 0.7057682124593277, 0.03655108444940021, 0.9917462387486143, 0.9961826017031347, 0.968493286438787, 0.9963426762088591, 0.4744911164798658, 0.5251116340477693, 0.9677747385269437, 0.9912389004125564, 0.008587318112104928, 0.9872468026809011, 0.9778195869922576, 0.9949998163279123, 0.004592811549156206, 0.9969389471742724, 0.9977093575082858, 0.0020563618011225676, 0.9974913245485992, 0.7425129281580559, 0.9956620097290776, 0.13303467137317354, 0.8626222864648958, 0.004206154921464384, 0.9920863174277869, 0.9997144059300355, 0.9982010197215077, 0.9717980895092954, 0.9958991068007066, 0.06349020631596668, 0.8944752823520167, 0.04174217984309411, 0.0003438608626347055, 0.9994315972477715, 0.9834231505208841, 0.9982463537616072, 0.5165412793410074, 0.9817103196677857, 0.9522091182585175, 0.9956793798929282, 0.037913424682061665, 0.9616885991469103, 0.9989066923413908, 0.9643536418713741, 0.8414094041560484, 0.9374594876729428, 0.9988773598161185, 0.9797534284670253, 0.9924210568002143, 0.9935030870811191, 0.998927032098239, 0.9872673451336952, 0.9869790504446146, 0.924854447471276, 0.678154663547314, 0.24720637198843334, 0.752032571195582, 0.993511198762057, 0.9998219761697167, 0.7968655361684523, 0.9957037350850854, 0.9958130232700709, 0.9870344878473115, 0.18208807113891093, 0.8050209460878168, 0.9815260656661003, 0.9924558843787975, 0.9976547912515963, 0.9987357721072417, 0.998763609814308, 0.9992955341196939, 0.958523958778078, 0.9886480485913411, 0.9864598506057055, 0.9947038415812658, 0.9959347045892151, 0.9974809256241766, 0.9808199594424912, 0.9847675912921142, 0.9909992737984498, 0.005116920025962859, 0.9945431832280538, 0.9953325636277535, 0.9970944044734673, 0.42788428973511883, 0.9946337406058088, 0.9538685913057159, 0.9945378953134553, 0.9788586703664217, 0.9967473277947356, 0.9997361151696529, 0.4802252128118363, 0.5174257574662743, 0.0174007996653744, 0.981902266831841, 0.9810749458869549, 0.9976378702599198, 0.9606852253274197, 0.994651759209101, 0.774174695324403, 0.996068180184355, 0.9892603386279065, 0.997871667176201, 0.991344591104875, 0.8442695120358769, 0.15538036064495545, 0.9923365307324815, 0.953258304237046, 0.0175135070487093, 0.9807563947277209, 0.9951398535324303, 0.997667945501357, 0.9747914931598091, 0.025117673101106835, 0.06835592112476854, 0.07720198150562094, 0.6899927097064872, 0.1640542106994445, 0.9912555530074965, 0.999790751167402, 0.9911364634062999, 0.8691645179558937, 0.9942338979039278, 0.9964620688332302, 0.986272487679287, 0.9576469733786471, 0.003080665815096751, 0.01694366198303213, 0.02200475582211965, 0.9952145047388701, 0.9993035797375895, 0.010417367850482941, 0.9881617503886676, 0.9979731844871564, 0.0006394531297440132, 0.0010657552162400218, 0.9847219103630028, 0.01502561075078443, 0.4079672108259977, 0.8172422460175729, 0.9997777327894785, 0.08252035497442574, 0.9169661844758188, 0.99985961760897, 0.9990950777828015, 0.9975950233352583, 0.0006295961018209266, 0.0006295961018209266, 0.00094439415273139, 0.9972795938661707, 0.6707248785637554, 0.3278623208314843, 0.9952894732950694, 0.9817583345538454, 0.46062723484101487, 0.5382129892505757, 0.0006109114520437863, 0.8691593877244636, 0.9968135352462489, 0.9943733937702764, 0.994493702389862, 0.9933431014949203, 0.9949601237466373, 0.9992302097672615, 0.5942278256375781, 0.9913678814780912, 0.07820545201025181, 0.9212062898862765, 0.756946507579276, 0.5308641828053152, 0.40893896395480694, 0.06007264432415553, 0.7408006019536669, 0.25811542986310154, 0.9474645934624039, 0.6380782022322469, 0.9666138041099726, 0.9913084912522635, 0.9820677277256162, 0.017620608208858163, 0.989757051889127, 0.9986139063600872, 0.13321002748372024, 0.8658651786441816, 0.9976522966556551, 0.9951907129873525, 0.9979101388383802, 0.9961820977500976, 0.587611894084194, 0.4113283258589358, 0.9601233817284485, 0.003144263706245211, 0.014598367207567053, 0.02200984594371648, 0.9664524337181827, 0.9792798805585627, 0.0205350083886191, 0.9753825353391035, 0.9995701514321471, 0.9984814217345847, 0.9795814355942373, 0.9925354816436196, 0.7954222675826373, 0.9993070048242413, 0.989283336803327, 0.9900585366794794, 0.9992820386636753, 0.9577930484161215, 0.04134358482371747, 0.9976160711756259, 0.9853120961259065, 0.9523449807092325, 0.04747497387715469, 0.9992333977520463, 0.9969920932369681, 0.984777163361578, 0.10463109088963365, 0.05284398529779478, 0.8391624865289811, 0.0021137594119117913, 0.6026047639025741, 0.35709911934967353, 0.993754330469352, 0.9893221649006414, 0.980486812652153, 0.988996124618805, 0.5675059957351481, 0.428585257195815, 0.985474744240859, 0.9927330753804876, 0.9993241396068324, 0.996881822241126, 0.9991964205276657, 0.9907367474981859, 0.9989700613208028, 0.9961351715352299, 0.998659454132973, 0.9899448742656561, 0.9464488140394882, 0.9858021614081622, 0.9942626721510597, 0.9512321914451227, 0.22605962365061016, 0.7738388453251399, 0.9969627491127775, 0.29204139106880556, 0.6639521396115433, 0.04389766212307725, 0.7310252187127437, 0.9984275787396254, 0.9486990542564299, 0.9926507883068096, 0.9967316304426883, 0.5731703253083853, 0.9938055678560158, 0.9991842540655334, 0.9980334272685815, 0.9990775326856328, 0.9996558405425287, 0.9948872895894179, 0.9993442600920412, 0.9991848249276186, 0.9983490981208409, 0.9902671361361339, 0.9980040940702893, 0.9914937761567064, 0.20060979496755812, 0.7737806377320099, 0.9916034198894138, 0.9977122053604871, 0.9912894824059166, 0.45135905204648286, 0.5480788489135864, 0.002720348099828891, 0.995647404537374, 0.9966875678044999, 0.9983615760333951, 0.9985100641617178, 0.990146038967634, 0.012741897908527713, 0.9870070149144159, 0.9621558231473063, 0.9990122054499445, 0.984388051112305, 0.9885764603377978, 0.001645538302772404, 0.9955506731773044, 0.9982226964506807, 0.7101521116698707, 0.9991343080611446, 0.9977460078273629, 0.8247548575803262, 0.17520147444819026, 0.2205975351829324, 0.779080000287932, 0.9776298608221556, 0.9950396871999624, 0.07370674398098544, 0.0031143694639853, 0.9069043879125194, 0.01619472121272356, 0.9935288104803642, 0.9983158738953835, 0.9997444829842953, 0.957679849189688, 0.9941358690637292, 0.9977260886796503, 0.1672657496720716, 0.008239692102072493, 0.6814225368413952, 0.014007476573523239, 0.1285391967923309, 0.9813909177663179, 0.9984498219435498, 0.9965403483755304, 0.9865249100524677, 0.9870268082993465, 0.990270526638471, 0.9944569340902979, 0.9948244354326831, 0.9986611631368073, 0.948801409077909, 0.9996459427535377, 0.9986564597395118, 0.9812828647062005, 0.9584880723673985, 0.9923533953652376, 0.9877912162921729, 0.9882346963554748, 0.9960127968029827, 0.9982104181816911, 0.9962753140102062, 0.999800959630589, 0.9957524708257791, 0.9895345338451601, 0.9969650648911026, 0.9954768379022143, 0.9995917580340078, 0.9971347464190279, 0.816764963957307, 0.9853554709974759, 0.9514958877628423, 0.998467214370192, 0.8572514234812258, 0.0116181219058443, 0.13028893851553963, 0.998142456631095, 0.9493531413306906, 0.04392780522286993, 0.9556369702258307, 0.9969018449215521, 0.9981209582295746, 0.9855544653434399, 0.8992353793373473, 0.9993057104696711, 0.9655166388275278, 0.9736375172796143, 0.9953601463982001, 0.9954176286115562, 0.9650753255634648, 0.9868428200087609, 0.9994790541664013, 0.989261315346291, 0.7700210939205485, 0.9976040508654562, 0.9899258525705413, 0.6589151746278553, 0.3395020259515474, 0.10301699909288717, 0.8934383375874032, 0.9989848270479555, 0.9935504813548446, 0.9965500514876221, 0.07276105387699533, 0.9268766067739972, 0.9966172125208589, 0.9857702288869298, 0.5638419028483107, 0.9099441371481447, 0.9985114007808804, 0.9987348102737792, 0.0238035517207435, 0.9759456205504835, 0.9990138084764114, 0.6838713725775315, 0.31362254029033293, 0.9978093025228391, 0.9969376494948192, 0.9513207483195971, 0.9765352762289851, 0.02248279414741407, 0.9796631754442883, 0.8176033914302736, 0.18218215986871372, 0.17166263983791857, 0.8265605721675501, 0.9980242434399118, 0.023845827813415814, 0.23109412542707386, 0.7090627035106879, 0.03541806778169114, 0.5784815550134801, 0.3856543700089867, 0.9729032970167178, 0.9976363653265853, 0.9811166065947176, 0.3539281718473448, 0.6454432037130073, 0.9979236089734169, 0.9995106878512541, 0.9934272314130471, 0.9969591190496869, 0.988306003116761, 0.9950712081746328, 0.5369530373726306, 0.4122597072112103, 0.05036473270363554, 0.35218984567070444, 0.6461227243883599, 0.9884105342493597, 0.7686110028528188, 0.9917077564437242, 0.9883878570090783, 0.9898366360699132, 0.999779752144324, 0.9270512127441486, 0.9996487165504656, 0.9370660636238075, 0.9137654334302808, 0.9997009252725169, 0.9964094316392165, 0.966681905024791, 0.9721835481173918, 0.9883942618890299, 0.0020703181927197523, 0.009377323578789466, 0.998535963789381, 0.9954909330493279, 0.9587530370490418, 0.9985142740685807, 0.9945196148096426, 0.9943113599702889, 0.9893797582020487, 0.9919692357784556, 0.9995414173169477, 0.9938788337222841, 0.8044100401024805, 0.9962368007687863, 0.49757231589476875, 0.5564294761358535, 0.031679180459525176, 0.4113183914502866, 0.4086115126410932, 0.9937387968859017, 0.9683446219711843, 0.9913536602963945, 0.8935295145642508, 0.9958566435639731, 0.0026842497131104397, 0.9939998837720395, 0.9000732921858664, 0.9993716876243244, 0.6671715808212872, 0.3113467377166007, 0.9732849771111491, 0.926241098525836, 0.07209660442579481, 0.9821819813416752, 0.994890587006978, 0.9862847189278281, 0.8713441369062329, 0.1281388436626813, 0.9690897680207898, 0.8970314895872779, 0.9981183570496822, 0.9842392912170916, 0.9989594444810346, 0.9846456030215176, 0.9660788534217526, 0.007156139654975946, 0.026579947289910654, 0.9993695247365241, 0.9813558655846457, 0.993223206162807, 0.9986098421129213, 0.991862853495272, 0.9995726164915097, 0.00021240387090767313, 0.999560026194471, 0.99920089132086, 0.9951511749628519, 0.999455261047882, 0.00044459753605332827, 0.9985791916812047, 0.9164560142994869, 0.9623766677372247, 0.027184346211162367, 0.009908313105003105, 0.9966635821887184, 0.8727605134862304, 0.991163464398462, 0.9979279484993421, 0.9959754845620614, 0.9756304693316626, 0.9927132484834947, 0.9990217384886632, 0.9970513493107932, 0.9988165140070385, 0.61799182669681, 0.000273447710927792, 0.38145955674426985, 0.3849521851920965, 0.2583400046804103, 0.35596353780221546, 0.9613432754005179, 0.03825785081982322, 0.9479458289214473, 0.05189276344406559, 0.9956529828743157, 0.9673454353574206, 0.9858821410286271, 0.7472963038001539, 0.9847590417371475, 0.9635258204720201, 0.8760268408783979, 0.11829559530109753, 0.92656212203901, 0.5093331669476274, 0.835640057476044, 0.9884434895785673, 0.8232364644026258, 0.1691581776169779, 0.022449657946970996, 0.9753129174739622, 0.9945347130047077, 0.9892275332176984], \"Term\": [\"&\", \"&\", \"0\", \"1\", \"1\", \"1\", \"1\", \"10\", \"10\", \"10\", \"11\", \"123\", \"1300\", \"14\", \"15\", \"160\", \"200\", \"2021\", \"220\", \"3\", \"3\", \"3\", \"3070\", \"3080\", \"34\", \"4\", \"4\", \"4\", \"4\", \"45\", \"5\", \"5\", \"5\", \"5\", \"50fp\", \"6\", \"6\", \"70\", \"74\", \"9\", \"9\", \"95\", \">\", \"abil\", \"abil\", \"absolut\", \"absolut\", \"accept\", \"access\", \"accessori\", \"act\", \"action\", \"ad\", \"ad\", \"addict\", \"admir\", \"admir\", \"ador\", \"adult\", \"advanc\", \"ai\", \"ai\", \"ai\", \"aint\", \"aliv\", \"alright\", \"also\", \"also\", \"also\", \"also\", \"amaz\", \"ambiti\", \"amus\", \"amus\", \"anti\", \"app\", \"appar\", \"applic\", \"area\", \"area\", \"ark\", \"around\", \"arriv\", \"art\", \"art\", \"artist\", \"artist\", \"ass\", \"aswel\", \"asynchron\", \"audienc\", \"autom\", \"automat\", \"averag\", \"averag\", \"aw\", \"awesom\", \"b\", \"babi\", \"ball\", \"banger\", \"basi\", \"battl\", \"battl\", \"bb\", \"bear\", \"bearabl\", \"beaten\", \"beauti\", \"beauti\", \"becam\", \"bend\", \"benefit\", \"best\", \"best\", \"better\", \"better\", \"bit\", \"bit\", \"bite\", \"black\", \"blast\", \"block\", \"bloodborn\", \"board\", \"bodi\", \"boi\", \"bot\", \"boy\", \"brain\", \"brand\", \"brrrrrr\", \"buck\", \"buddi\", \"bug\", \"bug\", \"busi\", \"busi\", \"c\", \"ca\", \"call\", \"call\", \"calm\", \"campaign\", \"campaign\", \"can\", \"cant\", \"capabl\", \"car\", \"cathart\", \"caus\", \"center\", \"certain\", \"certain\", \"chanc\", \"chang\", \"chang\", \"charact\", \"charact\", \"charact\", \"cheap\", \"cheaper\", \"children\", \"chines\", \"choic\", \"chore\", \"christma\", \"clan\", \"closest\", \"club\", \"co\", \"coffe\", \"colleg\", \"colour\", \"com\", \"combat\", \"combat\", \"combat\", \"combin\", \"combin\", \"commun\", \"commun\", \"complet\", \"complic\", \"comput\", \"comput\", \"con\", \"consol\", \"construct\", \"content\", \"content\", \"cool\", \"copi\", \"cost\", \"could\", \"could\", \"crash\", \"crazi\", \"creativ\", \"cri\", \"crossbow\", \"crumb\", \"cultur\", \"cultur\", \"cup\", \"cure\", \"current\", \"d\", \"dad\", \"daddi\", \"dang\", \"dark\", \"dark\", \"darn\", \"data\", \"daughter\", \"daunt\", \"dc\", \"deaf\", \"dear\", \"decay\", \"dedic\", \"defeat\", \"defi\", \"defiantli\", \"deficit\", \"definit\", \"definit\", \"del\", \"deserv\", \"design\", \"design\", \"design\", \"destroy\", \"dev\", \"dev\", \"develop\", \"develop\", \"diagnos\", \"dialogu\", \"difficult\", \"difficult\", \"dimens\", \"dirti\", \"dive\", \"divers\", \"divers\", \"divis\", \"dlc\", \"do\", \"documentari\", \"doesn\", \"dollar\", \"don\", \"don\", \"done\", \"dont\", \"door\", \"doubl\", \"download\", \"dragon\", \"dread\", \"dri\", \"drifter\", \"drove\", \"drug\", \"drunkenli\", \"duck\", \"e\", \"eargasm\", \"earli\", \"east\", \"easter\", \"eastern\", \"edit\", \"effect\", \"effect\", \"effect\", \"egg\", \"elden\", \"element\", \"email\", \"encount\", \"enemi\", \"engag\", \"engag\", \"engin\", \"english\", \"english\", \"enjoy\", \"enjoy\", \"enter\", \"entir\", \"entir\", \"entir\", \"environ\", \"equip\", \"eras\", \"escap\", \"euro\", \"even\", \"even\", \"even\", \"everybodi\", \"everytim\", \"everywher\", \"excus\", \"exist\", \"expans\", \"expens\", \"experi\", \"export\", \"faction\", \"fail\", \"famili\", \"fantast\", \"farm\", \"faster\", \"favor\", \"favorit\", \"featur\", \"feel\", \"fellow\", \"fetch\", \"fi\", \"fight\", \"fight\", \"fight\", \"fighter\", \"filedetail\", \"finish\", \"first\", \"fist\", \"fix\", \"flat\", \"fold\", \"food\", \"forc\", \"forc\", \"forgotten\", \"format\", \"formula\", \"fp\", \"friend\", \"fuck\", \"fuel\", \"fun\", \"fun\", \"fusion\", \"g\", \"ga\", \"game\", \"game\", \"gameplay\", \"gameplay\", \"gameplay\", \"gameplay\", \"gameplay\", \"gap\", \"gather\", \"gay\", \"gem\", \"gener\", \"gener\", \"geniu\", \"get\", \"get\", \"girl\", \"girlfriend\", \"give\", \"give\", \"glad\", \"go\", \"go\", \"god\", \"goliath\", \"gon\", \"good\", \"good\", \"good\", \"googl\", \"got\", \"grab\", \"grade\", \"grandma\", \"graphic\", \"graphic\", \"graphic\", \"great\", \"great\", \"greatest\", \"group\", \"gunsling\", \"h\", \"hail\", \"hang\", \"happen\", \"happen\", \"happi\", \"hassl\", \"heartfelt\", \"heartwarm\", \"heavi\", \"heck\", \"hero\", \"hesit\", \"high\", \"hilari\", \"hill\", \"hog\", \"hollywood\", \"hope\", \"hope\", \"hot\", \"hour\", \"hq\", \"http\", \"hurt\", \"id\", \"ident\", \"ident\", \"idiot\", \"ill\", \"im\", \"impact\", \"import\", \"improv\", \"in\", \"incorpor\", \"inde\", \"infin\", \"influenc\", \"instantli\", \"investig\", \"invit\", \"irl\", \"issu\", \"issu\", \"it\", \"itch\", \"itu\", \"ive\", \"joe\", \"joke\", \"juic\", \"justifi\", \"keep\", \"kick\", \"kick\", \"kill\", \"kill\", \"killer\", \"kinda\", \"l\", \"lake\", \"lama\", \"languag\", \"laugh\", \"launch\", \"leaderboard\", \"left\", \"left\", \"legend\", \"legitim\", \"licens\", \"licens\", \"lie\", \"light\", \"like\", \"like\", \"limit\", \"limit\", \"limit\", \"limit\", \"link\", \"littl\", \"local\", \"lockdown\", \"logist\", \"lol\", \"lone\", \"look\", \"look\", \"look\", \"look\", \"lord\", \"lose\", \"lost\", \"lost\", \"lot\", \"lot\", \"lot\", \"love\", \"love\", \"lowlif\", \"mach\", \"made\", \"main\", \"main\", \"make\", \"man\", \"mani\", \"mani\", \"mani\", \"mani\", \"market\", \"master\", \"master\", \"materi\", \"mc\", \"mean\", \"mean\", \"mean\", \"medit\", \"meet\", \"mehh\", \"men\", \"mental\", \"mile\", \"minut\", \"mixtap\", \"mmo\", \"mode\", \"mode\", \"mommi\", \"money\", \"money\", \"money\", \"month\", \"month\", \"monument\", \"mop\", \"moron\", \"ms\", \"much\", \"much\", \"multi\", \"multiplay\", \"music\", \"music\", \"na\", \"nail\", \"name\", \"nasa\", \"necessari\", \"necessari\", \"need\", \"need\", \"need\", \"need\", \"netflix\", \"new\", \"new\", \"newli\", \"nice\", \"non\", \"nonetheless\", \"nonsens\", \"noon\", \"not\", \"notch\", \"oddli\", \"offer\", \"often\", \"often\", \"ok\", \"omg\", \"one\", \"one\", \"onlin\", \"op\", \"orang\", \"order\", \"order\", \"order\", \"order\", \"outpost\", \"outpost\", \"outright\", \"outstand\", \"overpr\", \"own\", \"paint\", \"paint\", \"pandem\", \"passion\", \"patch\", \"path\", \"pay\", \"peac\", \"perform\", \"period\", \"physic\", \"piss\", \"pl\", \"plagu\", \"platform\", \"platinum\", \"play\", \"play\", \"playabl\", \"player\", \"player\", \"player\", \"pleb\", \"plenti\", \"pod\", \"polici\", \"polit\", \"pong\", \"potato\", \"potenti\", \"prefer\", \"present\", \"pretti\", \"primari\", \"pro\", \"probabl\", \"product\", \"profit\", \"promis\", \"protect\", \"proud\", \"proud\", \"publish\", \"pull\", \"pump\", \"punch\", \"punch\", \"purchas\", \"purchas\", \"pure\", \"push\", \"puzzl\", \"pve\", \"quest\", \"quest\", \"quot\", \"race\", \"radar\", \"rage\", \"rang\", \"rang\", \"rate\", \"rdr2\", \"realist\", \"realiti\", \"realli\", \"realli\", \"reason\", \"reason\", \"rebel\", \"receiv\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"record\", \"relax\", \"releas\", \"reliev\", \"repair\", \"repetit\", \"requir\", \"requir\", \"requir\", \"requir\", \"requir\", \"resist\", \"resourc\", \"respons\", \"revisit\", \"rid\", \"ring\", \"rip\", \"risk\", \"rpg\", \"rx\", \"say\", \"scale\", \"scam\", \"scaveng\", \"sci\", \"scienc\", \"scout\", \"scratch\", \"screen\", \"season\", \"see\", \"self\", \"sent\", \"separ\", \"sequel\", \"seri\", \"session\", \"sfw\", \"shadow\", \"sharedfil\", \"shit\", \"short\", \"short\", \"short\", \"shot\", \"shower\", \"side\", \"side\", \"sight\", \"sim\", \"six\", \"skeleton\", \"skill\", \"slave\", \"smile\", \"smoke\", \"social\", \"solar\", \"sold\", \"someth\", \"song\", \"soni\", \"soon\", \"sooner\", \"soul\", \"soul\", \"soundtrack\", \"soundtrack\", \"space\", \"spare\", \"spawn\", \"spend\", \"spend\", \"sport\", \"spread\", \"squid\", \"stabilis\", \"stage\", \"standard\", \"star\", \"star\", \"state\", \"station\", \"station\", \"stay\", \"steal\", \"stealthi\", \"steam\", \"steam\", \"steamcommun\", \"still\", \"still\", \"stop\", \"stop\", \"store\", \"stori\", \"stori\", \"stori\", \"stori\", \"strand\", \"strand\", \"struck\", \"stuck\", \"stumbl\", \"style\", \"style\", \"suck\", \"support\", \"surfac\", \"surprisingli\", \"swarm\", \"sweet\", \"system\", \"system\", \"system\", \"t\", \"t\", \"tap\", \"tedious\", \"teen\", \"teleport\", \"tens\", \"thing\", \"thingi\", \"think\", \"thorough\", \"thou\", \"though\", \"thousand\", \"threaten\", \"ti\", \"time\", \"time\", \"time\", \"titl\", \"today\", \"tomorrow\", \"ton\", \"tough\", \"tradit\", \"trailer\", \"transport\", \"tri\", \"trial\", \"trike\", \"truck\", \"tsushima\", \"turn\", \"turn\", \"turn\", \"typewrit\", \"unabl\", \"understood\", \"underwhelm\", \"unholi\", \"uniqu\", \"uniqu\", \"unreal\", \"unsuspect\", \"updat\", \"urg\", \"urg\", \"url\", \"us\", \"us\", \"usa\", \"usag\", \"usd\", \"v\", \"v\", \"va\", \"van\", \"varieti\", \"verdict\", \"version\", \"videogam\", \"visual\", \"visual\", \"visual\", \"voic\", \"void\", \"w\", \"wall\", \"wan\", \"want\", \"want\", \"war\", \"wast\", \"wave\", \"way\", \"way\", \"weapon\", \"weav\", \"well\", \"well\", \"well\", \"went\", \"whale\", \"what\", \"whatev\", \"wheel\", \"wholesom\", \"wife\", \"win\", \"wont\", \"word\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"worth\", \"worth\", \"would\", \"would\", \"wow\", \"www\", \"yard\", \"yearn\", \"yell\", \"yellow\", \"youtub\", \"youtub\", \"zip\", \"ziplin\", \"|\", \"\\u2013\", \"\\u2018\", \"\\u2018\", \"\\u2019\", \"\\u2019\", \"\\u201c\", \"\\u201d\"]}, \"R\": 25, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [14, 23, 4, 8, 20, 22, 21, 16, 11, 13, 6, 24, 15, 12, 17, 25, 5, 1, 18, 3, 19, 7, 9, 2, 10]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el257223554179110722206257735\", ldavis_el257223554179110722206257735_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el257223554179110722206257735\", ldavis_el257223554179110722206257735_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el257223554179110722206257735\", ldavis_el257223554179110722206257735_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "13    -0.420470 -0.313089       1        1  48.505229\n",
       "22    -0.111985  0.515828       2        1  11.170353\n",
       "3     -0.502573  0.050162       3        1  10.700845\n",
       "7     -0.495161 -0.152324       4        1   9.995363\n",
       "19    -0.450056  0.290528       5        1   9.036553\n",
       "21    -0.239951  0.375147       6        1   1.517139\n",
       "20    -0.024988 -0.478811       7        1   1.097022\n",
       "15     0.314437 -0.386843       8        1   0.987106\n",
       "10     0.447322 -0.150161       9        1   0.862563\n",
       "12     0.179098  0.421481      10        1   0.764457\n",
       "5      0.444791  0.081935      11        1   0.729215\n",
       "23     0.357921  0.272776      12        1   0.658834\n",
       "14    -0.228462  0.151977      13        1   0.629464\n",
       "11    -0.185209 -0.311399      14        1   0.554172\n",
       "16     0.141129 -0.319475      15        1   0.393279\n",
       "24     0.014561 -0.260934      16        1   0.308195\n",
       "4     -0.097703 -0.149916      17        1   0.306547\n",
       "0     -0.142248 -0.000651      18        1   0.292892\n",
       "17     0.135342  0.257561      19        1   0.287354\n",
       "2      0.212884 -0.165319      20        1   0.278417\n",
       "18     0.270108 -0.034286      21        1   0.256811\n",
       "6      0.012057  0.182278      22        1   0.229723\n",
       "8      0.228235  0.111369      23        1   0.217930\n",
       "1      0.041428  0.044444      24        1   0.162603\n",
       "9      0.099491 -0.032275      25        1   0.057935, topic_info=       Term          Freq         Total Category  logprob  loglift\n",
       "57     game  25661.000000  25661.000000  Default  25.0000   25.000\n",
       "144    play  11917.000000  11917.000000  Default  24.0000   24.000\n",
       "60     good   8321.000000   8321.000000  Default  23.0000   23.000\n",
       "174     fun   7554.000000   7554.000000  Default  22.0000   22.000\n",
       "249   great   5816.000000   5816.000000  Default  21.0000   21.000\n",
       "...     ...           ...           ...      ...      ...      ...\n",
       "7332     rt      0.017955      1.043681  Topic25 -10.5346    3.391\n",
       "4172  steel      0.017955      1.043681  Topic25 -10.5346    3.391\n",
       "3102     20      0.017955      1.043682  Topic25 -10.5346    3.391\n",
       "3086    mod      0.017955      1.043682  Topic25 -10.5346    3.391\n",
       "3244     ui      0.017955      1.043682  Topic25 -10.5346    3.391\n",
       "\n",
       "[842 rows x 6 columns], token_table=      Topic      Freq Term\n",
       "term                      \n",
       "1222      3  0.072593    &\n",
       "1222      5  0.925963    &\n",
       "1092      2  0.998515    0\n",
       "0         1  0.290370    1\n",
       "0         2  0.653807    1\n",
       "...     ...       ...  ...\n",
       "130      10  0.169158    \n",
       "1351      6  0.022450    \n",
       "1351     10  0.975313    \n",
       "1352     10  0.994535    \n",
       "1353     10  0.989228    \n",
       "\n",
       "[892 rows x 3 columns], R=25, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[14, 23, 4, 8, 20, 22, 21, 16, 11, 13, 6, 24, 15, 12, 17, 25, 5, 1, 18, 3, 19, 7, 9, 2, 10])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_vis(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "15986cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "# Making Topic models for each genre\n",
    "genre = df.genre.unique().tolist()\n",
    "lda_by_genre = dict()\n",
    "for g in genre:\n",
    "    subset = df[df['genre'] == g]\n",
    "    subset_fd = FrequencyDistribution(subset.review_stemmed)\n",
    "    vis = lda_vis(subset_fd)\n",
    "    lda_by_genre[g] = vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "4137d05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the topic models as HTML\n",
    "# https://stackoverflow.com/questions/41936775/export-pyldavis-graphs-as-standalone-webpage\n",
    "for k,v in lda_by_genre.items():\n",
    "    pyLDAvis.save_html(v, f'{k}_lda.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "afa0e64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el257223550595436166552096264\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el257223550595436166552096264_data = {\"mdsDat\": {\"x\": [0.178718200738704, 0.41895236054870977, -0.18120519503227306, -0.06816696347324264, 0.2985944106343457, 0.4293672942156639, 0.07106005825201671, -0.3255948797478253, -0.3507909089970714, 0.15039030975642473, 0.2859625644011496, 0.00394490700507522, -0.3641558816742353, -0.11348532845069864, -0.0889830310666626, -0.30708468327565813, 0.06194856758718802, -0.141465436022937, 0.03898481786985508, 0.21086569547867892, -0.19866319653891143, 0.035510782118246126, 0.13305212830629645, -0.04410248729649149, -0.1336541053363477], \"y\": [0.31867406989822067, -0.2809738468660229, 0.3164244588712238, 0.3768353278772761, -2.9919622540488523e-05, 0.08214475725797898, 0.344030891618668, -0.23967621651241036, 0.21518532338841867, 0.14445874534765635, 0.1865674071415002, 0.19118127365784657, -0.05388520229669437, 0.12122251173341379, -0.4259923164729144, 0.07801551975889488, -0.1115393518372691, 0.009088258441562158, 0.03676739047887352, -0.14723778476619204, -0.11937039923430767, -0.318092631995212, -0.28814544595172276, -0.1915236775232621, -0.24412914239298553], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [52.87645053461504, 7.425169280461709, 5.4905809362919555, 4.65589314799648, 3.140493402238532, 2.863348460027755, 2.8448625417786775, 2.6849568390275804, 2.595288467148547, 2.264626344224709, 1.846477037383848, 1.64289662707944, 1.224857693743881, 1.0173466576508128, 1.006641138384518, 0.9342794670727733, 0.8550399039186878, 0.8121277375381144, 0.7748469177694168, 0.717620652057272, 0.7102682034213149, 0.5459579221027888, 0.43185413380161, 0.3868487992239839, 0.2512671550405637]}, \"tinfo\": {\"Term\": [\"game\", \"play\", \"fun\", \"love\", \"word\", \"recommend\", \"good\", \"secret\", \"friend\", \"like\", \"best\", \"realli\", \"get\", \"great\", \"life\", \"way\", \"10\", \"even\", \"long\", \"make\", \"hour\", \"look\", \"better\", \"dlc\", \"would\", \"come\", \"quick\", \"log\", \"choos\", \"genuin\", \"thank\", \"year\", \"30\", \"bought\", \"ridicul\", \"still\", \"open\", \"complaint\", \"reflect\", \"till\", \"creator\", \"messag\", \"1st\", \"gave\", \"wow\", \"said\", \"load\", \"endless\", \"part\", \"regret\", \"question\", \"peopl\", \"sometim\", \"incred\", \"rate\", \"player\", \"right\", \"commun\", \"even\", \"made\", \"make\", \"full\", \"know\", \"buy\", \"take\", \"back\", \"thing\", \"simul\", \"get\", \"would\", \"time\", \"say\", \"much\", \"go\", \"want\", \"mani\", \"feel\", \"need\", \"see\", \"like\", \"think\", \"one\", \"tri\", \"first\", \"recommend\", \"game\", \"play\", \"enjoy\", \"realli\", \"hour\", \"way\", \"hardcor\", \"ocd\", \"demo\", \"ingam\", \"program\", \"love\", \"fun\", \"throw\", \"ass\", \"dead\", \"great\", \"10\", \"tabl\", \"alarm\", \"goti\", \"9\", \"game\", \"15\", \"good\", \"play\", \"simplist\", \"ever\", \"yeah\", \"nail\", \"funni\", \"realli\", \"nice\", \"style\", \"replay\", \"hour\", \"like\", \"run\", \"one\", \"lot\", \"time\", \"look\", \"get\", \"recommend\", \"everi\", \"obvious\", \"drink\", \"bounc\", \"pleasant\", \"happili\", \"starv\", \"furnitur\", \"conclus\", \"outfit\", \"tldr\", \"movi\", \"premis\", \"closest\", \"bear\", \"curv\", \"nope\", \"micromanag\", \"chaotic\", \"npc\", \"hang\", \"bobbl\", \"quirk\", \"function\", \"sorta\", \"distanc\", \"local\", \"chain\", \"guess\", \"becam\", \"somehow\", \"walk\", \"effect\", \"hous\", \"pass\", \"float\", \"food\", \"figur\", \"cool\", \"learn\", \"music\", \"stack\", \"live\", \"ground\", \"stuff\", \"build\", \"relax\", \"system\", \"long\", \"go\", \"issu\", \"tri\", \"hard\", \"everyth\", \"intens\", \"signific\", \"gear\", \"rais\", \"fenc\", \"descript\", \"threat\", \"currenc\", \"construct\", \"trophi\", \"chose\", \"written\", \"bolster\", \"landscap\", \"grand\", \"harsh\", \"3rd\", \"bolt\", \"oppos\", \"2nd\", \"chase\", \"drive\", \"portrait\", \"preset\", \"stood\", \"difficult\", \"check\", \"head\", \"color\", \"con\", \"ask\", \"floor\", \"pro\", \"box\", \"dumb\", \"send\", \"knew\", \"forget\", \"limit\", \"fire\", \"machin\", \"system\", \"choic\", \"wing\", \"water\", \"lack\", \"custom\", \"big\", \"certain\", \"could\", \"way\", \"regard\", \"empti\", \"loss\", \"newer\", \"grew\", \"11\", \"screen\", \"usag\", \"advanc\", \"flesh\", \"doesn\", \"legend\", \"won\", \"knowledg\", \"orient\", \"length\", \"ordinari\", \"isn\", \"longev\", \"url\", \"randomli\", \"delight\", \"rich\", \"cent\", \"banner\", \"infantri\", \"brain\", \"fantasi\", \"unus\", \"t\", \"occur\", \"infin\", \"abil\", \"attent\", \"divid\", \"isol\", \"slowli\", \"enough\", \"rock\", \"4\", \"\\u2019\", \"certain\", \"1\", \"also\", \"annoy\", \"offer\", \"three\", \"3\", \"6\", \"2\", \"life\", \"easi\", \"bad\", \"time\", \"worth\", \"much\", \"awesom\", \"patch\", \"believ\", \"obviou\", \"bring\", \"egg\", \"storylin\", \"meant\", \"stretch\", \"tune\", \"futur\", \"brief\", \"risk\", \"upcom\", \"smoothli\", \"forgot\", \"fastest\", \"advis\", \"ux\", \"hesit\", \"kudo\", \"connect\", \"expans\", \"wors\", \"inform\", \"suppos\", \"spoiler\", \"cant\", \"huge\", \"not\", \"forward\", \"can\", \"properli\", \"version\", \"fix\", \"updat\", \"pleas\", \"dev\", \"team\", \"half\", \"develop\", \"work\", \"wait\", \"easili\", \"doesnt\", \"scenario\", \"onto\", \"request\", \"sum\", \"pattern\", \"corner\", \"count\", \"earth\", \"rins\", \"stick\", \"percentag\", \"shortcut\", \"common\", \"fidel\", \"certainli\", \"rememb\", \"nostalg\", \"platform\", \"polish\", \"center\", \"eight\", \"doom\", \"highfleet\", \"impend\", \"model\", \"manufactur\", \"via\", \"ship\", \"although\", \"origin\", \"sort\", \"b\", \"optim\", \"uniqu\", \"safe\", \"eventu\", \"pass\", \"neat\", \"concept\", \"number\", \"detail\", \"non\", \"pile\", \"done\", \"way\", \"seem\", \"around\", \"type\", \"gameplay\", \"feel\", \"one\", \"ad\", \"would\", \"put\", \"remain\", \"roleplay\", \"emot\", \"realis\", \"strong\", \"fell\", \"opportun\", \"uh\", \"ear\", \"consist\", \"cat\", \"grate\", \"judg\", \"moral\", \"correct\", \"complic\", \"punish\", \"began\", \"predict\", \"edgi\", \"disagre\", \"piss\", \"dick\", \"borderlin\", \"everybodi\", \"appear\", \"favourit\", \"mistak\", \"due\", \"im\", \"fire\", \"reaction\", \"yeah\", \"dlc\", \"amaz\", \"buggi\", \"hunter\", \"cancel\", \"practic\", \"stage\", \"ive\", \"asset\", \"dull\", \"explos\", \"roam\", \"bias\", \"tricki\", \"boom\", \"spike\", \"mediev\", \"#\", \"bang\", \"static\", \"colour\", \"gruel\", \"cannon\", \"spam\", \"mate\", \"came\", \"im\", \"custom\", \"miss\", \"feedback\", \"graphic\", \"instead\", \"tree\", \"un\", \"enemi\", \"total\", \"larg\", \"power\", \"free\", \"side\", \"best\", \"build\", \"world\", \"map\", \"also\", \"look\", \"2\", \"tailor\", \"raw\", \"thrown\", \"packag\", \"fanci\", \"relic\", \"unreli\", \"significantli\", \"librari\", \"interconnect\", \"evil\", \"occupi\", \"subsequ\", \"output\", \"oddli\", \"depend\", \"incorpor\", \"pictur\", \"discourag\", \"treat\", \"acquir\", \"3x\", \"aaanyway\", \"backdrop\", \"boon\", \"whether\", \"va\", \"slightli\", \"necessarili\", \"x\", \"carri\", \"complain\", \"ton\", \"edg\", \"hint\", \"rush\", \"aspect\", \"stay\", \"depth\", \"easier\", \"coupl\", \"gener\", \"entir\", \"balanc\", \"finish\", \"hard\", \"item\", \"find\", \"probabl\", \"manag\", \"thing\", \"tend\", \"block\", \"pair\", \"invis\", \"arent\", \"weather\", \"zen\", \"priorit\", \"questlin\", \"case\", \"ensur\", \"critiqu\", \"ressourc\", \"lowest\", \"lightn\", \"softlock\", \"deconstruct\", \"inhabit\", \"rend\", \"budget\", \"16\", \"storm\", \"outstand\", \"buck\", \"shake\", \"focu\", \"qualiti\", \"therefor\", \"underground\", \"slower\", \"confirm\", \"gone\", \"non\", \"enemi\", \"plu\", \"per\", \"cours\", \"due\", \"charg\", \"team\", \"produc\", \"built\", \"control\", \"view\", \"point\", \"actual\", \"problem\", \"etc\", \"custom\", \"lack\", \"refund\", \"warm\", \"bum\", \"gimmick\", \"couldnt\", \"unwind\", \"debri\", \"degrad\", \"whim\", \"cheeri\", \"nomin\", \"nuisanc\", \"quaint\", \"sultanwav\", \"tripl\", \"cruel\", \"evok\", \"fieri\", \"landlubb\", \"nurtur\", \"pummel\", \"renew\", \"stylist\", \"cluster\", \"allot\", \"penal\", \"grab\", \"dr\", \"tl\", \"fan\", \"skylin\", \"legit\", \"butter\", \"refresh\", \"mile\", \"citi\", \"builder\", \"meh\", \"genr\", \"scratch\", \"reset\", \"caught\", \"expand\", \"fli\", \"kingdom\", \"fairli\", \"build\", \"charm\", \"quest\", \"hour\", \"standard\", \"manag\", \"20\", \"difficulti\", \"3\", \"much\", \"relax\", \"short\", \"thing\", \"enjoy\", \"stori\", \"around\", \"like\", \"littl\", \"price\", \"dollar\", \"averag\", \"bob\", \"wholesom\", \"nowher\", \"scour\", \"yah\", \"worri\", \"burn\", \"6hr\", \"overrun\", \"tin\", \"hivemind\", \"apm\", \"starcracft\", \"zerg\", \"demolish\", \"flotsam\", \"tamat\", \"border\", \"hmm\", \"oop\", \"scarc\", \"timberborn\", \"hefti\", \"captur\", \"8\", \"meat\", \"frostpunk\", \"beauti\", \"watch\", \"lore\", \"big\", \"grind\", \"fast\", \"paint\", \"chill\", \"replac\", \"fire\", \"depth\", \"sale\", \"left\", \"free\", \"requir\", \"difficulti\", \"gameplay\", \"short\", \"noth\", \"get\", \"care\", \"need\", \"surviv\", \"money\", \"invest\", \"accid\", \"outcom\", \"success\", \"forgiv\", \"freak\", \"bigger\", \"memor\", \"rout\", \"cooler\", \"emptier\", \"gush\", \"theorycraft\", \"exclus\", \"steril\", \"overstay\", \"unintent\", \"playthrough\", \"simciti\", \"tag\", \"rpg\", \"mediocr\", \"tactic\", \"justifi\", \"travers\", \"draw\", \"valu\", \"took\", \"similar\", \"campaign\", \"credit\", \"understand\", \"epic\", \"genr\", \"ultim\", \"probabl\", \"concept\", \"built\", \"price\", \"short\", \"complet\", \"pretti\", \"slow\", \"make\", \"citi\", \"well\", \"end\", \"new\", \"word\", \"secret\", \"upset\", \"mac\", \"pricey\", \"disclaim\", \"reliev\", \"ultrawid\", \"ssd\", \"appreci\", \"21\", \"2k\", \"3840x1080\", \"cutoff\", \"resolut\", \"prici\", \"factorio\", \"m1\", \"moni\", \"rimworld\", \"sock\", \"tad\", \"surprisingli\", \"blow\", \"gui\", \"hit\", \"found\", \"expect\", \"shit\", \"actual\", \"calm\", \"short\", \"kinda\", \"research\", \"least\", \"somewhat\", \"made\", \"think\", \"someth\", \"kind\", \"lot\", \"got\", \"nice\", \"enjoy\", \"say\", \"civil\", \"http\", \"com\", \"display\", \"steampow\", \"blown\", \"curat\", \"gem\", \"quot\", \"sunset\", \"tackl\", \"furthermor\", \"highli\", \"gg\", \"fanstast\", \"propheci\", \"td\", \"arabesqu\", \"victori\", \"war\", \"mysteri\", \"tactic\", \"chill\", \"arab\", \"introduct\", \"discord\", \"peac\", \"anyon\", \"recommend\", \"box\", \"surviv\", \"music\", \"look\", \"relax\", \"fantast\", \"forward\", \"give\", \"tri\", \"short\", \"would\", \"builder\", \"go\", \"fight\", \"interest\", \"breed\", \"bodi\", \"soldier\", \"reload\", \"stabl\", \"hot\", \"tweak\", \"innov\", \"road\", \"bonus\", \"hord\", \"playtim\", \"adjac\", \"driven\", \"indefinit\", \"minus\", \"regrow\", \"signifi\", \"closer\", \"bartend\", \"fulfil\", \"sweet\", \"buri\", \"brilliantli\", \"hive\", \"downsid\", \"ok\", \"usual\", \"intuit\", \"abil\", \"war\", \"unlock\", \"combin\", \"natur\", \"platform\", \"visual\", \"enemi\", \"short\", \"sandbox\", \"singl\", \"well\", \"set\", \"paper\", \"earn\", \"instruct\", \"learnt\", \"6th\", \"rebel\", \"blueprint\", \"unclear\", \"17\", \"nonetheless\", \"12th\", \"snobbish\", \"ancient\", \"antfarm\", \"drift\", \"spread\", \"hr\", \"sigh\", \"tast\", \"anywher\", \"customis\", \"sign\", \"hardest\", \"peasant\", \"pure\", \"worthwhil\", \"four\", \"aw\", \"difficult\", \"act\", \"surviv\", \"creat\", \"stori\", \"neat\", \"difficulti\", \"els\", \"interest\", \"guess\", \"charact\", \"balanc\", \"manag\", \"enjoy\", \"purchas\", \"sustain\", \"bone\", \"assign\", \"beer\", \"strictli\", \"waypoint\", \"artsi\", \"eastern\", \"poli\", \"urgenc\", \"bethesda\", \"pressur\", \"eg\", \"thoroughli\", \"modern\", \"pirat\", \"arab\", \"welcom\", \"uncov\", \"titl\", \"paradis\", \"navig\", \"scaveng\", \"green\", \"relax\", \"clunki\", \"super\", \"forward\", \"combat\", \"builder\", \"nice\", \"citi\", \"polish\", \"ship\", \"20\", \"enjoy\", \"found\", \"head\", \"bit\", \"first\", \"like\", \"way\", \"current\", \"state\", \"steep\", \"equal\", \"modifi\", \"regular\", \"scope\", \"statement\", \"<\", \"philosophis\", \"ye\", \"albeit\", \"fascin\", \"combin\", \"thoroughli\", \"basi\", \"sky\", \"deep\", \"entertain\", \"opinion\", \"vibe\", \"real\", \"tech\", \"buck\", \"impress\", \"life\", \"better\", \"3\", \"yet\", \"mostli\", \"probabl\", \"relax\", \"design\", \"manag\", \"bare\", \"touch\", \"tediou\", \"n\", \"quiz\", \"rng\", \"pleasantli\", \"profil\", \"branch\", \"fab\", \"xxx\", \"pointless\", \"stronger\", \"bind\", \"harder\", \"event\", \"bar\", \"gain\", \"split\", \"evolut\", \"rang\", \"round\", \"act\", \"buggo\", \"comfort\", \"multipl\", \"throughout\", \"within\", \"base\", \"achiev\", \"way\", \"build\", \"well\", \"3\", \"due\", \"point\", \"addict\", \"joke\", \"tire\", \"tip\", \"wave\", \"cheat\", \"incom\", \"storytel\", \"fare\", \"race\", \"countless\", \"biolog\", \"ghibli\", \"5\\u20ac\", \"mould\", \"quirki\", \"ai\", \"space\", \"beaten\", \"sequel\", \"best\", \"seek\", \"meet\", \"bar\", \"insid\", \"background\", \"attack\", \"oh\", \"7\", \"option\", \"ever\", \"6\", \"limit\", \"put\", \"get\", \"friend\", \"alert\", \"cant\", \"fact\", \"cosmet\", \"stop\", \"escap\", \"high\", \"lose\", \"cool\", \"golf\", \"board\", \"card\", \"overwatch\", \"uno\", \"ga\", \"panarona\", \"putt\", \"\\u274c\", \"ahhhh\", \"\\u2705\", \"miseri\", \"former\", \"dont\", \"tabletop\", \"cri\", \"way\", \"detail\", \"without\", \"overal\", \"even\", \"play\", \"wobbl\", \"price\", \"dog\", \"stori\", \"bad\", \"need\", \"nearli\", \"year\", \"40\", \"mental\", \"cup\", \"tea\", \"spare\", \"completionist\", \"approx\", \"snap\", \"narr\", \"internet\", \"buri\", \"lore\", \"entertain\", \"dark\", \"dumb\", \"sit\", \"hell\", \"paus\", \"refresh\", \"kinda\", \"stori\", \"tilt\", \"decent\", \"hit\", \"beat\", \"long\", \"die\", \"earli\", \"never\", \"finish\", \"end\", \"5\", \"answer\", \"chat\", \"hotkey\", \"someday\", \"mediocr\", \"cool\", \"activ\", \"tax\", \"cursor\", \"global\", \"glass\", \"quirki\", \"fuel\", \"rt\", \"comment\", \"damn\", \"coordin\", \"whateva\", \"pway\", \"pweas\", \"test\", \"markipli\", \"especiali\", \"heheh\", \"rob\", \"achiev\", \"choic\", \"matter\", \"super\", \"finish\", \"interest\", \"use\", \"complet\", \"mind\", \"pretti\", \"even\", \"make\"], \"Freq\": [3567.0, 1693.0, 1212.0, 810.0, 379.0, 635.0, 857.0, 295.0, 246.0, 1243.0, 296.0, 904.0, 1073.0, 593.0, 317.0, 532.0, 476.0, 640.0, 313.0, 785.0, 676.0, 498.0, 259.0, 258.0, 622.0, 397.48320171379, 52.61642329258818, 50.01914760960155, 49.95293515173937, 45.96513334659414, 145.82835239319385, 323.1860426885046, 101.56058909148715, 186.215895526825, 41.43359353121681, 467.31321508901345, 104.76388837573754, 34.118219604876415, 33.78115614382293, 33.652468605461905, 31.84567242496792, 31.77533957911723, 31.7383548636451, 81.87234316958566, 31.42417124783109, 131.02316940674547, 81.47945490199325, 29.147644789111776, 172.31088427006802, 61.0027795923664, 146.9675308163631, 460.4083801725489, 164.20637944180461, 102.73348857643312, 78.14377908230799, 257.08224695651836, 256.204983293902, 254.88978125079345, 583.8561074067835, 331.15646960781925, 708.819298829102, 229.2820295150084, 304.08872974923577, 379.0673735323219, 341.4345073218526, 317.36128160821175, 574.699073253833, 208.68099267123935, 889.8537621117879, 533.3702432078229, 845.7825437361472, 383.9661036181892, 545.3797147222376, 518.5773891318137, 542.4677373511704, 354.680998331555, 441.0492653600694, 449.6830828089144, 434.2431262341709, 882.0366684566402, 345.7345797055315, 600.4856385395849, 384.16452731216384, 358.42302020269335, 439.4160112938328, 1013.86650662025, 638.9949943280386, 382.6897670743068, 475.4939995070444, 413.8944087268719, 387.4017492427305, 19.614274475346104, 9.395442234185104, 9.083776258152739, 9.836043709058682, 31.733906777881522, 695.6908386744874, 1014.8927427803898, 34.22664126125478, 35.42537040257961, 56.50097463152727, 457.0813317371083, 359.62198530293233, 35.63918476290282, 3.0345861706054444, 3.0345861706054444, 51.0769208151388, 2452.912546128134, 52.98844601643113, 545.7604481701657, 1014.2615538027876, 6.394569712068493, 171.6982033255439, 23.313886126527446, 12.031993461265632, 77.21085294803301, 379.68857715504504, 132.86374891646417, 50.19994587135168, 30.127762724801187, 172.66330286704346, 266.6677049044157, 84.19101677160937, 145.45961955289312, 76.40319737352115, 93.66661448032887, 65.79237440303307, 68.28981925191081, 59.12274595453805, 47.59104199872385, 43.603395273587715, 35.93556436559676, 26.53332475278401, 26.442439160493425, 23.728191200569775, 23.52468242757566, 22.46632595149652, 20.638524823642737, 18.90979458444827, 16.694645405521786, 15.945486490287813, 15.33375448116932, 14.368207677889698, 43.349162111824135, 42.684126876700525, 9.457290252230461, 9.143259046138088, 9.10427673963898, 58.28259230183047, 23.078880174305077, 7.932644837984046, 7.474202095834712, 50.628720606304675, 6.820337914736428, 23.731289154391817, 24.6806110524921, 19.619984589740206, 73.13003669029395, 28.755964281681308, 33.84087535325875, 96.15672890196984, 57.35086845600903, 46.07318118555947, 48.38752684586779, 30.943545020730873, 61.25982304985293, 44.70330284598381, 61.547594492236996, 73.5796921282913, 63.59098473842123, 29.654873260963956, 55.7222590606341, 37.899928321635116, 52.77203242252206, 59.85815468035421, 49.786582886738195, 42.5857693547051, 53.22651731934253, 53.819629788705726, 43.88639084337694, 48.15378371814016, 43.3318738549275, 42.91775948283234, 47.87631520300494, 45.33166065879708, 34.44886517262132, 29.965497889899062, 29.44075160456918, 27.933513962864872, 24.770043905590033, 21.482045650452203, 19.058231567724253, 18.171093117827887, 16.52339347388843, 15.888556116535652, 15.514126623775358, 14.694304627855146, 13.807075298047913, 13.026996223147924, 12.557668223463896, 12.162924230628022, 11.854998952640525, 11.719068875960474, 10.7467858895737, 32.274994984748886, 8.743157914728046, 8.514221479268448, 8.47458860814029, 57.50806351620528, 90.07480005481463, 91.52867982885223, 51.367563538795494, 39.005424863556605, 77.38828825772026, 33.19251856484358, 35.7065891790481, 32.9902340832438, 21.56073345327536, 27.185825056196666, 25.878900938025254, 42.16224080735778, 51.46901959928759, 46.08174084311992, 26.211092572212355, 51.75779619605608, 36.81067785616012, 35.16484551038922, 38.20077129275923, 43.78800289281355, 41.316873036492815, 40.932257893722586, 31.909240263480395, 34.35401210565302, 34.864288605174835, 39.695664904623285, 38.87733239241924, 28.461648680814033, 19.30627282052913, 17.829281731806535, 16.646428756034464, 47.526865638075854, 15.89418374046778, 14.794189646969178, 11.71631664040499, 11.413950874535326, 10.077456298128372, 8.924710402007916, 8.193605378010309, 8.126737260054078, 8.082390306601663, 7.652863243523081, 7.608949223611928, 7.483978455183695, 7.396438239540925, 29.7254678913429, 6.954998799354617, 25.858691317387315, 6.741614285660534, 6.507075413133836, 18.21673426361555, 35.44026917705944, 26.639584656447774, 17.551474408410304, 29.557993489374113, 15.502340572921705, 12.913451633950906, 33.97724498413043, 36.31849593749861, 12.983987235379475, 17.636578415065898, 25.891065551225147, 63.82422352793295, 20.767329656171068, 55.02918246843881, 24.662421233106357, 31.023747462375244, 34.873187538328914, 52.37718867674104, 33.23358392961862, 26.426148420604953, 24.782703963442305, 38.7813169280157, 27.92412720790313, 36.11672208837797, 34.516228086651886, 30.68527580541827, 30.630471399745694, 33.68137130574458, 30.654903410299998, 30.41207558452973, 115.49298661234114, 81.56625665267256, 76.59078096178867, 28.42035780211572, 99.63816708710492, 27.71994327448049, 26.270529532872118, 21.007754987866793, 17.472940913515263, 14.498262844028151, 109.7211531220848, 13.083167702623022, 11.189531652576942, 10.292744484229658, 8.880218565731495, 7.973895930041872, 7.7576279676892685, 6.231702121675917, 5.955549083992834, 5.747218465964299, 5.419824626555291, 33.524771455379586, 77.39475386100773, 41.63743074784435, 12.509612704152746, 21.170573158234067, 36.011576625748575, 46.39176813098062, 55.67489621519067, 55.85246427505442, 51.37166966200473, 58.30715677896014, 24.54556270468096, 52.58869535776638, 106.56837603164371, 110.05064941021865, 58.559649736351446, 100.05625702767449, 57.2266439740268, 42.214651823052705, 63.08584850894532, 55.08733537138323, 44.07976443374212, 30.85947952273415, 18.315717720788587, 18.093508251437843, 14.506820576849709, 14.486196458966088, 13.100733814130463, 12.741553131250894, 9.759106992683607, 8.973416601949662, 8.897719928978654, 8.478804790069812, 24.89923288665419, 7.639831812914612, 7.518326518665366, 21.766662778857707, 5.928516348223684, 16.262438850593927, 37.81059703950194, 11.58822590900912, 32.26023651702934, 51.02150980228414, 21.032749596146697, 11.519733361018664, 3.0275187729715345, 3.0275187729715345, 3.0275187729715345, 49.755308700379, 7.369333969281365, 10.473588213084074, 38.260999315053475, 38.43701492508053, 35.58557314860388, 29.817966133402088, 32.00011407102556, 24.041568055415404, 48.45206227507633, 21.661124907469727, 24.508294359316213, 37.70631553002795, 24.27312562094108, 42.10899638529894, 27.72788314741494, 34.66529845844704, 24.1032408488175, 23.17589651993471, 35.03981578027111, 46.59433683091645, 34.67956267796772, 35.768654926743864, 26.28281012096558, 28.396939249276105, 30.404794736237974, 30.983536810732453, 26.185315395256236, 27.26079589975983, 25.686468567836208, 34.40521162658127, 32.09549448835277, 30.21707683979012, 29.56487856082368, 25.298887588103234, 24.416126054499664, 23.10371262521915, 22.10599947442501, 21.58082807157582, 20.586089106352123, 20.292502924988057, 19.4484598501239, 17.91278760874159, 17.424934373517978, 17.37672832831955, 17.207085107087526, 16.554141643961092, 14.583892740376328, 14.422968992699934, 14.419506752448882, 13.727990591880259, 13.702584725326593, 13.372614722629862, 13.170959700098372, 12.590066537522809, 47.31891246367347, 18.46060605898508, 19.079097447408987, 28.159359693495826, 24.862422580106337, 24.002991042896447, 16.676970922271604, 15.329322565478721, 257.21095708170265, 182.7673843601612, 93.04564502720811, 26.898649555229728, 25.36848583219733, 24.63481290191885, 22.87368378785348, 15.270419059208955, 14.841775522783776, 14.415441687761072, 14.21037059819507, 12.944506212447353, 11.205173619122213, 10.1830595103063, 10.0454850512041, 9.113436632512308, 8.692267568446136, 6.8390242696611665, 6.726959425637448, 6.725198258529665, 5.935634591143728, 4.172137597796142, 24.03111331265896, 11.855471711214589, 10.53615570586172, 54.333954036185226, 63.66725002121846, 70.06755133784748, 46.42298135661201, 13.03363947342407, 76.84122324746956, 43.62843579434555, 24.093364435516794, 16.714289551638892, 27.146463576849783, 23.33749367585824, 24.73080244581135, 29.825720369865564, 26.457013750531672, 25.51303451304635, 29.557237765990035, 29.851210988085356, 26.522467427150445, 25.71966176195343, 29.199606070422213, 28.530248588312624, 25.56756182977486, 18.8631452813882, 17.428156776078527, 17.278187843221623, 11.980305463853364, 11.808912792409647, 9.649941717974551, 8.86881513530924, 7.845793975331064, 7.808425720486699, 7.715467498521311, 7.624266385040185, 7.09064068847971, 7.084063964035076, 6.204694710892943, 5.583249160515142, 34.48369880627499, 5.281868794380982, 14.070578016952492, 4.8460630676714, 11.96983050030292, 17.996218069219143, 3.130569183756698, 3.130569183756698, 3.130569183756698, 3.130569183756698, 39.16819699205556, 7.175993752614825, 23.85898043603678, 6.356400722302487, 15.674125994412472, 16.130110841199183, 11.522602877758336, 26.205029690436668, 12.341108075577468, 10.70790267625988, 14.91363783887698, 20.654876538488608, 15.160485334798594, 13.940655592211643, 13.44603507939007, 15.450118146037084, 16.168100023923795, 15.088674576501111, 14.421833948853687, 15.2656217551895, 15.83628693944084, 14.574448214059146, 15.424235330259073, 14.464619264170565, 14.026719373575652, 14.466825319398055, 23.673888053825497, 23.442300227357983, 15.704715050233785, 15.599385704425277, 13.227410330162188, 12.081223140189826, 9.566023643342591, 9.382048416032058, 9.113983276797592, 34.26538862078957, 6.948729006408686, 6.947038411671274, 6.946930848848208, 6.803231985636601, 6.606030473181876, 5.983861199382556, 5.64655995461789, 5.087643815499248, 4.807294551345671, 4.806769689821597, 12.553975826470761, 4.561663300172348, 4.44565792180738, 23.54447927770977, 13.416973192710765, 22.61835098167184, 46.54101664003439, 11.1684765603912, 14.742571792621247, 27.11289339078225, 15.845662895693142, 22.161730466226317, 26.14944586354551, 34.590805018339594, 30.18614683469517, 17.164400001173032, 27.10941759107562, 29.404965919035018, 23.12159857302635, 30.794965928999122, 17.616233689414024, 17.671176328911496, 27.105387645725543, 16.90537182854458, 24.01202502230446, 22.869748388580128, 20.14875361655568, 19.423246317064937, 18.216269800531876, 18.323510498285348, 17.181869104688865, 12.193137546677601, 9.851824318540817, 6.625162601495891, 6.5930046706058345, 6.31601123880618, 6.313962879974185, 5.5206687794069085, 4.542295336830705, 2.9336564626778094, 2.9336564626778094, 2.9336564626778094, 2.9336564626778094, 2.9336564626778094, 2.9336564626778094, 2.9335384615758193, 2.9335384615758193, 2.9335384615758193, 2.9335384615758193, 2.9335384615758193, 2.9335384615758193, 2.9335384615758193, 2.9335384615758193, 2.9334109402150705, 2.9332829177880706, 2.9332829177880706, 30.4114985464858, 18.342538244401585, 18.342538244401585, 41.50939846791968, 9.503541184886762, 8.137602517817012, 6.162227993736471, 13.125715037157972, 12.504293675435514, 50.075578983898914, 37.319735018731016, 10.137827923656744, 22.085406930825293, 11.330436751225257, 9.107629699858839, 6.092854369223935, 7.3424890456942125, 19.868667791752284, 13.477721093053152, 15.569184650311707, 35.315269590279655, 10.929054624874508, 20.383000272287987, 32.617296402913155, 11.093327190831928, 17.657927417830113, 16.641458409295574, 13.469320216297474, 20.911933823362908, 21.449591939991322, 16.850641543284212, 15.337505645941166, 18.941085931789274, 18.057639991434236, 16.129146227881833, 16.40320641785113, 16.877875496130727, 15.189699119107274, 14.79113498934927, 51.32271090070599, 22.07687206957437, 16.17582158000552, 8.665708093241403, 6.367165677009547, 5.705371065036341, 5.697533218931321, 35.3854817091347, 20.404128300531326, 2.527489632911379, 2.5274515288980948, 2.5274515288980948, 2.527377562284073, 2.527309572770174, 2.527309572770174, 2.527309572770174, 2.527195073945943, 2.527195073945943, 2.5270878597124873, 2.5267820936843206, 2.526686460082353, 2.526686460082353, 2.526686460082353, 2.526686460082353, 2.5261259201614443, 11.830526573047585, 40.48496123251059, 8.603405042971968, 5.008788983209856, 56.76456170767644, 51.61455476841091, 9.687624855468162, 38.01898305767111, 31.80226397930712, 24.69525434334969, 15.441282297416784, 17.114882286052445, 10.453644777267137, 20.948323841153584, 15.285157453654389, 30.000930372493233, 23.04712277833806, 25.224336786310985, 19.606978894154334, 15.075522621040985, 21.697977715678316, 18.004321106878365, 18.670607883162752, 27.277153892249345, 16.733053175996005, 18.33098606562476, 16.400305023599447, 16.14527262127695, 26.3546502277793, 17.233946565154547, 12.823769665613144, 10.767832843953107, 8.316002238341598, 8.309087336772897, 23.012236398755253, 4.974645843785209, 20.619237376049522, 2.5362243015405204, 2.5362243015405204, 2.5362243015405204, 2.5362243015405204, 2.5361684511284963, 2.5361684511284963, 2.5361656586078953, 2.5359386887390314, 27.517855446844276, 5.7278854573092115, 15.32143484269232, 14.490367059452788, 6.111067066923896, 6.726027265877899, 5.714174801717462, 2.536227559481222, 6.880834679046191, 19.297008658268574, 21.881235768383558, 15.729883799266778, 6.449632264613169, 6.283440055233858, 15.079614969831225, 6.5462261727687485, 11.430080966207667, 8.064691518797233, 12.565571969706088, 11.418089262185989, 8.4514078389329, 13.745074338999661, 11.378962945762533, 12.499054128985547, 12.648646354791067, 8.625591001152571, 11.265862138053203, 9.101468780723696, 9.220937776524025, 9.17988400032561, 9.139965228611548, 378.17731867604147, 290.3361726689219, 6.121461472635094, 4.986735276815649, 4.986463261529513, 4.24680795329247, 4.094228254790103, 3.695004989391692, 2.918684722315532, 33.93903900808303, 1.8859443028222203, 1.8859443028222203, 1.8859443028222203, 1.8859443028222203, 1.8859443028222203, 1.8858726148207385, 1.8851987169053124, 1.8851987169053124, 1.8851987169053124, 1.8851987169053124, 1.8851987169053124, 1.8851987169053124, 12.025137559904168, 7.938455426691861, 1.8859443028222203, 24.630083991611247, 15.808176336160203, 12.413720299583744, 8.106528329934124, 15.289494149988892, 3.9546512553830118, 10.736777376087334, 8.105470356259154, 5.639523455171775, 9.57309413990927, 5.876922186519156, 12.325298752779911, 11.690424899419284, 10.42233630173446, 8.20206402821689, 10.55486239475172, 9.325311547379407, 7.006471495482146, 7.4081950910050365, 5.50580245979149, 52.34527142076031, 23.787733523460066, 18.8032227572794, 14.185916626144392, 7.222508189468766, 6.696158104391434, 6.497577344639316, 6.353084359248559, 5.370402123468776, 4.455964321618051, 4.270750352661808, 4.041499205058383, 78.12988894105551, 8.303386614618189, 2.1263173305449463, 2.125473322138692, 2.125473322138692, 2.1254730371932475, 6.7598844433544, 48.3541431616868, 5.183763427290632, 6.522264448049528, 28.830800578460863, 2.125473322138692, 3.1742928202863423, 16.525465756355153, 14.429454938329041, 26.488248123266334, 63.514645013863465, 11.759424941944344, 23.577516731541238, 18.803311660258046, 29.172607456158023, 16.58670509147957, 11.414177063060848, 11.704582062155374, 14.075117299769817, 13.628974798158401, 10.597622580481833, 12.336091833372109, 9.360192848626372, 10.90524056398048, 9.134577604893238, 9.472464203166615, 86.23742578050886, 58.30182934977882, 26.185180079367274, 22.71580348722815, 20.711854389398248, 18.02713230481095, 7.8200599191366855, 6.313572374389343, 6.12910465013933, 5.958481640962861, 5.621417983513097, 4.665095789282734, 2.225970970286939, 2.225970970286939, 2.225970970286939, 2.225970970286939, 2.225970970286939, 2.2256627304448418, 7.15172970380553, 1.8360750821352134, 8.771438801652335, 8.83548280024871, 4.0982914851913055, 4.830476634917889, 3.0863429521471812, 7.11736748086692, 16.531813369164567, 13.574215053738392, 9.04086128590176, 11.80972502403529, 13.650163055982857, 13.319525495139652, 7.599801950479939, 9.260261294253516, 8.63732317695516, 10.636767592016549, 9.816900724567365, 11.177379593654187, 9.323900565070758, 9.81251234887971, 11.411519829446458, 7.9434966762079116, 13.2069715578626, 11.475039698906812, 8.982805169132108, 7.337457469063658, 5.847372048956467, 5.638987598771079, 5.416149420226207, 4.444281953879068, 4.048719318841014, 14.092102288006888, 2.3990590803952965, 2.3990459528015915, 2.3990006254497427, 2.3990006254497427, 2.3990006254497427, 2.3990006254497427, 8.026389623646919, 1.9789638211630247, 8.285544670950134, 7.074894201206126, 6.337891770761079, 8.24857043174476, 4.818740372048673, 4.699291158476026, 8.767804907100372, 6.947735870152918, 11.41470725976291, 10.40951997906929, 11.411254454928088, 9.16339528346852, 16.40863611287412, 11.542088512218651, 15.107904590508339, 7.41957973223403, 8.340328844075385, 8.64990623572758, 10.759032006152527, 8.45926087999415, 8.688963056213527, 8.04094490508978, 8.265506513766697, 8.121138155056881, 7.358714263247621, 9.697589861610734, 8.503932810860482, 7.554300458887428, 5.564038775987426, 2.283179138014505, 2.283179138014505, 2.2831540880745758, 2.2831540880745758, 2.2831540880745758, 2.2831266749327668, 2.282908787719424, 7.404652590393012, 4.794903057210947, 10.202437843409475, 10.789159426983886, 10.830713023562419, 2.2831540880745758, 10.002890967217219, 2.2831266749327668, 12.146779205296935, 2.283179138014505, 2.283179138014505, 2.283179138014505, 14.058620869648816, 55.90606647747861, 11.85745050855752, 21.847336303188296, 12.43356982321715, 13.582767484357124, 11.703422681698568, 17.795798720891955, 11.947525366137809, 7.935578032298036, 7.638733880939596, 9.781484473634244, 15.797642525791366, 10.185184579123405, 8.641584593971247, 10.017480430344067, 9.52034215835889, 10.501661739061154, 8.39504497575327, 77.40782109857953, 54.415666588313684, 21.90213667906048, 9.081898790359219, 9.007927948067593, 8.69649190574105, 4.748491129489223, 10.085337301994567, 24.533001132078876, 2.3058790251399515, 88.54937269583492, 3.449986059498419, 7.555463232598399, 9.397896016314457, 5.51378113901246, 2.0452555314508967, 7.459828956738236, 12.880300185060243, 9.59308841160941, 11.015360017484275, 2.136231446833461, 29.91366749743622, 3.3093079066371196, 3.552436284521962, 5.887696642381059, 35.08820688332924, 27.09919307895159, 25.160674278348036, 12.466304922055308, 8.418268459061693, 9.847654986117059, 8.409945393273206, 6.653838919133188, 6.423322739261953, 47.1558991578046, 45.11615034792989, 28.614084621281123, 25.92468283062449, 11.068826177405478, 6.679272815674419, 5.326442874723711, 4.302131136456688, 9.300223475749066, 2.453360959243272, 2.292011123016516, 10.40606049104304, 5.165825878696832, 5.452247470076371, 11.366556426720084, 10.062252507990555, 3.5481505655767718, 4.891416541164035, 4.647050835709518, 2.292011123016516, 12.492081130079542, 6.869864123985323, 17.274707619390497, 2.2920100398947305, 7.224181421095387, 22.60464767773431, 6.32428701579852, 6.783056245387526, 12.838100414568721, 8.944706512949699, 10.049044487692935, 8.681374478538649, 8.637190908178752, 8.081769522710491, 6.169425295689556, 6.341847452677845, 67.05355620576513, 25.287925085974685, 21.22229775888081, 9.176101973539716, 9.167731110250374, 8.945392375974059, 5.569365198795648, 5.25024168796519, 3.8316281294497605, 14.394650539388417, 3.3448495744911035, 2.080296399807983, 2.0801606929815026, 1.7162202941009794, 1.7162202941009794, 9.833214741753036, 22.32017297858639, 32.98470209659492, 2.080296399807983, 4.369838739087202, 73.42245640577204, 2.5453547025377334, 4.948287172077523, 1.7162202941009794, 5.542969804766637, 4.147040433226732, 5.171490766791555, 6.598913779738595, 7.644546367058266, 8.100470018507185, 6.99100427024009, 4.634726142529763, 4.513663664485476, 4.2075646787682475, 4.170731015366977, 242.87589769178206, 3.12089045192457, 12.939670623624055, 15.143984600388173, 2.1981300143480826, 15.510015747421367, 1.1951479725326803, 7.309318450317711, 5.0059651893005945, 6.511022390238329, 0.03512817147373321, 0.035127998603066535, 0.035127986255161774, 0.03512796979128876, 0.03512796979128876, 0.03512797390725701, 0.03512795332741574, 0.03512794921144748, 0.03512794921144748, 0.03512794921144748, 0.03512794921144748, 0.03512794921144748, 0.03512794921144748, 0.03512796979128876, 0.03512794509547923, 0.03512795332741574, 7.422096770747604, 1.255889258796338, 2.5787240497639243, 1.89892691328829, 2.5638270106145313, 3.635964431492968, 0.0351279656753205, 0.7893889730663952, 0.035127998603066535, 0.3966925750902432, 0.09345184088410355, 0.03512965733827296, 0.03512801506693955, 0.03512800271903479, 0.035127998603066535, 16.79538956804017, 15.895877008547746, 6.9662532870152365, 11.08333530488819, 3.3712281800567685, 1.8302882669891485, 4.9642914308053125, 4.622413399452366, 22.510395339670186, 3.4330248578838827, 7.149674861612364, 19.255726259772747, 8.9797564191814, 4.318610960120419, 8.184727448587118, 10.678699255322813, 1.8302882669891485, 1.8302882669891485, 6.327593857524413, 18.318209371095936, 1.8302868511712025, 4.715463786497476, 7.169415611233037, 2.5120300926972408, 12.683975069404939, 7.481352396603904, 5.019173253784304, 5.81918731936635, 3.5003970267822306, 4.197811013273592, 3.416166949570863, 50.15321221318341, 29.010217872488777, 1.3728789965189943, 1.1327508171545044, 5.485859181913926, 36.93764495567702, 11.539429270483192, 1.3728789965189943, 2.566512647500845, 3.5158567478760947, 0.7947280307787868, 1.3916572014916364, 0.9655808970295587, 1.247551770754525, 1.3795896693658356, 0.025010752054169593, 0.0250107305008997, 0.02501071134243757, 0.02501071134243757, 0.02501071134243757, 0.025010737685322998, 0.0250107161320531, 0.02501070415801427, 0.02501070415801427, 0.02501070415801427, 1.6063840188916476, 1.1327508171545044, 1.182955567163492, 1.0225430665329822, 1.3692206498585415, 1.5230867039795792, 1.387089670852391, 1.2978438833508206, 0.5123447519980322, 1.0161521867359977, 1.309139406086672, 1.0233373763728497], \"Total\": [3567.0, 1693.0, 1212.0, 810.0, 379.0, 635.0, 857.0, 295.0, 246.0, 1243.0, 296.0, 904.0, 1073.0, 593.0, 317.0, 532.0, 476.0, 640.0, 313.0, 785.0, 676.0, 498.0, 259.0, 258.0, 622.0, 405.3375571565192, 53.715195744653386, 51.11791504483141, 51.0517021634598, 47.082244253150016, 149.392185113126, 331.1054464261669, 104.20830751897394, 191.08669666967722, 42.53251095278379, 479.8191286943867, 108.12392920631095, 35.21698661659685, 34.879923155543366, 34.75123561718234, 32.94443943668835, 32.874106590837656, 32.837121875365526, 84.7238047338046, 32.523116672713996, 135.63555020541648, 84.45109007767857, 30.246411800832213, 178.8077881909729, 63.46441219404356, 152.91589781356217, 484.6677932357931, 171.84943188309157, 107.35539272635158, 81.53522757256904, 272.6449041901995, 271.8624218015973, 271.12863053381375, 640.5201394194421, 357.051606781078, 785.3419768088148, 245.72244201067744, 329.14572454212, 416.1979415124833, 373.9228214204834, 350.7505120102494, 658.2300864675406, 227.18595572421074, 1073.573993636185, 622.6334704011628, 1030.1931872679224, 440.35790749952906, 649.0589753379115, 621.7443176898854, 655.9101657043414, 405.93664249492355, 520.7757150768929, 539.7116195287194, 518.1668082959785, 1243.067959367245, 396.8473720879155, 809.7368380355047, 456.7780648359394, 418.5344716557628, 635.3049005850629, 3567.2596368073837, 1693.9946815563367, 496.6170757458737, 904.8368181119899, 676.5670577940139, 532.3168847953008, 21.683905453625577, 10.494678724916815, 10.18301274888445, 11.046505315987364, 35.825395813596465, 810.9179391783539, 1212.9054826576241, 41.996469908243164, 44.5975004824655, 71.34812949828688, 593.6510856645085, 476.282071931537, 48.0075191846801, 4.133822661337154, 4.133822661337154, 74.0000412519846, 3567.2596368073837, 79.64879008924447, 857.1351781223268, 1693.9946815563367, 10.726031258162717, 290.7526843669832, 40.65104100856785, 21.10586592736068, 139.69809272314268, 904.8368181119899, 294.99766414523344, 113.40492575871278, 67.08555849903661, 676.5670577940139, 1243.067959367245, 315.2184730623813, 809.7368380355047, 466.95210807506714, 1030.1931872679224, 498.96110261804466, 1073.573993636185, 635.3049005850629, 422.24914821471896, 44.70274275316127, 37.03491181615113, 27.63267220333838, 27.541786623842853, 24.827538651124144, 24.624029878130028, 23.56567720365989, 21.73787264112473, 20.00914203500264, 17.793992856076155, 17.04483394084218, 16.44528596570075, 15.467555128444065, 46.690100183814195, 47.094231923624804, 10.556637702784828, 10.242606496692455, 10.203624190193347, 65.59794971267729, 26.121123458546013, 9.031992288538413, 8.573549546389081, 58.094526803454855, 7.919685365290794, 27.851937800931324, 29.050735359265396, 23.431503382113902, 96.28188529710488, 36.20238410309487, 44.443595371908515, 145.02913667044928, 84.61921113071082, 75.95556384888773, 87.1380354713185, 45.683577309664656, 130.8056338233722, 81.0354403601244, 147.56318541850706, 201.7151299853251, 161.17389487526037, 43.03709582036183, 174.19551705112858, 79.62089649670796, 190.25430291574992, 344.298617128981, 219.35700487589884, 137.23000620582474, 313.23169812987993, 621.7443176898854, 241.11389878146724, 456.7780648359394, 233.03315793573196, 269.0784527890876, 48.97371949366806, 46.42906494946019, 35.54626946328443, 31.062902180562176, 30.538155895232293, 29.030918253527986, 25.867448196253147, 22.579449941115318, 20.155635858387367, 19.26849741158294, 17.620797780300336, 16.985960407198768, 16.61153091443847, 15.791708918518259, 14.904479588711025, 14.124400513811036, 13.655072514127008, 13.260328521291134, 12.952403243303637, 12.816473166623586, 11.844190180236811, 36.30906248764018, 9.840562205391159, 9.61162576993156, 9.571992898803403, 69.97302298466319, 113.23675879694267, 119.90262888488861, 68.1719665916141, 50.16066926651747, 118.13106878474797, 43.76890439557059, 48.06702585704176, 45.80833105558755, 26.94341050893861, 37.03856102420509, 34.75028240024233, 69.52021382519436, 106.00582268240848, 92.03204595179773, 37.68320642673285, 137.23000620582474, 73.47268800934302, 70.50650618914158, 90.44889227376056, 148.44950657371055, 135.37813717290916, 143.80835665940765, 92.72534018837074, 370.989091288024, 532.3168847953008, 40.79493058881291, 39.976598076608866, 29.56091436500367, 20.405538504718766, 18.92854741599617, 17.7456944402241, 50.72186720232478, 16.993449424657413, 15.893455335028973, 12.815582795357958, 12.513216558724958, 11.176721982318004, 10.023976086197548, 9.292871062199941, 9.22600294424371, 9.181655990791295, 8.752128927712713, 8.708214907801562, 8.583244139373328, 8.495703923730558, 34.3061226419393, 8.054264483544246, 30.06669293393148, 7.840879969850163, 7.625052341756329, 22.023868415535848, 48.24066998306682, 35.719984158099145, 22.641692575603095, 41.547723357699496, 19.90683842922241, 16.720527212223573, 56.8499511442498, 65.58837499837291, 16.963224163680714, 26.090726271959667, 45.63336367722429, 248.15656059271072, 37.365467236203195, 218.6531130838026, 52.94168520356602, 92.72534018837074, 144.3097088345084, 508.91571084954967, 147.61798667271222, 76.67670502780393, 68.7387431126455, 347.6384985998208, 114.99745233366593, 340.48275271901156, 317.57810382216377, 206.91775739244918, 312.93835230091895, 1030.1931872679224, 385.00813964917495, 649.0589753379115, 116.59714656994022, 82.67041662208324, 77.69494091938775, 29.52451775971481, 103.58041941558022, 28.82410323207958, 27.37468949047121, 22.111914945465884, 18.57710105997416, 15.60242280162724, 118.21333162144596, 14.187327660222111, 12.293691610176031, 11.396904441828747, 9.984378527200747, 9.07805588764096, 8.861787925288358, 7.335862079275002, 7.0597140167511725, 6.851378423563385, 6.523984619351636, 43.030356463424134, 103.81116355142325, 56.9919778572936, 17.562313983515963, 30.187349425209085, 55.596972819787716, 75.01075038873321, 96.28148808424481, 97.78527835690471, 91.98646128434875, 107.97792479158282, 36.90258573737888, 99.51149655857984, 274.0045410906336, 287.80045262892725, 125.70773579169033, 302.6169801645664, 133.1262772028062, 91.34079653922086, 257.52797413609886, 416.5214526417018, 227.87144260229843, 83.52756105038351, 19.41511698128377, 19.192907539760455, 15.606219837344891, 15.585598266828402, 14.200133074625645, 13.840952391746077, 10.85850625317879, 10.072815862444845, 9.997119189473837, 9.578204050564995, 28.184056404826386, 8.739231073409794, 8.818590893488889, 25.555960800728784, 7.027915608718863, 20.629087127830456, 48.8127145463313, 14.964059470709861, 41.95641224186975, 66.43263829174377, 27.459354561215587, 15.109099803465107, 4.126918033466715, 4.126918033466715, 4.126918033466715, 70.0510291921627, 10.307309321537527, 14.885851213632028, 60.67268408240568, 69.03134651612824, 63.34268460740472, 52.68364867885731, 60.75959179359842, 41.51006642987855, 111.35908768905884, 36.46120004112825, 44.19994989685422, 87.1380354713185, 44.73513907743011, 112.7830326522388, 59.282479762044446, 96.64491797052011, 51.31350802961006, 49.097391892310036, 182.80870477371033, 532.3168847953008, 304.0238370295958, 359.03811591559787, 139.90347268513244, 270.160560195869, 520.7757150768929, 809.7368380355047, 172.87866100074442, 622.6334704011628, 269.58038260153296, 35.50564377314231, 33.19592663491381, 31.317508986351168, 30.665310707384727, 26.39931973466428, 25.51655820106071, 24.204144771780197, 23.206431620986056, 22.681260218136867, 21.68652125291317, 21.392935071549104, 20.548891996684947, 19.013219755302636, 18.525366520079025, 18.4771604748806, 18.307517256047127, 17.65457379052214, 15.684324886937372, 15.523401139260978, 15.519938899009926, 14.828422738441303, 14.803016871887637, 14.473046869190906, 14.271391846659416, 13.690498684083853, 66.51490467221491, 24.73688194807322, 29.290947756493534, 69.35867012877256, 93.19633463335133, 92.03204595179773, 27.798709677800826, 40.65104100856785, 258.3152658179675, 183.871693096426, 94.1499537634729, 28.00295829149452, 26.47279456846212, 25.73912163818364, 23.97799252411827, 16.374727795473746, 15.946084259048563, 15.51975042402586, 15.314679472261789, 14.048815367621609, 12.309482355387, 11.287368246571088, 11.149793869781336, 10.217745380520835, 9.796576304710923, 7.9433330059259495, 7.831268161902231, 7.82950709265895, 7.039943327408511, 5.276446334060925, 31.663643851548706, 16.03331446569744, 14.556842748002103, 76.06463972583659, 93.19633463335133, 135.37813717290916, 84.82796729452564, 19.865649546737316, 294.11808995515725, 125.18226099527094, 56.134552197751525, 30.36457885228603, 76.37153353098353, 63.93733385979397, 73.47837136671805, 124.02475345846966, 137.16737939926998, 122.58590064350913, 296.66789897854585, 344.298617128981, 262.62480504527133, 251.77513410017363, 508.91571084954967, 498.96110261804466, 340.48275271901156, 19.960669268199165, 18.525680751665067, 18.37571185805589, 13.077829439439904, 12.906436767996187, 10.747465693561091, 9.966339110895781, 8.943317950917605, 8.905951396942791, 8.81299147410785, 8.721790360626725, 8.188164664066246, 8.181587947104562, 7.302218686479478, 6.680773136101677, 41.58698910302978, 6.379392769967517, 17.213735862770335, 5.943587043257935, 15.463972562354597, 24.159137926902503, 4.228093159343234, 4.228093159343234, 4.228093159343234, 4.228093159343234, 61.71201453150515, 10.533443437332155, 40.2625314642731, 9.704186725841424, 28.71454181642637, 31.9411565532875, 22.16444613709538, 83.13044434997458, 26.214181659017747, 20.589030583854985, 45.91118048212593, 123.32129059123137, 58.15495650965467, 58.274468260588996, 57.8774976899117, 98.53687984092981, 135.7168964447104, 105.87123175222894, 100.22596122766221, 172.473124779119, 233.03315793573196, 121.73376399182293, 306.60433719370457, 143.7446202609362, 144.94698719693878, 658.2300864675406, 24.77654107879175, 24.544953236575445, 16.80736805945125, 16.70203871364274, 14.330063339379649, 13.183876149407286, 10.66867667618324, 10.484701425249519, 10.216636860439511, 38.57047704596055, 8.051382015626144, 8.049691420888731, 8.049583858065665, 7.905884994854059, 7.708683482399334, 7.086514208600014, 6.749212963835348, 6.190296824716706, 5.909947560563129, 5.909422699039055, 15.571159622493846, 5.664316309389806, 5.548310931024838, 30.49951750778608, 17.498440929779477, 32.058897750978765, 72.40302502335904, 15.21345841844408, 21.26228070960198, 43.93445310023214, 23.3597255813739, 39.79877962059738, 51.31350802961006, 76.37153353098353, 64.14879950652526, 28.79721517893112, 59.50984008313302, 69.35867012877256, 66.12140860402108, 133.1262772028062, 41.37992937361946, 45.01306648231458, 181.63318480115637, 48.620481763023065, 274.14679988123663, 336.3768159509841, 166.6065135391024, 141.88562078343145, 135.37813717290916, 148.44950657371055, 67.54978794925779, 13.294253725552432, 10.952938216420588, 7.726276499375658, 7.694118568485601, 7.4171251366859465, 7.415076777853952, 6.621782677286675, 5.643409234710472, 4.034770360557577, 4.034770360557577, 4.034770360557577, 4.034770360557577, 4.034770360557577, 4.034770360557577, 4.0346523594555865, 4.0346523594555865, 4.0346523594555865, 4.0346523594555865, 4.0346523594555865, 4.0346523594555865, 4.0346523594555865, 4.0346523594555865, 4.034524838094838, 4.034396815667838, 4.034396815667838, 45.24109580674999, 28.784915527829142, 28.784915527829142, 81.5014133608881, 15.764116068022835, 13.255819744269013, 9.659907774245992, 24.65435698090843, 24.7019614969565, 151.86102782753173, 112.83798678057927, 20.445607827684178, 68.25298881878396, 29.30068958287137, 21.71264428371999, 12.208072645948524, 16.28655110920249, 76.54174719133177, 42.21066143747785, 56.150085270050795, 344.298617128981, 31.7837989433498, 171.85340541379153, 676.5670577940139, 37.01537848938387, 144.94698719693878, 135.91453329471153, 75.33235536166201, 347.6384985998208, 649.0589753379115, 219.35700487589884, 160.42831828144406, 658.2300864675406, 496.6170757458737, 260.9145981295777, 359.03811591559787, 1243.067959367245, 370.72697236008867, 232.72702243295964, 52.431223518216164, 23.18538468708455, 17.284334276259656, 9.774220784958057, 7.47567829451972, 6.813883682546514, 6.806045836441494, 44.106514164811706, 27.38304593480911, 3.6360022504215537, 3.6359641464082695, 3.6359641464082695, 3.635890179794248, 3.635822190280349, 3.635822190280349, 3.635822190280349, 3.6357076914561177, 3.6357076914561177, 3.635600477222662, 3.6352947111944953, 3.6351990775925276, 3.6351990775925276, 3.6351990775925276, 3.6351990775925276, 3.634638537671619, 20.6906704717791, 88.58764736006964, 15.27783236222818, 8.302725858397043, 166.08507071993338, 209.27759584170133, 22.306825119871775, 143.80835665940765, 116.81760195802225, 100.44917282567093, 50.625200678395494, 64.4674438606486, 31.48288394643442, 92.03204595179773, 58.274468260588996, 170.36803643559594, 113.31744306434965, 137.16737939926998, 123.04967724473418, 75.33235536166201, 270.160560195869, 160.42831828144406, 218.38501186004393, 1073.573993636185, 158.30550968242676, 539.7116195287194, 191.9103609157252, 267.18174936699245, 27.463005366749456, 18.342301704124704, 13.932124804583301, 11.876187982923264, 9.424357377311756, 9.417442489164321, 27.64424851054091, 6.083000982755363, 29.45473829942603, 3.644579440510676, 3.644579440510676, 3.644579440510676, 3.644579440510676, 3.644523590098652, 3.644523590098652, 3.644520797578051, 3.644293827709187, 42.26047979609746, 8.923799878997892, 24.872882269450336, 30.039718657707983, 12.68027070759804, 14.317914592633876, 12.283272691026996, 5.795188810625539, 16.22545874799924, 49.9878408155516, 82.5174559509156, 52.37648899923759, 19.533506071862824, 19.086770746520013, 104.54980898954179, 22.147123890028674, 68.25298881878396, 36.055318451032164, 143.7446202609362, 112.7830326522388, 45.01306648231458, 232.72702243295964, 160.42831828144406, 323.2199888588177, 367.6841127183814, 62.84372281282021, 785.3419768088148, 151.86102782753173, 417.7043783061469, 366.50468598822, 410.2127589304298, 379.2975201826299, 295.1277091372255, 7.241658971615222, 6.106932767921382, 6.106660752635246, 5.367011030095849, 5.214425745895836, 4.815202480497425, 4.038882218968135, 49.90346394618039, 3.0061417939279544, 3.0061417939279544, 3.0061417939279544, 3.0061417939279544, 3.0061417939279544, 3.0060701059264727, 3.0053962080110463, 3.0053962080110463, 3.0053962080110463, 3.0053962080110463, 3.0053962080110463, 3.0053962080110463, 19.758383523929446, 16.0812133690686, 6.079627464035208, 136.0004698011544, 159.21436663360666, 153.78546212956792, 79.00387959987128, 336.3768159509841, 15.914882235145202, 160.42831828144406, 86.2970650397964, 42.124688898150296, 173.82240526929868, 52.999488523070205, 357.051606781078, 396.8473720879155, 363.4550775715986, 170.15171860170778, 466.95210807506714, 360.68723766112674, 294.99766414523344, 496.6170757458737, 440.35790749952906, 53.46109284093798, 24.903554878823158, 19.9190441219183, 15.301737981507479, 8.339253906132647, 7.81197945975452, 7.613398700002402, 7.468905714611645, 6.486223478831862, 5.571785676981137, 5.386571708024894, 5.311533602901558, 108.31821106440275, 11.644128797647863, 3.2421386859080337, 3.2412946775017795, 3.2412947411068522, 3.2412945048005812, 10.955270207271194, 80.66292868686538, 10.071641273121514, 14.317914592633876, 64.4674438606486, 5.482860248997539, 8.201628425279102, 44.66197639352346, 40.50562804917423, 120.30657019873429, 635.3049005850629, 45.80833105558755, 191.9103609157252, 161.17389487526037, 498.96110261804466, 219.35700487589884, 80.27577190788462, 91.98646128434875, 396.71187312275896, 456.7780648359394, 160.42831828144406, 622.6334704011628, 112.83798678057927, 621.7443176898854, 100.4747135741648, 286.45267841713775, 87.35143273429705, 59.41583625279568, 27.299186982384143, 23.82981039024502, 21.825861292415116, 19.14113920782782, 8.934067050383518, 7.42757928127637, 7.243111553156194, 7.072488543979725, 6.735424886529961, 5.779102692299598, 3.3399778733038032, 3.3399778733038032, 3.3399778733038032, 3.3399778733038032, 3.3399778733038032, 3.339669633461706, 11.245136082299618, 2.9500819851520776, 15.887848536034289, 17.671060685329124, 8.611985050971763, 10.550105359873324, 8.135567700563692, 19.49685560018729, 63.7547289848813, 50.822746343457034, 29.52502983285989, 56.8499511442498, 80.66292868686538, 81.9239766259145, 31.925312468634928, 48.14269269733544, 41.95641224186975, 75.44498170326716, 76.37153353098353, 160.42831828144406, 82.58906770065869, 118.7944056409609, 417.7043783061469, 193.83723641133756, 14.319156490717653, 12.585894028124475, 10.093659498349773, 8.44831179828132, 6.958226378174129, 6.749841927988741, 6.527003749443868, 5.55513628309673, 5.159573648058676, 18.275915998264225, 3.5099134096129596, 3.5099002820192546, 3.509854954667406, 3.509854954667406, 3.509854954667406, 3.509854954667406, 12.210729622971838, 3.089818150380688, 15.895058945875773, 15.3187849031455, 13.993898764865072, 18.294556570186273, 10.995915830970919, 11.066509913192622, 20.755363387839864, 17.68502130924196, 47.5466466123068, 53.349383878792246, 69.97302298466319, 51.079462037709604, 191.9103609157252, 110.50575965780439, 260.9145981295777, 44.73513907743011, 75.33235536166201, 101.42360789178834, 286.45267841713775, 96.28188529710488, 177.74049751754103, 100.22596122766221, 144.94698719693878, 496.6170757458737, 77.94272707346346, 10.810555862204671, 9.616897909908458, 8.667671418667759, 6.677003875035402, 3.3961442370624817, 3.3961442370624817, 3.396119187122553, 3.396119187122553, 3.396119187122553, 3.396091773980744, 3.395873886767401, 11.518571715876801, 7.995453226540164, 20.664738584339347, 22.17946513368366, 24.982910040880295, 5.482860248997539, 24.147644486381047, 6.312475197879414, 34.16139133716189, 6.469683780778923, 7.138740175911084, 8.235891001011371, 52.47669067525996, 219.35700487589884, 51.177512374818434, 117.81011254963506, 91.98646128434875, 119.99003644899774, 112.83798678057927, 294.99766414523344, 151.86102782753173, 66.43263829174377, 60.67268408240568, 135.91453329471153, 496.6170757458737, 159.21436663360666, 119.90262888488861, 328.09742060728513, 418.5344716557628, 1243.067959367245, 532.3168847953008, 78.52313473613323, 55.53098022586737, 23.017450316614173, 10.197212427912909, 10.123241585621283, 9.81180554329474, 5.863804767042914, 14.264389617783728, 35.36378535357406, 3.421192662693643, 154.93928854243669, 7.638839240768553, 18.779737826582107, 31.925312468634928, 20.664738584339347, 9.015552958331906, 38.26341793989562, 69.0002267390025, 68.72972627668514, 85.20495938390717, 16.55889582869027, 236.60370344700317, 27.068902910301706, 30.49951750778608, 52.80149877833005, 317.57810382216377, 259.9007061066541, 347.6384985998208, 154.76156689613856, 95.37133482625765, 143.7446202609362, 219.35700487589884, 141.49650254504135, 144.94698719693878, 48.268703678367274, 46.228954868492565, 29.726889141843802, 27.03748735118717, 12.181630697968155, 7.792077336237096, 6.4392473952863885, 5.414935663841953, 13.324624583819718, 3.56616547980595, 3.404815643579194, 16.985496952323576, 9.358194807813215, 10.021469964805755, 25.36941244373018, 23.40149631587019, 8.78298616748178, 13.077422565129021, 12.44105923445294, 6.2018397372819045, 34.55278409452114, 19.960508931693045, 51.079462037709604, 8.820698492770273, 30.561170535035732, 100.39651052650146, 28.004136373423588, 51.20597240863579, 256.3654600282341, 105.1778477535694, 532.3168847953008, 344.298617128981, 417.7043783061469, 347.6384985998208, 69.35867012877256, 274.14679988123663, 68.17021631302181, 26.40458478959221, 22.338957462498335, 10.292761677157241, 10.2843908138679, 10.062052079591584, 6.686024902413172, 6.366901391582714, 4.948287833067285, 18.677995740784812, 4.461509278108628, 3.196956103425508, 3.196820542524324, 2.832879997718505, 2.832879997718505, 17.878088492981668, 40.605144926156946, 94.50887803511895, 7.373435953073025, 15.955260586710764, 296.66789897854585, 11.241586369300252, 25.27613714931143, 8.78298616748178, 39.67914876720401, 30.153918855327202, 43.14950445477252, 76.85430535283699, 105.99397906044513, 154.2384249027414, 290.7526843669832, 114.99745233366593, 106.00582268240848, 269.58038260153296, 1073.573993636185, 246.7004572467765, 10.312225961016043, 75.01075038873321, 115.3137464150444, 17.247896793508282, 135.3670788237703, 12.520728970109168, 76.88511498932941, 87.84055688075247, 147.56318541850706, 1.1545541136539044, 1.1545540942154022, 1.1545538791420271, 1.1545537256632532, 1.1545537632809804, 1.1545539600948702, 1.154553678287545, 1.1545536609024432, 1.1545536609024432, 1.1545536653254878, 1.1545536800594165, 1.1545536875140658, 1.1545537032140034, 1.1545544055471837, 1.1545537562468249, 1.1545540518378783, 532.3168847953008, 96.64491797052011, 253.6053135861913, 259.0487309982645, 640.5201394194421, 1693.9946815563367, 1.1545545281537313, 232.72702243295964, 1.1545567942824666, 260.9145981295777, 312.93835230091895, 539.7116195287194, 50.735006175182015, 331.1054464261669, 53.60983852079516, 17.916604988546677, 17.01709242905425, 8.08746870752174, 14.601116615507395, 4.492443600563272, 2.9515036874956526, 9.057871269151738, 8.823193228512576, 45.542408111056595, 8.611985050971763, 22.306825119871775, 68.72972627668514, 33.610422328926525, 26.94341050893861, 57.2936742609049, 79.90775345942096, 17.325543188450574, 24.65435698090843, 86.2970650397964, 260.9145981295777, 32.312826362924646, 84.0068598774893, 136.0004698011544, 56.82537893273038, 313.23169812987993, 211.33464333734997, 181.76965123751512, 248.78915369805733, 172.473124779119, 366.50468598822, 280.420616511624, 51.28275538738965, 30.13976080790563, 2.5024219319358503, 2.2622937525713605, 12.68027070759804, 147.56318541850706, 60.677147264153305, 7.423667862692913, 16.832317054361212, 26.024436709296367, 7.466042314821662, 17.878088492981668, 14.514600046034108, 19.755452340558254, 50.993140216036345, 1.1545540292422711, 1.1545538907093347, 1.1545536467592934, 1.1545536688745162, 1.1545536688745162, 1.1545549650486013, 1.154554126231651, 1.154553654257971, 1.1545536558095089, 1.1545536558095089, 105.1778477535694, 73.47268800934302, 87.62263539014634, 117.81011254963506, 172.473124779119, 286.45267841713775, 342.9683030532526, 323.2199888588177, 125.96971696726102, 367.6841127183814, 640.5201394194421, 785.3419768088148], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic21\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic22\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic23\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic24\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\", \"Topic25\"], \"logprob\": [25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.1605, -7.1827, -7.2333, -7.2346, -7.3178, -6.1632, -5.3675, -6.525, -5.9188, -7.4216, -4.9987, -6.494, -7.6158, -7.6258, -7.6296, -7.6848, -7.687, -7.6882, -6.7405, -7.6981, -6.2703, -6.7453, -7.7733, -5.9964, -7.0348, -6.1555, -5.0136, -6.0446, -6.5135, -6.7871, -5.5963, -5.5997, -5.6048, -4.776, -5.3431, -4.5821, -5.7107, -5.4284, -5.208, -5.3125, -5.3856, -4.7918, -5.8049, -4.3546, -4.8665, -4.4054, -5.1951, -4.8442, -4.8946, -4.8496, -5.2745, -5.0565, -5.0371, -5.0721, -4.3634, -5.3, -4.7479, -5.1946, -5.264, -5.0602, -4.2242, -4.6858, -5.1985, -4.9813, -5.1201, -5.1862, -6.2063, -6.9424, -6.9761, -6.8965, -5.7252, -2.6377, -2.2601, -5.6496, -5.6152, -5.1483, -3.0577, -3.2975, -5.6092, -8.0725, -8.0725, -5.2493, -1.3776, -5.2125, -2.8804, -2.2607, -7.3271, -4.0369, -6.0335, -6.695, -4.8361, -3.2432, -4.2933, -5.2666, -5.7771, -4.0313, -3.5966, -4.7495, -4.2027, -4.8466, -4.6429, -4.9961, -4.9588, -5.103, -5.32, -5.1056, -5.299, -5.6024, -5.6058, -5.7141, -5.7227, -5.7687, -5.8536, -5.9411, -6.0657, -6.1116, -6.1507, -6.2157, -5.1115, -5.1269, -6.634, -6.6677, -6.672, -4.8155, -5.7418, -6.8098, -6.8693, -4.9562, -6.9608, -5.714, -5.6747, -5.9042, -4.5885, -5.5219, -5.3591, -4.3148, -4.8316, -5.0505, -5.0015, -5.4486, -4.7656, -5.0807, -4.7609, -4.5824, -4.7283, -5.4911, -4.8604, -5.2458, -4.9148, -4.7888, -4.973, -5.1292, -4.9062, -4.8951, -5.0992, -5.0064, -5.1119, -5.1215, -4.8472, -4.9018, -5.1764, -5.3158, -5.3335, -5.386, -5.5062, -5.6486, -5.7684, -5.816, -5.9111, -5.9503, -5.9741, -6.0284, -6.0907, -6.1488, -6.1855, -6.2175, -6.2431, -6.2546, -6.3412, -5.2416, -6.5476, -6.5741, -6.5788, -4.6639, -4.2152, -4.1992, -4.7768, -5.0522, -4.367, -5.2135, -5.1405, -5.2196, -5.645, -5.4132, -5.4624, -4.9743, -4.7749, -4.8854, -5.4497, -4.7693, -5.1101, -5.1558, -5.073, -4.9365, -4.9946, -5.0039, -5.253, -5.1791, -5.1644, -4.6409, -4.6617, -4.9735, -5.3617, -5.4413, -5.5099, -4.4608, -5.5561, -5.6279, -5.8611, -5.8873, -6.0118, -6.1333, -6.2187, -6.2269, -6.2324, -6.287, -6.2928, -6.3093, -6.3211, -4.9301, -6.3826, -5.0695, -6.4138, -6.4492, -5.4198, -4.7543, -5.0397, -5.457, -4.9357, -5.5811, -5.7638, -4.7964, -4.7298, -5.7584, -5.4521, -5.0682, -4.166, -5.2887, -4.3142, -5.1168, -4.8873, -4.7704, -4.3636, -4.8185, -5.0477, -5.112, -4.6642, -4.9926, -4.7353, -4.7807, -4.8983, -4.9001, -4.8052, -4.8993, -4.9073, -3.4805, -3.8283, -3.8912, -4.8826, -3.6282, -4.9076, -4.9613, -5.1848, -5.3691, -5.5557, -3.5318, -5.6584, -5.8147, -5.8983, -6.0459, -6.1535, -6.181, -6.4001, -6.4454, -6.481, -6.5396, -4.7174, -3.8808, -4.5007, -5.7032, -5.1771, -4.6459, -4.3926, -4.2102, -4.207, -4.2906, -4.164, -5.0292, -4.2672, -3.5609, -3.5288, -4.1597, -3.624, -4.1827, -4.4869, -4.0852, -4.2208, -4.4437, -4.8003, -5.3155, -5.3277, -5.5486, -5.55, -5.6506, -5.6784, -5.945, -6.029, -6.0374, -6.0857, -5.0084, -6.1899, -6.2059, -5.1429, -6.4435, -5.4344, -4.5906, -5.7732, -4.7494, -4.291, -5.1772, -5.7792, -7.1155, -7.1155, -7.1155, -4.3161, -6.2259, -5.8744, -4.5788, -4.5742, -4.6513, -4.8281, -4.7575, -5.0435, -4.3427, -5.1477, -5.0242, -4.5934, -5.0339, -4.483, -4.9008, -4.6775, -5.0409, -5.0801, -4.6668, -4.3818, -4.6771, -4.6462, -4.9543, -4.877, -4.8086, -4.7898, -4.958, -4.9178, -4.9773, -4.6272, -4.6967, -4.757, -4.7788, -4.9346, -4.9701, -5.0254, -5.0695, -5.0936, -5.1408, -5.1551, -5.1976, -5.2799, -5.3075, -5.3103, -5.3201, -5.3587, -5.4855, -5.4966, -5.4968, -5.5459, -5.5478, -5.5722, -5.5874, -5.6325, -4.3085, -5.2497, -5.2168, -4.8275, -4.952, -4.9872, -5.3514, -5.4356, -2.5815, -2.9232, -3.5983, -4.8393, -4.8979, -4.9273, -5.0014, -5.4055, -5.434, -5.4631, -5.4774, -5.5707, -5.715, -5.8107, -5.8243, -5.9217, -5.969, -6.2088, -6.2253, -6.2256, -6.3504, -6.703, -4.9521, -5.6586, -5.7766, -4.1363, -3.9777, -3.882, -4.2936, -5.5639, -3.7897, -4.3557, -4.9495, -5.3152, -4.8302, -4.9814, -4.9234, -4.736, -4.8559, -4.8922, -4.7451, -4.7352, -4.8534, -4.8842, -4.7573, -4.7805, -4.8901, -5.0579, -5.137, -5.1457, -5.5119, -5.5263, -5.7282, -5.8126, -5.9352, -5.9399, -5.9519, -5.9638, -6.0364, -6.0373, -6.1698, -6.2754, -4.4546, -6.3309, -5.351, -6.417, -5.5127, -5.105, -6.8539, -6.8539, -6.8539, -6.8539, -4.3273, -6.0244, -4.823, -6.1457, -5.2431, -5.2144, -5.5508, -4.7292, -5.4822, -5.6241, -5.2929, -4.9672, -5.2764, -5.3603, -5.3964, -5.2575, -5.2121, -5.2812, -5.3264, -5.2695, -5.2328, -5.3159, -5.2592, -5.3234, -5.3542, -5.3233, -4.6266, -4.6365, -5.037, -5.0438, -5.2087, -5.2993, -5.5328, -5.5522, -5.5812, -4.2569, -5.8524, -5.8527, -5.8527, -5.8736, -5.903, -6.0019, -6.06, -6.1642, -6.2209, -6.221, -5.261, -6.2733, -6.2991, -4.6321, -5.1945, -4.6722, -3.9507, -5.3779, -5.1003, -4.491, -5.0281, -4.6926, -4.5272, -4.2474, -4.3836, -4.9482, -4.4911, -4.4098, -4.6502, -4.3636, -4.9222, -4.9191, -4.4913, -4.9634, -4.6124, -4.6612, -4.7879, -4.8245, -4.8887, -4.8828, -4.9471, -5.1733, -5.3865, -5.7833, -5.7882, -5.8311, -5.8314, -5.9657, -6.1607, -6.5979, -6.5979, -6.5979, -6.5979, -6.5979, -6.5979, -6.598, -6.598, -6.598, -6.598, -6.598, -6.598, -6.598, -6.598, -6.598, -6.5981, -6.5981, -4.2594, -4.765, -4.765, -3.9483, -5.4225, -5.5777, -5.8557, -5.0996, -5.1481, -3.7606, -4.0547, -5.3579, -4.5793, -5.2467, -5.4651, -5.8671, -5.6805, -4.685, -5.0731, -4.9289, -4.1099, -5.2828, -4.6595, -4.1893, -5.2678, -4.803, -4.8623, -5.0738, -4.6339, -4.6085, -4.8498, -4.9439, -4.7328, -4.7806, -4.8936, -4.8767, -4.8482, -4.9536, -4.9802, -3.4424, -4.286, -4.597, -5.2212, -5.5294, -5.6391, -5.6405, -3.8142, -4.3648, -6.4533, -6.4533, -6.4533, -6.4534, -6.4534, -6.4534, -6.4534, -6.4534, -6.4534, -6.4535, -6.4536, -6.4536, -6.4536, -6.4536, -6.4536, -6.4539, -4.9099, -3.6796, -5.2284, -5.7694, -3.3416, -3.4367, -5.1097, -3.7425, -3.921, -4.1739, -4.6435, -4.5406, -5.0336, -4.3385, -4.6537, -3.9793, -4.243, -4.1527, -4.4047, -4.6675, -4.3033, -4.4899, -4.4536, -4.0745, -4.5632, -4.472, -4.5832, -4.5989, -3.9233, -4.348, -4.6436, -4.8184, -5.0767, -5.0776, -4.0589, -5.5906, -4.1687, -6.2642, -6.2642, -6.2642, -6.2642, -6.2643, -6.2643, -6.2643, -6.2644, -3.8801, -5.4496, -4.4657, -4.5214, -5.3848, -5.2889, -5.452, -6.2642, -5.2662, -4.235, -4.1093, -4.4394, -5.3309, -5.357, -4.4816, -5.316, -4.7587, -5.1074, -4.664, -4.7597, -5.0606, -4.5742, -4.7632, -4.6693, -4.6574, -5.0402, -4.7731, -4.9865, -4.9734, -4.9779, -4.9823, -1.249, -1.5133, -5.3725, -5.5776, -5.5776, -5.7382, -5.7748, -5.8774, -6.1132, -3.6598, -6.5499, -6.5499, -6.5499, -6.5499, -6.5499, -6.5499, -6.5503, -6.5503, -6.5503, -6.5503, -6.5503, -6.5503, -4.6973, -5.1126, -6.5499, -3.9804, -4.4238, -4.6655, -5.0917, -4.4572, -5.8094, -4.8107, -5.0918, -5.4545, -4.9254, -5.4133, -4.6727, -4.7256, -4.8404, -5.08, -4.8278, -4.9516, -5.2375, -5.1818, -5.4785, -3.1519, -3.9406, -4.1757, -4.4575, -5.1325, -5.2082, -5.2383, -5.2608, -5.4288, -5.6155, -5.658, -5.7131, -2.7514, -4.9931, -6.3554, -6.3557, -6.3557, -6.3557, -5.1987, -3.2312, -5.4642, -5.2345, -3.7483, -6.3557, -5.9547, -4.3048, -4.4405, -3.833, -2.9585, -4.6451, -3.9494, -4.1757, -3.7365, -4.3011, -4.6749, -4.6498, -4.4653, -4.4975, -4.7491, -4.5972, -4.8733, -4.7205, -4.8977, -4.8614, -2.564, -2.9555, -3.7559, -3.8981, -3.9904, -4.1292, -4.9644, -5.1784, -5.2081, -5.2363, -5.2945, -5.481, -6.2209, -6.2209, -6.2209, -6.2209, -6.2209, -6.2211, -5.0538, -6.4135, -4.8496, -4.8423, -5.6105, -5.4462, -5.8941, -5.0586, -4.2158, -4.4129, -4.8194, -4.5522, -4.4074, -4.4319, -4.993, -4.7954, -4.865, -4.6568, -4.737, -4.6072, -4.7885, -4.7375, -4.5865, -4.9488, -4.3889, -4.5294, -4.7743, -4.9766, -5.2036, -5.2399, -5.2802, -5.478, -5.5712, -4.324, -6.0945, -6.0946, -6.0946, -6.0946, -6.0946, -6.0946, -4.8869, -6.2871, -4.8551, -5.0131, -5.1231, -4.8596, -5.3971, -5.4222, -4.7985, -5.0312, -4.5347, -4.6269, -4.535, -4.7544, -4.1718, -4.5236, -4.2544, -4.9655, -4.8485, -4.8121, -4.5939, -4.8344, -4.8076, -4.8851, -4.8575, -4.8752, -4.9737, -4.6508, -4.7821, -4.9005, -5.2063, -6.0971, -6.0971, -6.0971, -6.0971, -6.0971, -6.0971, -6.0972, -4.9205, -5.3551, -4.6, -4.5441, -4.5402, -6.0971, -4.6198, -6.0971, -4.4256, -6.0971, -6.0971, -6.0971, -4.2794, -2.899, -4.4497, -3.8386, -4.4022, -4.3138, -4.4628, -4.0437, -4.4421, -4.8513, -4.8894, -4.6421, -4.1628, -4.6017, -4.766, -4.6183, -4.6692, -4.5711, -4.795, -2.4968, -2.8493, -3.7593, -4.6396, -4.6478, -4.683, -5.2881, -4.5348, -3.6459, -6.0104, -2.3623, -5.6075, -4.8236, -4.6054, -5.1387, -6.1304, -4.8364, -4.2902, -4.5849, -4.4466, -6.0869, -3.4476, -5.6492, -5.5783, -5.073, -3.288, -3.5464, -3.6206, -4.3229, -4.7155, -4.5587, -4.7165, -4.9507, -4.986, -2.9821, -3.0264, -3.4817, -3.5804, -4.4315, -4.9366, -5.1629, -5.3765, -4.6056, -5.9381, -6.0062, -4.4932, -5.1935, -5.1396, -4.4049, -4.5268, -5.5692, -5.2481, -5.2994, -6.0062, -4.3105, -4.9085, -3.9864, -6.0062, -4.8582, -3.7175, -4.9912, -4.9212, -4.2832, -4.6445, -4.5281, -4.6744, -4.6795, -4.746, -5.016, -4.9884, -2.367, -3.3422, -3.5175, -4.3559, -4.3568, -4.3814, -4.8552, -4.9142, -5.2292, -3.9057, -5.3651, -5.84, -5.8401, -6.0324, -6.0324, -4.2867, -3.467, -3.0765, -5.84, -5.0978, -2.2763, -5.6382, -4.9735, -6.0324, -4.86, -5.1501, -4.9293, -4.6856, -4.5385, -4.4806, -4.6279, -5.0389, -5.0654, -5.1356, -5.1444, -0.8455, -5.1999, -3.7778, -3.6205, -5.5504, -3.5966, -6.1598, -4.3489, -4.7274, -4.4646, -9.6868, -9.6868, -9.6868, -9.6868, -9.6868, -9.6868, -9.6868, -9.6868, -9.6868, -9.6868, -9.6868, -9.6868, -9.6868, -9.6868, -9.6868, -9.6868, -4.3336, -6.1102, -5.3908, -5.6968, -5.3966, -5.0472, -9.6868, -6.5746, -9.6868, -7.2626, -8.7084, -9.6868, -9.6868, -9.6868, -9.6868, -3.4069, -3.4619, -4.2869, -3.8226, -5.0127, -5.6235, -4.6257, -4.6971, -3.114, -4.9946, -4.2609, -3.2702, -4.033, -4.7651, -4.1257, -3.8597, -5.6235, -5.6235, -4.3831, -3.3201, -5.6235, -4.6772, -4.2582, -5.3069, -3.6877, -4.2156, -4.6147, -4.4668, -4.9751, -4.7934, -4.9995, -1.8814, -2.4288, -5.4796, -5.6718, -4.0943, -2.1873, -3.3507, -5.4796, -4.8539, -4.5392, -6.0262, -5.466, -5.8315, -5.5753, -5.4747, -9.4849, -9.4849, -9.4849, -9.4849, -9.4849, -9.4849, -9.4849, -9.4849, -9.4849, -9.4849, -5.3225, -5.6718, -5.6285, -5.7742, -5.4822, -5.3757, -5.4693, -5.5358, -6.4652, -5.7805, -5.5271, -5.7734], \"loglift\": [25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.6176, 0.6165, 0.6155, 0.6155, 0.6132, 0.6131, 0.613, 0.6115, 0.6114, 0.611, 0.6108, 0.6056, 0.6055, 0.6052, 0.6051, 0.6033, 0.6032, 0.6032, 0.603, 0.6028, 0.6026, 0.6014, 0.6002, 0.6002, 0.5977, 0.5975, 0.5859, 0.5917, 0.5932, 0.5947, 0.5784, 0.5779, 0.5754, 0.5446, 0.5619, 0.5347, 0.568, 0.558, 0.5438, 0.5463, 0.5372, 0.5015, 0.5522, 0.4495, 0.4825, 0.44, 0.5002, 0.4632, 0.4558, 0.4473, 0.5022, 0.471, 0.4547, 0.4605, 0.2941, 0.4993, 0.3382, 0.4641, 0.4822, 0.2686, -0.6208, -0.3377, 0.3766, -0.0062, 0.1458, 0.3194, 2.5, 2.4897, 2.4861, 2.4842, 2.479, 2.447, 2.4221, 2.3957, 2.37, 2.367, 2.3389, 2.3193, 2.3024, 2.2912, 2.2912, 2.2296, 2.2258, 2.1927, 2.1489, 2.0874, 2.0831, 2.0736, 2.0443, 2.0383, 2.0074, 1.7319, 1.8027, 1.7853, 1.7998, 1.2346, 1.061, 1.2801, 0.8835, 0.7901, 0.2025, 0.5743, -0.1547, 0.2258, 0.4173, 2.8772, 2.872, 2.8615, 2.8614, 2.8568, 2.8565, 2.8544, 2.8502, 2.8456, 2.8384, 2.8355, 2.8322, 2.8284, 2.8279, 2.8038, 2.7922, 2.7886, 2.7881, 2.7839, 2.7783, 2.7723, 2.7649, 2.7646, 2.7527, 2.742, 2.7391, 2.7246, 2.6271, 2.6719, 2.6296, 2.4912, 2.5132, 2.4022, 2.3139, 2.5126, 2.1435, 2.3073, 2.0277, 1.8936, 1.9721, 2.5297, 1.7623, 2.1598, 1.6198, 1.1526, 1.4192, 1.732, 1.1297, 0.4552, 1.1985, 0.6523, 1.2198, 1.0664, 3.0444, 3.0431, 3.0357, 3.0311, 3.0304, 3.0285, 3.0237, 3.0172, 3.0111, 3.0084, 3.0027, 3.0002, 2.9987, 2.995, 2.9906, 2.9862, 2.9833, 2.9807, 2.9785, 2.9775, 2.9698, 2.9493, 2.9488, 2.9458, 2.9453, 2.8709, 2.8382, 2.797, 2.784, 2.8155, 2.6441, 2.7904, 2.7698, 2.7388, 2.8442, 2.7578, 2.7723, 2.5669, 2.3445, 2.3753, 2.704, 2.092, 2.3759, 2.3714, 2.2051, 1.8462, 1.8802, 1.8105, 2.0003, 0.6876, 0.3413, 3.4335, 3.4329, 3.4229, 3.4054, 3.401, 3.3968, 3.3957, 3.3939, 3.3891, 3.3711, 3.3688, 3.3573, 3.3446, 3.3349, 3.3339, 3.3333, 3.3266, 3.3258, 3.3237, 3.3222, 3.3175, 3.314, 3.31, 3.3097, 3.3022, 3.271, 3.1524, 3.1675, 3.2061, 3.1203, 3.2107, 3.2024, 2.9461, 2.8697, 3.1935, 3.0692, 2.894, 2.1029, 2.8734, 2.0812, 2.6969, 2.3659, 2.0405, 1.187, 1.9697, 2.3955, 2.4406, 1.2676, 2.0454, 1.2172, 1.2415, 1.5523, 1.1368, 0.0402, 0.9303, 0.4001, 3.5437, 3.5397, 3.5389, 3.5151, 3.5144, 3.5141, 3.512, 3.502, 3.4919, 3.4798, 3.4786, 3.4722, 3.4591, 3.4513, 3.436, 3.4235, 3.4201, 3.3901, 3.3831, 3.3774, 3.3678, 3.3036, 3.2595, 3.2393, 3.2139, 3.1984, 3.1189, 3.0727, 3.0054, 2.9931, 2.9706, 2.937, 3.1454, 2.9154, 2.6088, 2.5919, 2.7893, 2.4464, 2.7089, 2.7813, 2.1465, 1.5302, 1.9104, 2.5574, 3.5014, 3.5007, 3.4866, 3.4865, 3.4791, 3.4769, 3.4529, 3.4441, 3.4432, 3.4377, 3.4357, 3.4252, 3.4001, 3.3992, 3.3895, 3.3218, 3.3043, 3.304, 3.2969, 3.2957, 3.293, 3.2884, 3.2499, 3.2499, 3.2499, 3.2175, 3.2241, 3.2081, 3.0986, 2.9741, 2.983, 2.9905, 2.9185, 3.0135, 2.7275, 3.0389, 2.9699, 2.722, 2.9483, 2.5745, 2.7998, 2.5344, 2.804, 2.809, 1.9077, 1.1239, 1.3887, 1.2533, 1.8876, 1.3069, 0.7189, 0.2964, 1.6723, 0.4311, 1.2088, 3.586, 3.5838, 3.5817, 3.581, 3.5749, 3.5734, 3.571, 3.5689, 3.5678, 3.5654, 3.5647, 3.5625, 3.5579, 3.5563, 3.5561, 3.5555, 3.5531, 3.5448, 3.544, 3.544, 3.5404, 3.5403, 3.5384, 3.5373, 3.5337, 3.277, 3.3248, 3.1888, 2.7161, 2.2962, 2.2735, 3.1065, 2.6422, 3.6472, 3.6454, 3.6397, 3.6112, 3.6089, 3.6076, 3.6043, 3.5817, 3.5797, 3.5777, 3.5766, 3.5696, 3.5575, 3.5485, 3.5472, 3.5371, 3.5319, 3.5018, 3.4995, 3.4994, 3.4808, 3.4166, 3.3757, 3.3496, 3.3282, 3.315, 3.2704, 2.9929, 3.0486, 3.23, 2.3092, 2.5974, 2.8057, 3.0545, 2.6171, 2.6436, 2.5625, 2.2264, 2.0058, 2.0818, 1.3452, 1.2062, 1.3587, 1.3702, 0.7933, 0.7899, 1.0624, 3.7312, 3.7267, 3.7262, 3.7001, 3.6989, 3.68, 3.6711, 3.6568, 3.6562, 3.6548, 3.6533, 3.6438, 3.6437, 3.6249, 3.6083, 3.6005, 3.599, 3.5861, 3.5836, 3.5316, 3.4933, 3.4872, 3.4872, 3.4872, 3.4872, 3.3331, 3.4039, 3.2645, 3.3647, 3.1824, 3.1046, 3.1336, 2.6333, 3.0344, 3.134, 2.6633, 2.0009, 2.4433, 2.3574, 2.3281, 1.9349, 1.6602, 1.8395, 1.8491, 1.3631, 1.0989, 1.6652, 0.7981, 1.4914, 1.4524, -0.0299, 3.9464, 3.9459, 3.924, 3.9236, 3.9118, 3.9045, 3.8828, 3.8808, 3.8777, 3.8735, 3.8446, 3.8446, 3.8446, 3.8417, 3.8375, 3.8228, 3.8135, 3.7957, 3.7854, 3.7854, 3.7765, 3.7754, 3.7703, 3.7331, 3.7263, 3.6431, 3.55, 3.6828, 3.6257, 3.5092, 3.6038, 3.4064, 3.3178, 3.1999, 3.2381, 3.4744, 3.2056, 3.1338, 2.9412, 2.5279, 3.1379, 3.0569, 2.0896, 2.9355, 1.5568, 1.3035, 1.8794, 2.0033, 1.9861, 1.8998, 2.6229, 4.0223, 4.0028, 3.955, 3.9543, 3.948, 3.948, 3.9268, 3.8917, 3.79, 3.79, 3.79, 3.79, 3.79, 3.79, 3.79, 3.79, 3.79, 3.79, 3.79, 3.79, 3.79, 3.79, 3.79, 3.79, 3.79, 3.7115, 3.6581, 3.6581, 3.434, 3.6026, 3.6208, 3.6592, 3.4783, 3.4279, 2.9993, 3.0023, 3.4072, 2.9804, 3.1586, 3.2399, 3.4137, 3.312, 2.76, 2.9671, 2.826, 1.8315, 3.0412, 1.9768, 1.0765, 2.9037, 2.0035, 2.0086, 2.3872, 1.2979, 0.6989, 1.5424, 1.7612, 0.5605, 0.7945, 1.3251, 1.0228, -0.1906, 0.9139, 1.3529, 4.381, 4.3534, 4.3361, 4.282, 4.2418, 4.2248, 4.2246, 4.182, 4.1082, 4.0387, 4.0387, 4.0387, 4.0387, 4.0387, 4.0387, 4.0387, 4.0387, 4.0387, 4.0386, 4.0386, 4.0386, 4.0386, 4.0386, 4.0386, 4.0385, 3.8433, 3.6193, 3.8281, 3.897, 3.3288, 3.0025, 3.5683, 3.0719, 3.1013, 2.9993, 3.2149, 3.0761, 3.2999, 2.9223, 3.0641, 2.6656, 2.8097, 2.709, 2.5656, 2.7935, 1.8805, 2.2151, 1.943, 0.7296, 2.1552, 1.0199, 1.9426, 1.596, 4.5468, 4.5256, 4.5051, 4.49, 4.4629, 4.4628, 4.4046, 4.3868, 4.2313, 4.2254, 4.2254, 4.2254, 4.2254, 4.2254, 4.2254, 4.2254, 4.2254, 4.159, 4.1446, 4.1034, 3.8589, 3.858, 3.8324, 3.8227, 3.7616, 3.7301, 3.6361, 3.2606, 3.3851, 3.4799, 3.4769, 2.6517, 3.3692, 2.801, 3.0904, 2.1509, 2.2977, 2.9154, 1.7588, 1.9419, 1.3353, 1.2183, 2.6021, 0.3436, 1.7734, 0.7747, 0.901, 0.784, 4.5956, 4.5822, 4.4305, 4.3959, 4.3959, 4.3644, 4.3567, 4.3338, 4.2737, 4.213, 4.1323, 4.1323, 4.1323, 4.1323, 4.1323, 4.1323, 4.1322, 4.1322, 4.1322, 4.1322, 4.1322, 4.1322, 4.102, 3.8926, 3.428, 2.8899, 2.2888, 2.0818, 2.3217, 1.5075, 3.2062, 1.8944, 2.2333, 2.5877, 1.6995, 2.3993, 1.2323, 1.0738, 1.0468, 1.5662, 0.8089, 0.9433, 0.8584, 0.3933, 0.2168, 4.6521, 4.6273, 4.6155, 4.5974, 4.5294, 4.519, 4.5147, 4.5113, 4.4844, 4.4497, 4.441, 4.3999, 4.3464, 4.335, 4.2513, 4.2512, 4.2512, 4.2512, 4.1903, 4.1614, 4.009, 3.8869, 3.8684, 3.7255, 3.7239, 3.6789, 3.641, 3.1598, 2.3703, 3.3133, 2.5764, 2.5247, 1.8339, 2.0911, 2.7225, 2.6115, 1.3343, 1.1612, 1.9559, 0.7517, 2.1837, 0.6299, 2.2753, 1.264, 4.7489, 4.7428, 4.7201, 4.7139, 4.7094, 4.7018, 4.6286, 4.5993, 4.5948, 4.5904, 4.581, 4.5476, 4.356, 4.356, 4.356, 4.356, 4.356, 4.356, 4.3092, 4.2876, 4.1677, 4.0686, 4.0192, 3.9806, 3.7925, 3.7541, 3.412, 3.4416, 3.5783, 3.1903, 2.9852, 2.9452, 3.3265, 3.1133, 3.1812, 2.8027, 2.7103, 2.0978, 2.5805, 2.268, 1.1616, 1.5671, 4.7324, 4.7209, 4.6967, 4.6723, 4.6393, 4.6335, 4.6267, 4.5902, 4.5708, 4.5533, 4.4328, 4.4328, 4.4327, 4.4327, 4.4327, 4.4327, 4.3937, 4.3677, 4.1618, 4.0407, 4.0212, 4.0167, 3.9883, 3.9568, 3.9515, 3.879, 3.3865, 3.1791, 2.9998, 3.0951, 2.354, 2.5542, 1.9643, 3.0166, 2.6125, 2.3515, 1.5314, 2.3812, 1.795, 2.2904, 1.949, 0.6999, 2.4532, 4.7516, 4.7373, 4.7228, 4.6779, 4.4632, 4.4632, 4.4632, 4.4632, 4.4632, 4.4632, 4.4631, 4.4184, 4.3489, 4.1545, 4.1396, 4.0245, 3.9842, 3.9789, 3.8433, 3.8262, 3.8187, 3.7203, 3.5773, 3.5431, 3.4932, 3.3979, 3.1753, 2.859, 2.6817, 2.5942, 2.0523, 2.3178, 2.7354, 2.788, 2.2287, 1.4123, 2.1109, 2.2302, 1.3713, 1.0769, 0.0865, 0.7107, 4.9227, 4.9167, 4.8873, 4.8212, 4.8203, 4.8163, 4.726, 4.5903, 4.5713, 4.5425, 4.3775, 4.1421, 4.0265, 3.7141, 3.6158, 3.4536, 3.302, 3.2586, 2.9678, 2.8912, 2.8891, 2.8689, 2.8353, 2.7869, 2.7433, 2.7341, 2.6762, 2.3111, 2.4181, 2.5096, 2.2562, 1.6757, 1.8799, 1.8206, 4.924, 4.9229, 4.9091, 4.9053, 4.8515, 4.7932, 4.7576, 4.7172, 4.5877, 4.5733, 4.5515, 4.4573, 4.3531, 4.3386, 4.1444, 4.1033, 4.0409, 3.9639, 3.9625, 3.9519, 3.9299, 3.8807, 3.8631, 3.5996, 3.505, 3.4563, 3.4593, 2.9259, 1.9531, 2.4827, 0.9775, 1.267, 1.0686, 1.1857, 2.5276, 1.1808, 5.1939, 5.1672, 5.1591, 5.0955, 5.0954, 5.0928, 5.0276, 5.0175, 4.9546, 4.9499, 4.9223, 4.7807, 4.7807, 4.7092, 4.7092, 4.6126, 4.612, 4.1577, 3.945, 3.9153, 3.814, 3.725, 3.5796, 3.5777, 3.2421, 3.2265, 3.0889, 2.7554, 2.581, 2.2638, 1.4825, 1.9991, 2.054, 1.0504, -0.3403, 5.4292, 4.2496, 3.6875, 3.4148, 3.3848, 3.2783, 3.0957, 3.0917, 2.5799, 2.3241, 1.9524, 1.9524, 1.9524, 1.9524, 1.9524, 1.9524, 1.9524, 1.9524, 1.9524, 1.9524, 1.9524, 1.9524, 1.9524, 1.9524, 1.9524, 1.9524, 1.1721, 1.1016, 0.8564, 0.5291, -0.0759, -0.6991, 1.9524, -0.2415, 1.9524, -1.0439, -2.6715, -4.1949, -1.8305, -3.7064, -1.8857, 5.4903, 5.4867, 5.4057, 5.2792, 5.2678, 5.0771, 4.9535, 4.9084, 4.8502, 4.6352, 4.4171, 4.2825, 4.235, 3.7241, 3.609, 3.5423, 3.3072, 2.9544, 2.942, 2.8986, 2.6839, 2.6748, 2.6121, 2.436, 2.3483, 2.2139, 1.9654, 1.7994, 1.6575, 1.0854, 1.1471, 5.9641, 5.9482, 5.3861, 5.2947, 5.1485, 4.6014, 4.3266, 4.2986, 4.1057, 3.9847, 3.7463, 3.4333, 3.2762, 3.2242, 2.3765, 2.1542, 2.1542, 2.1542, 2.1542, 2.1542, 2.1542, 2.1542, 2.1542, 2.1542, 2.1542, 1.8047, 1.8141, 1.6814, 1.2396, 1.1504, 0.7496, 0.476, 0.4688, 0.4816, 0.0952, -0.2065, -0.6566]}, \"token.table\": {\"Topic\": [9, 1, 2, 3, 5, 11, 21, 1, 2, 5, 16, 21, 5, 18, 1, 2, 5, 17, 11, 15, 18, 1, 1, 3, 5, 7, 9, 11, 12, 16, 21, 1, 6, 12, 19, 20, 15, 15, 4, 1, 3, 5, 9, 10, 11, 12, 13, 20, 21, 1, 2, 15, 4, 10, 1, 5, 8, 10, 12, 19, 1, 4, 1, 11, 12, 13, 19, 24, 22, 1, 5, 6, 15, 17, 19, 21, 22, 13, 18, 1, 4, 5, 11, 15, 20, 22, 1, 4, 9, 13, 21, 2, 4, 5, 3, 20, 10, 3, 5, 9, 17, 14, 1, 2, 3, 5, 10, 11, 15, 21, 22, 25, 10, 12, 17, 4, 14, 18, 21, 3, 7, 10, 25, 1, 4, 5, 9, 11, 12, 14, 15, 1, 4, 5, 7, 10, 12, 14, 22, 17, 5, 6, 9, 22, 2, 10, 20, 10, 23, 12, 1, 2, 3, 5, 8, 9, 10, 12, 13, 15, 17, 21, 1, 7, 19, 9, 18, 1, 5, 25, 18, 1, 16, 3, 18, 13, 3, 8, 11, 5, 15, 21, 24, 16, 19, 16, 11, 1, 2, 4, 7, 10, 12, 17, 18, 19, 19, 4, 6, 19, 20, 1, 2, 3, 4, 10, 2, 6, 9, 19, 1, 9, 22, 1, 5, 10, 14, 13, 1, 18, 6, 6, 7, 16, 1, 3, 5, 6, 10, 12, 10, 3, 12, 22, 1, 5, 3, 4, 5, 6, 8, 9, 10, 12, 14, 16, 18, 19, 9, 5, 3, 21, 22, 21, 17, 1, 5, 9, 13, 18, 19, 21, 8, 20, 3, 19, 1, 10, 11, 12, 15, 18, 24, 11, 22, 1, 3, 4, 6, 7, 11, 12, 13, 14, 3, 8, 19, 8, 6, 1, 4, 6, 9, 20, 22, 19, 1, 3, 9, 20, 9, 1, 4, 6, 7, 8, 12, 13, 14, 19, 7, 21, 22, 1, 3, 10, 11, 14, 16, 18, 19, 20, 21, 11, 4, 15, 16, 18, 13, 3, 17, 4, 4, 19, 17, 9, 10, 13, 8, 1, 12, 3, 4, 16, 5, 7, 6, 21, 17, 6, 17, 18, 21, 6, 10, 1, 11, 20, 11, 9, 9, 21, 1, 3, 4, 5, 7, 9, 10, 11, 12, 14, 17, 18, 19, 20, 21, 24, 3, 7, 9, 10, 11, 12, 14, 16, 17, 18, 19, 24, 4, 8, 10, 11, 14, 12, 17, 24, 10, 11, 13, 8, 12, 1, 3, 6, 7, 10, 12, 18, 3, 11, 15, 6, 9, 9, 10, 14, 3, 4, 5, 6, 10, 11, 13, 9, 5, 9, 3, 6, 11, 23, 11, 13, 1, 3, 13, 3, 10, 8, 11, 8, 1, 12, 5, 7, 8, 13, 1, 4, 5, 10, 11, 2, 7, 3, 13, 3, 1, 3, 4, 6, 7, 8, 14, 18, 21, 1, 11, 14, 5, 8, 12, 4, 25, 22, 2, 4, 12, 3, 5, 10, 13, 16, 20, 4, 5, 8, 10, 14, 18, 21, 25, 1, 4, 3, 4, 5, 7, 10, 12, 14, 16, 17, 18, 19, 20, 24, 16, 5, 17, 3, 3, 4, 19, 12, 4, 9, 9, 16, 1, 3, 9, 19, 10, 17, 18, 20, 1, 10, 17, 3, 4, 12, 14, 21, 1, 4, 25, 7, 16, 1, 3, 7, 18, 1, 10, 1, 1, 3, 10, 12, 14, 17, 18, 19, 25, 24, 8, 1, 4, 16, 1, 5, 7, 9, 12, 14, 19, 3, 10, 11, 5, 6, 8, 4, 1, 11, 3, 7, 9, 13, 14, 20, 21, 23, 25, 14, 7, 8, 8, 10, 18, 23, 1, 3, 4, 5, 7, 8, 9, 12, 17, 12, 7, 22, 1, 3, 5, 10, 1, 5, 8, 11, 1, 4, 5, 8, 9, 18, 1, 3, 8, 14, 11, 12, 24, 16, 4, 20, 1, 25, 3, 18, 4, 9, 11, 21, 5, 18, 15, 4, 5, 8, 24, 2, 6, 12, 1, 3, 5, 18, 24, 11, 1, 4, 14, 20, 12, 5, 2, 13, 5, 10, 4, 7, 10, 13, 4, 1, 4, 5, 6, 8, 14, 20, 1, 3, 4, 7, 8, 10, 14, 22, 23, 1, 6, 7, 12, 15, 1, 4, 6, 8, 8, 1, 3, 12, 24, 4, 18, 3, 5, 9, 10, 12, 13, 18, 21, 8, 15, 6, 16, 10, 16, 3, 19, 5, 12, 9, 5, 7, 13, 1, 6, 7, 8, 10, 13, 17, 18, 7, 7, 17, 11, 12, 10, 14, 18, 3, 4, 19, 17, 5, 8, 11, 21, 9, 4, 24, 8, 1, 5, 6, 21, 24, 18, 7, 1, 2, 3, 4, 5, 7, 9, 10, 12, 14, 15, 21, 1, 5, 10, 12, 1, 4, 5, 6, 10, 18, 19, 19, 8, 10, 15, 8, 3, 4, 14, 16, 19, 6, 7, 14, 1, 3, 5, 11, 18, 8, 5, 14, 1, 2, 3, 5, 6, 8, 9, 10, 11, 14, 18, 21, 22, 24, 1, 9, 11, 17, 22, 1, 2, 3, 4, 10, 11, 12, 15, 16, 18, 19, 1, 3, 5, 8, 9, 11, 13, 14, 17, 19, 11, 3, 10, 20, 24, 1, 4, 5, 8, 10, 1, 10, 12, 14, 20, 3, 23, 1, 4, 5, 7, 10, 11, 19, 1, 3, 4, 10, 23, 24, 25, 4, 21, 6, 7, 18, 1, 2, 4, 6, 8, 10, 17, 22, 1, 2, 4, 5, 6, 7, 8, 9, 17, 21, 8, 1, 3, 5, 7, 8, 13, 14, 10, 12, 11, 21, 14, 8, 12, 17, 3, 6, 1, 10, 12, 13, 15, 16, 17, 9, 21, 1, 5, 8, 17, 18, 23, 15, 4, 10, 12, 1, 4, 8, 12, 10, 16, 5, 8, 1, 11, 16, 18, 22, 3, 20, 1, 9, 10, 13, 6, 4, 8, 9, 12, 1, 4, 5, 7, 11, 12, 14, 8, 4, 7, 12, 1, 3, 5, 11, 16, 1, 3, 8, 12, 19, 1, 3, 9, 10, 13, 14, 18, 1, 5, 7, 9, 10, 17, 19, 20, 24, 25, 4, 8, 13, 1, 4, 5, 6, 8, 10, 14, 16, 19, 1, 6, 14, 16, 5, 3, 4, 5, 7, 9, 12, 14, 17, 18, 3, 5, 9, 4, 9, 13, 4, 11, 17, 1, 3, 4, 10, 11, 4, 7, 8, 14, 6, 6, 9, 16, 19, 1, 3, 7, 8, 11, 15, 19, 21, 4, 7, 18, 14, 1, 9, 10, 13, 17, 14, 23, 13, 17, 4, 10, 25, 8, 14, 17, 1, 7, 10, 15, 16, 1, 2, 7, 19, 3, 12, 1, 2, 8, 18, 3, 16, 5, 6, 12, 7, 21, 1, 2, 3, 5, 6, 7, 9, 10, 11, 12, 13, 14, 16, 18, 19, 20, 24, 1, 3, 5, 7, 8, 9, 13, 14, 15, 17, 19, 24, 1, 24, 4, 16, 1, 5, 7, 8, 10, 15, 17, 1, 5, 10, 12, 14, 16, 1, 1, 2, 3, 4, 5, 7, 9, 10, 13, 17, 19, 21, 22, 9, 16, 22, 12, 1, 3, 4, 8, 9, 10, 11, 16, 4, 25, 1, 10, 25, 1, 3, 4, 7, 9, 12, 13, 14, 15, 16, 19, 21, 5, 10, 11, 1, 2, 3, 4, 5, 6, 9, 12, 19, 1, 3, 6, 10, 12, 15, 19, 22, 2, 3, 12, 19, 4, 1, 4, 5, 7, 9, 14, 16, 19, 8, 1, 2, 4, 6, 10, 11, 3, 7, 19, 5, 1, 13, 1, 3, 14, 21, 9, 3, 4, 8, 18, 10, 15, 14, 1, 3, 6, 3, 7, 3, 1, 3, 8, 10, 11, 12, 13, 16, 20, 1, 2, 4, 21, 5, 18, 4, 3, 4, 19, 24, 13, 1, 3, 7, 8, 24, 6, 1, 3, 5, 23, 7, 3, 4, 6, 16, 5, 10, 1, 3, 4, 7, 15, 17, 24, 11, 17, 13, 13, 17, 17, 25, 1, 2, 4, 5, 6, 7, 8, 12, 13, 14, 17, 19, 20, 24, 1, 3, 4, 10, 18, 16, 1, 6, 18, 9, 4, 8, 9, 7, 3, 6, 8, 20, 22, 10, 1, 8, 17, 5, 9, 5, 9, 5, 6, 2, 11, 17, 1, 22, 1, 4, 5, 8, 9, 18, 4, 10, 1, 3, 4, 5, 9, 10, 12, 13, 14, 16, 17, 18, 20, 25, 1, 8, 24, 5, 16, 7, 15, 17, 19, 14, 11, 5, 5, 10, 1, 3, 6, 12, 18, 1, 8, 10, 9, 22, 8, 5, 13, 14, 1, 4, 8, 12, 15, 1, 3, 5, 7, 12, 15, 19, 24, 3, 4, 5, 10, 12, 14, 4, 7, 8, 1, 6, 7, 11, 12, 18, 22, 5, 6, 1, 4, 5, 7, 11, 19, 12, 4, 1, 3, 8, 9, 1, 3, 4, 11, 14, 18, 1, 8, 15, 20, 21, 22, 1, 4, 6, 12, 13, 21, 5, 7, 12, 5, 10, 1, 5, 19, 20, 11, 1, 2, 3, 4, 7, 9, 10, 11, 12, 17, 18, 19, 1, 4, 5, 7, 8, 14, 22, 1, 3, 4, 6, 7, 9, 10, 11, 12, 13, 17, 18, 19, 22, 24, 1, 3, 5, 6, 8, 1, 8, 3, 19, 1, 1, 3, 4, 5, 7, 19, 24, 5, 1, 2, 3, 4, 9, 13, 14, 15, 16, 19, 11, 13, 24, 1, 8, 10, 11, 23, 5, 1, 2, 3, 4, 10, 12, 14, 15, 18, 19, 22, 1, 2, 13, 11, 15, 15, 1, 4, 1, 9, 10, 14, 15, 1, 3, 4, 5, 8, 10, 11, 12, 14, 17, 21, 25, 1, 4, 5, 6, 8, 10, 11, 12, 17, 18, 20, 1, 2, 3, 4, 7, 7, 18, 1, 3, 4, 5, 7, 9, 10, 19, 8, 9, 1, 8, 21, 25, 6, 7, 13, 9, 14, 25, 3, 5, 22, 3, 12, 14, 24, 1, 3, 8, 12, 1, 8, 11, 13, 25, 17, 3, 4, 9, 10, 4, 8, 4, 7, 3, 19, 20, 1, 9, 10, 12, 13, 14, 15, 8, 1, 3, 5, 7, 17, 18, 20, 22, 3, 1, 4, 5, 7, 9, 10, 12, 1, 6, 9, 18, 21, 1, 3, 4, 8, 10, 11, 12, 14, 16, 18, 20, 5, 16, 21, 2, 8, 4, 24, 1, 5, 7, 8, 10, 11, 17, 3, 19, 1, 10, 3, 4, 7, 18, 10, 21, 1, 4, 5, 7, 9, 10, 11, 12, 13, 14, 17, 19, 1, 3, 4, 6, 8, 12, 13, 14, 24, 1, 3, 4, 5, 7, 8, 10, 11, 12, 14, 16, 17, 18, 20, 21, 22, 5, 1, 2, 3, 4, 7, 15, 17, 19, 21, 12, 7, 11, 10, 18, 3, 7, 15, 4, 5, 6, 11, 13, 1, 8, 9, 10, 13, 13, 3, 10, 12, 1, 7, 12, 6, 3, 10, 1, 5, 2, 10, 1, 5, 7, 8, 12, 14, 17, 1, 3, 5, 22, 1, 9, 17, 1, 2, 4, 5, 7, 8, 10, 16, 18, 21, 24, 7, 13, 1, 8, 1, 2, 11, 20, 24, 8, 4, 7, 9, 17, 1, 3, 5, 7, 8, 10, 19, 22, 5, 5, 1, 3, 7, 14, 3, 10, 11, 1, 4, 7, 14, 19, 21, 23, 13, 14, 10, 3, 4, 9, 10, 13, 11, 18, 10, 19, 1, 12, 3, 7, 6, 7, 4, 24, 3, 12, 16, 19, 4, 18, 12, 1, 4, 7, 8, 10, 12, 18, 3, 6, 11, 7, 20, 10, 14, 6, 7, 7, 8, 19, 8, 7, 17, 1, 2, 4, 5, 6, 7, 12, 14, 23, 1, 2, 3, 5, 7, 8, 5, 6, 7, 14, 17, 1, 6, 18, 3, 21, 4, 11, 22, 1, 4, 5, 7, 8, 10, 11, 16, 21, 3, 21, 19, 7, 12, 19, 4, 1, 4, 8, 9, 10, 9, 8, 3, 4, 7, 19, 1, 3, 5, 7, 10, 12, 14, 17, 21, 25, 1, 4, 5, 7, 10, 12, 13, 14, 20, 23, 15, 15, 11, 1, 4, 5, 1, 3, 4, 5, 8, 10, 14, 20, 1, 3, 10, 11, 12, 18, 1, 7, 11, 21, 2, 8, 6, 7, 13, 16, 12, 8, 1, 4, 6, 16, 18, 8, 9, 18, 1, 7, 11, 22, 12, 6, 11, 1, 3, 4, 5, 10, 11, 12, 14, 17, 19, 1, 8, 11, 1, 3, 8, 22, 25, 21, 16, 8, 22, 4, 5, 10, 9, 10, 21, 1, 8, 10, 1, 8, 1, 3, 4, 7, 8, 20, 8, 1, 2, 3, 4, 5, 14, 18, 1, 2, 3, 4, 6, 10, 13, 14, 15, 16, 18, 19, 20, 1, 3, 12, 16, 24, 6, 7, 10, 11, 14, 16, 5, 1, 15, 17, 20, 1, 3, 4, 5, 6, 10, 12, 14, 16, 19, 20, 10, 15, 17, 8, 6, 7, 11, 12, 1, 5, 13, 2, 5, 10, 13, 14, 18, 20, 21, 7, 1, 3, 4, 5, 9, 10, 13, 17, 22, 3, 4, 5, 10, 13, 15, 18, 4, 12, 15, 11, 5, 10, 1, 1, 7, 14, 15, 7, 6, 21, 17, 9, 3, 5, 9, 8, 9, 21, 8, 10, 14, 9, 14, 2, 11, 14, 17, 21, 25, 1, 2, 3, 4, 5, 7, 8, 10, 12, 18, 19, 21, 1, 10, 3, 7, 1, 11, 1, 7, 8, 12, 13, 18, 19, 1, 7, 17, 1, 4, 6, 7, 8, 10, 11, 13, 14, 15, 18, 22, 13, 4, 19, 7, 20, 13, 4, 12, 5, 15, 8, 15, 1, 4, 6, 7, 8, 12, 18, 19, 8, 22, 1, 4, 6, 7, 8, 12, 14, 15, 17, 19, 4, 8, 16, 3, 9, 22, 1, 3, 4, 6, 11, 17, 19, 5, 11, 3, 7, 19, 1, 15, 1, 3, 5, 10, 12, 13, 14, 15, 16, 17, 19, 21, 7, 1, 4, 5, 9, 18, 18, 1, 18, 17, 4, 10, 14, 16, 3, 6, 14, 1, 2, 1, 3, 9, 18, 1, 3, 8, 9, 17, 1, 3, 18, 24, 5, 7, 8, 14, 20, 3, 12, 17, 8, 10, 17, 19, 1, 4, 10, 11, 14, 1, 11, 12, 4, 5, 6, 7, 24, 18, 15, 11, 17, 25, 3, 4, 1, 3, 4, 5, 7, 8, 11, 12, 14, 15, 16, 18, 1, 11, 22, 1, 10, 15, 1, 4, 5, 7, 12, 3, 1, 3, 22, 9, 10, 8, 24, 9, 10, 21, 5, 6, 8, 18, 15, 17, 3, 4, 9, 9, 5, 10, 11, 12, 15, 16, 13, 3, 20, 8, 20, 9, 3, 5, 8, 10, 12, 17, 16, 20, 14, 7, 17, 1, 5, 10, 21, 4, 1, 3, 10, 11, 23, 24, 1, 3, 4, 5, 8, 12, 18, 24, 11, 6, 22, 6, 19, 8, 4, 21, 1, 3, 9, 12, 1, 2, 3, 5, 8, 9, 12, 13, 14, 24, 12, 10, 14, 12, 7, 16, 1, 19, 25, 6, 10, 4, 15, 1, 3, 7, 13, 16, 18, 19, 12, 16, 17, 1, 3, 4, 6, 10, 3, 5, 2, 5, 7, 16, 14, 16, 15, 12, 14, 16, 10, 1, 2, 3, 6, 8, 10, 12, 14, 18, 13, 8, 18, 8, 10, 25, 16, 24, 1, 4, 6, 11, 7, 9, 10, 14, 20, 21, 11, 1, 8, 14, 1, 11, 1, 3, 4, 5, 10, 11, 12, 18, 20, 1, 3, 4, 8, 12, 14, 15, 18, 19, 10, 19, 20, 4, 1, 5, 10, 4, 8, 21, 2, 11, 10, 1, 3, 4, 5, 9, 10, 24, 13, 1, 2, 3, 4, 5, 7, 10, 12, 14, 16, 17, 22, 13, 22, 22, 8, 11, 19, 11, 12, 3, 1, 10, 1, 3, 10, 11, 14, 22, 1, 9, 21, 4, 14, 8, 10, 3, 5, 7, 9, 10, 18, 21, 24, 1, 3, 8, 10, 16, 9, 12, 4, 6, 17, 1, 3, 5, 7, 12, 8, 3, 8, 14, 17, 15, 3, 9, 18, 8, 19, 6, 11, 1, 4, 5, 10, 14, 14, 3, 4, 7, 10, 12, 14, 16, 19, 1, 3, 10, 14, 17, 21, 10, 5, 15, 12, 6, 1, 6, 7, 9, 19, 15, 19, 5, 5, 1, 3, 4, 5, 9, 10, 16, 17, 25, 1, 4, 10, 17, 6, 5, 10, 3, 5, 7, 8, 14, 21, 3, 4, 6, 1, 7, 1, 16, 20, 4, 16, 4, 8, 10, 11, 1, 3, 4, 6, 10, 12, 14, 17, 1, 6, 12, 13, 14, 1, 3, 1, 2, 3, 4, 11, 17, 19, 20, 21, 4, 8, 16, 17, 12, 1, 3, 13, 3, 4, 10, 11, 16, 22, 1, 3, 4, 7, 8, 9, 17, 19, 21, 23, 19, 11, 7, 10, 14, 19, 1, 4, 5, 10, 14, 17, 19, 21, 4, 5, 10, 14, 12, 13, 3, 4, 5, 7, 2, 3, 7, 8, 9, 10, 21, 1, 6, 7, 10, 13, 23, 24, 5, 15, 1, 3, 6, 10, 11, 12, 19, 21, 22, 1, 3, 4, 5, 6, 9, 11, 14, 16, 19, 21, 10, 13, 6, 7, 1, 5, 6, 10, 12, 14, 18, 20, 1, 14, 18, 1, 4, 7, 10, 11, 12, 16, 18, 19, 1, 4, 3, 10, 21, 13, 1, 3, 4, 10, 12, 20, 2, 8, 9, 1, 5, 14, 1, 12, 14, 17, 20, 21, 11, 13, 1, 5], \"Freq\": [0.8812421680896172, 0.3880542788996943, 0.17323851736593496, 0.10394311041956097, 0.24253392431230894, 0.05543632555709919, 0.027718162778549594, 0.22675638317019084, 0.7558546105673027, 0.00209959614046473, 0.00209959614046473, 0.01259757684278838, 0.9579788526881294, 0.5698146269142694, 0.1883267778856763, 0.665421281862723, 0.11299606673140579, 0.012555118525711754, 0.8348768052714847, 0.12844258542638226, 0.7752578551727868, 0.9745068438536467, 0.6608264242555763, 0.08223617724069393, 0.1057322278808922, 0.017622037980148702, 0.07636216458064438, 0.008811018990074351, 0.029370063300247835, 0.008811018990074351, 0.008811018990074351, 0.7357564903170736, 0.036787824515853676, 0.1250786033539025, 0.07357564903170735, 0.029430259612682943, 0.6653046120578078, 0.6653046120578078, 0.9362950200098862, 0.5666806202231756, 0.08341998977904615, 0.11218550349595861, 0.00862965411507374, 0.00862965411507374, 0.02301241097352997, 0.06040757880551617, 0.0373951678319862, 0.07191378429228115, 0.02301241097352997, 0.9788087190786411, 0.019192327825071392, 0.6653046120578078, 0.9520271669411279, 0.7095397113875328, 0.6951650395288146, 0.25153998140845263, 0.004573454207426412, 0.013720362622279235, 0.02286727103713206, 0.013720362622279235, 0.9140113335911838, 0.055959877566807176, 0.8487250436885564, 0.03566071612136791, 0.03566071612136791, 0.049925002569915077, 0.014264286448547166, 0.010698214836410374, 0.7059953127597092, 0.5652299131932248, 0.24348365491400453, 0.008695844818357305, 0.01739168963671461, 0.04347922409178652, 0.04347922409178652, 0.03478337927342922, 0.04347922409178652, 0.825081997584623, 0.8622887031701357, 0.3962489225548244, 0.15095197049707595, 0.24529695205774843, 0.03773799262426899, 0.028303494468201743, 0.056606988936403486, 0.07547598524853798, 0.27091813266527565, 0.14674732186035763, 0.07901778869403872, 0.4515302211087927, 0.033864766583159456, 0.689188804994514, 0.054054023921138346, 0.24324310764512255, 0.2827751582591637, 0.7069378956479092, 0.7095397113875328, 0.10554098779743422, 0.598065597518794, 0.07036065853162282, 0.21108197559486844, 0.9268193422081343, 0.3327696919792905, 0.08556934936610328, 0.30424657552392276, 0.05704623291073552, 0.04753852742561293, 0.02852311645536776, 0.009507705485122586, 0.08556934936610328, 0.019015410970245172, 0.019015410970245172, 0.7450596976788658, 0.12417661627981097, 0.08278441085320731, 0.39154680182878443, 0.07830936036575689, 0.176196060822953, 0.33281478155446675, 0.3460940559479191, 0.3790553946096257, 0.06592267732341316, 0.19776803197023948, 0.7759155435909477, 0.04756570382166729, 0.02080999542197944, 0.005945712977708411, 0.06837569924364673, 0.008918569466562618, 0.023782851910833645, 0.04459284733281309, 0.5842247933628147, 0.1503945012617147, 0.028922019473406668, 0.1503945012617147, 0.023137615578725335, 0.034706423368088005, 0.023137615578725335, 0.9828339064137857, 0.5988063621576216, 0.9437847015519772, 0.8178997826241814, 0.4186661574762407, 0.5418032626163115, 0.7257205366012484, 0.3927298252316888, 0.3927298252316888, 0.5818336431612513, 0.29091682158062565, 0.7436055839498257, 0.7093512585755525, 0.023579543221348004, 0.04126420063735901, 0.10217802062584136, 0.001964961935112334, 0.05698389611825768, 0.017684657416011003, 0.009824809675561668, 0.013754733545786336, 0.007859847740449335, 0.007859847740449335, 0.005894885805337001, 0.3331819696523863, 0.5504745585561165, 0.10140320815507409, 0.9952592316862561, 0.5698241169027226, 0.7654893725825926, 0.22354999376305804, 0.9749866133810533, 0.5698241169027226, 0.7730251128128198, 0.21611454766810015, 0.4569553031952715, 0.4569553031952715, 0.8251228588735462, 0.1954449166553561, 0.7066085448309029, 0.07517112179052159, 0.20038689119426148, 0.681315430060489, 0.10019344559713074, 0.6776207017708311, 0.3647731127864641, 0.3647731127864641, 0.617037420400356, 0.9071837082726232, 0.7046605604945712, 0.07241570977414565, 0.019496537246885368, 0.10026790584112474, 0.025066976460281186, 0.044563513707166554, 0.00835565882009373, 0.013926098033489548, 0.00835565882009373, 0.588907482276719, 0.6518183640605608, 0.26242038033606996, 0.04232586779614031, 0.03386069423691225, 0.44598949407938415, 0.13785129816999148, 0.040544499461762196, 0.20272249730881098, 0.17028689773940123, 0.7847973456216684, 0.17938225042780992, 0.9406698068516909, 0.9229699204760139, 0.6257314038982824, 0.2085771346327608, 0.11587618590708934, 0.2896854816188287, 0.5488777546462017, 0.09147962577436695, 0.060986417182911305, 0.9488736243507372, 0.7872630749667585, 0.18744358927779967, 0.9863020098096296, 0.3620827485927564, 0.5266658161349184, 0.08229153377108099, 0.9037763000920063, 0.028510293378296726, 0.005702058675659346, 0.022808234702637382, 0.01995720536480771, 0.017106176026978035, 0.7095397113875328, 0.7295900776795161, 0.09948955604720675, 0.13265274139627567, 0.897940434382401, 0.09906104436247129, 0.1995490964119587, 0.08979709338538142, 0.15963927712956696, 0.029932364461793804, 0.049887274102989675, 0.09977454820597935, 0.13968436748837107, 0.049887274102989675, 0.029932364461793804, 0.029932364461793804, 0.07981963856478348, 0.029932364461793804, 0.8938526756182087, 0.9180264850992017, 0.22771298529477604, 0.4554259705895521, 0.22771298529477604, 0.9737158120752294, 0.6779472604714406, 0.842547197957991, 0.03120545177622189, 0.007801362944055473, 0.0429074961923051, 0.011702044416083208, 0.011702044416083208, 0.05070885913636057, 0.6655165831459043, 0.2218388610486348, 0.9209661112465674, 0.04283563308123569, 0.38715095989141574, 0.07039108361662104, 0.0879888545207763, 0.21117325084986313, 0.05279331271246578, 0.12318439632908683, 0.05279331271246578, 0.5424879290275684, 0.2712439645137842, 0.22879840936503434, 0.11439920468251717, 0.06021010772764062, 0.06623111850040468, 0.06623111850040468, 0.03010505386382031, 0.07225212927316875, 0.34319761404755156, 0.018063032318292187, 0.8010522157163911, 0.1657349411827016, 0.8986066373921623, 0.9563688656113398, 0.9910555190445568, 0.583143645118513, 0.03707849766649504, 0.013483090060543653, 0.1011231754540774, 0.013483090060543653, 0.24606639360492166, 0.5889500219055069, 0.8118485061499333, 0.07310484178601295, 0.007695246503790837, 0.1038858278011763, 0.8936200306738381, 0.2851016516175304, 0.2851016516175304, 0.09735178347915673, 0.03476849409969883, 0.0208610964598193, 0.013907397639879533, 0.2642405551577111, 0.831999466045531, 0.14469555931226624, 0.39914304129509326, 0.49892880161886655, 0.6255950770975613, 0.8168305605815216, 0.05486175406890817, 0.009143625678151362, 0.027430877034454085, 0.0030478752260504538, 0.01523937613025227, 0.01523937613025227, 0.03047875226050454, 0.01523937613025227, 0.009143625678151362, 0.9370561751866266, 0.43529053681136687, 0.4974748992129907, 0.8960597036976804, 0.7660482806411784, 0.9256937377088504, 0.8857403487990119, 0.9761707258184209, 0.9631863602705673, 0.9049549549796208, 0.9358527130382805, 0.848357683818003, 0.8968775671362358, 0.7095397113875328, 0.8252425837063019, 0.9109132549705011, 0.9733801632540104, 0.020932906736645388, 0.9771041975715274, 0.720392977424895, 0.2619610826999618, 0.7255288952720912, 0.2487527640932884, 0.2251470561987122, 0.6754411685961366, 0.9845287857108447, 0.9163106901695732, 0.4739289162947312, 0.1895715665178925, 0.1895715665178925, 0.9654334338885514, 0.028963003016656542, 0.06557480784702345, 0.7868976941642815, 0.1311496156940469, 0.8461063380714094, 0.9877859338480203, 0.5668485329249333, 0.22673941316997334, 0.2875410910027346, 0.17426732788044522, 0.052280198364133564, 0.06389802022282991, 0.04937574289945948, 0.08713366394022261, 0.03775792104076313, 0.04066237650543721, 0.10165594126359304, 0.023235643717392694, 0.008713366394022261, 0.008713366394022261, 0.020331188252718606, 0.008713366394022261, 0.026140099182066782, 0.0058089109293481735, 0.17724527502330653, 0.07976037376048795, 0.035449055004661306, 0.07089811000932261, 0.035449055004661306, 0.32790375879311706, 0.02658679125349598, 0.07976037376048795, 0.017724527502330653, 0.02658679125349598, 0.10634716501398392, 0.017724527502330653, 0.19994194360288822, 0.08886308604572811, 0.11107885755716013, 0.39988388720577644, 0.17772617209145622, 0.9129970243973486, 0.464468990171859, 0.34835174262889423, 0.10955684065030998, 0.10955684065030998, 0.7303789376687332, 0.20704131413471086, 0.6211239424041326, 0.9106243981474195, 0.007208108692459785, 0.016818920282406166, 0.012013514487432975, 0.016818920282406166, 0.021624326077379356, 0.012013514487432975, 0.43983988675340235, 0.25133707814480133, 0.25133707814480133, 0.2760809763339615, 0.7099225105730439, 0.204776346104186, 0.409552692208372, 0.30716451915627896, 0.09261152239498797, 0.11113382687398557, 0.10187267463448678, 0.5371468298909302, 0.027783456718496392, 0.027783456718496392, 0.09261152239498797, 0.9443657312168807, 0.2210737346850755, 0.7579670903488303, 0.1466456466972263, 0.6132454316429463, 0.05332568970808228, 0.17330849155126743, 0.38664769278074124, 0.5799715391711119, 0.682225149438301, 0.20214078501875585, 0.10738729204121404, 0.46961355250162806, 0.5009211226684033, 0.07777969653901876, 0.8815032274422125, 0.9348880802521765, 0.4095650595312719, 0.4914780714375263, 0.8927569388788601, 0.7647667010229376, 0.10925238586041966, 0.10925238586041966, 0.2480443852055511, 0.34510523159033196, 0.3343206931031341, 0.010784538487197874, 0.05392269243598937, 0.14542572734363682, 0.7756038791660631, 0.8535517194029774, 0.1280327579104466, 0.8820395412690577, 0.6132536001776573, 0.07876651745401103, 0.03375707890886187, 0.061887977999580096, 0.08439269727215468, 0.028130899090718225, 0.028130899090718225, 0.050635618363292806, 0.02250471927257458, 0.5898240951513587, 0.34784497919182694, 0.04537108424241221, 0.34608827030418765, 0.2831631302488808, 0.34608827030418765, 0.9287253778105129, 0.9621841455488036, 0.8944497532719298, 0.19428320126550627, 0.7947949142679802, 0.743536739866757, 0.13960534901079993, 0.046535116336933316, 0.046535116336933316, 0.26369899257595547, 0.4498394579236887, 0.031023410891288877, 0.5035884898521061, 0.204157495885989, 0.0816629983543956, 0.068052498628663, 0.0408314991771978, 0.027220999451465198, 0.068052498628663, 0.013610499725732599, 0.9793992733074324, 0.9647690310029905, 0.11852942296981267, 0.03950980765660422, 0.03950980765660422, 0.11852942296981267, 0.07243464737044107, 0.3292483971383685, 0.059264711484906335, 0.05267974354213897, 0.013169935885534742, 0.026339871771069483, 0.07901961531320845, 0.026339871771069483, 0.013169935885534742, 0.9726699780476776, 0.266782009398903, 0.622491355264107, 0.9051204203730101, 0.5666551314102024, 0.17585848905833867, 0.23447798541111825, 0.743581988062972, 0.7481080941307855, 0.23470057855083465, 0.8522795881950194, 0.9538610328742124, 0.6167178724998055, 0.20835063260128564, 0.05833817712835997, 0.11667635425671995, 0.25058486139672437, 0.25058486139672437, 0.1879386460475433, 0.28190796907131493, 0.9794305832032739, 0.007401238664004589, 0.009868318218672786, 0.3926551172587791, 0.16360629885782463, 0.09816377931469478, 0.09816377931469478, 0.22904881840095448, 0.7451982725325424, 0.19610480856119536, 0.019610480856119536, 0.8608559142637526, 0.11738944285414808, 0.940512993769567, 0.0036882862500767334, 0.04057114875084407, 0.0110648587502302, 0.45117301547470506, 0.541407618569646, 0.9654431928022111, 0.8167811679348675, 0.05568962508646824, 0.024750944482874772, 0.02784481254323412, 0.03712641672431216, 0.012375472241437386, 0.00928160418107804, 0.00928160418107804, 0.0030938680603593465, 0.6677880162199151, 0.9285803073262027, 0.13955156704164112, 0.777501587803429, 0.05980781444641762, 0.3280635316314624, 0.02659974580795641, 0.3723964413113897, 0.02659974580795641, 0.10639898323182564, 0.09753240129584018, 0.02659974580795641, 0.9660558945529569, 0.2568523324085689, 0.6849395530895172, 0.18591526209642284, 0.7901398639097971, 0.9683434127167376, 0.9426643809946352, 0.8423570845134788, 0.14865125020826098, 0.42015899713848337, 0.06099082216526371, 0.10165137027543952, 0.020330274055087905, 0.05421406414690108, 0.020330274055087905, 0.020330274055087905, 0.04743730612853844, 0.2507400466794175, 0.8231402412728427, 0.9209369840416618, 0.9200547899722593, 0.28989041735696647, 0.40584658429975307, 0.1159561669427866, 0.1159561669427866, 0.7520436760858648, 0.02156397637522193, 0.09164689959469319, 0.03504146160973563, 0.051214443891152074, 0.008086491140708222, 0.016172982281416445, 0.013477485234513704, 0.008086491140708222, 0.909785823768216, 0.8934939467676862, 0.6724181914672142, 0.7509878546941996, 0.04059393809157835, 0.04059393809157835, 0.15222726784341883, 0.36968676053013794, 0.050411830981382445, 0.10082366196276489, 0.453706478832442, 0.3348241767178051, 0.28957766635053417, 0.07239441658763354, 0.02714790622036258, 0.16288743732217548, 0.10859162488145031, 0.9713323567546704, 0.2619615474195195, 0.36674616638732727, 0.3143538569034234, 0.8695985515463599, 0.7435584860165755, 0.9402311274211738, 0.7880843019554601, 0.9300492286023643, 0.9806027263015985, 0.7723238552372522, 0.17822858197782743, 0.913062985499697, 0.06370206875579282, 0.3028554008512728, 0.5170701965753438, 0.13296090769080268, 0.036933585469667414, 0.5002179962581352, 0.42875828250697307, 0.6653046120578078, 0.3570321099378838, 0.2677740824534129, 0.08925802748447095, 0.2677740824534129, 0.7988997104874153, 0.19622098152322479, 0.8091622217479589, 0.39282506271660755, 0.261883375144405, 0.261883375144405, 0.023807579558582277, 0.05951894889645569, 0.8889925435973209, 0.5507228279660193, 0.11594164799284618, 0.13043435399195194, 0.18840517798837503, 0.9061004101781462, 0.8691048095455239, 0.8838248779552939, 0.825148844350159, 0.1442758932399575, 0.8175633950264258, 0.2745627798515803, 0.20592208488868521, 0.24024243237013276, 0.2574026061108565, 0.9644889546887583, 0.6219234992892332, 0.22615399974153932, 0.03533656245961552, 0.007067312491923104, 0.02120193747576931, 0.03533656245961552, 0.049471187443461725, 0.22763742224615188, 0.07243008889650288, 0.1655544889062923, 0.3621504444825144, 0.05173577778321634, 0.04138862222657307, 0.05173577778321634, 0.020694311113286536, 0.010347155556643268, 0.6146383454717286, 0.33045072337189707, 0.03304507233718971, 0.009913521701156912, 0.009913521701156912, 0.6717716806507887, 0.06989531937407051, 0.24463361780924678, 0.01164921989567842, 0.8982213709038273, 0.8895843910451479, 0.0378546549380914, 0.033122823070829976, 0.033122823070829976, 0.8288908714535964, 0.15720344113775103, 0.19911762917788403, 0.07964705167115362, 0.10619606889487149, 0.053098034447435744, 0.17256861195416617, 0.19911762917788403, 0.10619606889487149, 0.06637254305929467, 0.9441327811424142, 0.7452937915666188, 0.6045410924518633, 0.3806369841363583, 0.8412428325873874, 0.9149287497223739, 0.8616994685087038, 0.10771243356358798, 0.7663637451560527, 0.17685317195908906, 0.9949082923388107, 0.8790705370099382, 0.9271126214357633, 0.9727028396024572, 0.640013286811634, 0.04923179129320262, 0.1914569661402324, 0.027350995162890343, 0.027350995162890343, 0.016410597097734205, 0.021880796130312276, 0.027350995162890343, 0.7269347187591039, 0.5641935410289612, 0.35903225338206624, 0.3126637627718183, 0.6253275255436366, 0.493052315145572, 0.43142077575237553, 0.5698241169027226, 0.9720557774974963, 0.8813226728421585, 0.08262400057895236, 0.5988063621576216, 0.07208904078923241, 0.40369862841970144, 0.41811643657754793, 0.08650684894707888, 0.9020763618934773, 0.8165261778089078, 0.14845930505616506, 0.9699637404806941, 0.8032143925347826, 0.07151908974624777, 0.07151908974624777, 0.016504405326057178, 0.027507342210095296, 0.873994328525202, 0.9002593476605016, 0.39629271568256585, 0.17881500585676752, 0.1111552739109636, 0.004832837996128852, 0.1498179778799944, 0.02416418998064426, 0.028997027976773112, 0.038662703969030814, 0.038662703969030814, 0.004832837996128852, 0.004832837996128852, 0.014498513988386556, 0.6047255219553256, 0.10366723233519866, 0.22461233672626377, 0.05183361616759933, 0.2873303098784758, 0.17958144367404738, 0.023944192489872982, 0.37113498359303126, 0.08380467371455544, 0.035916288734809475, 0.023944192489872982, 0.588907482276719, 0.4196201942552714, 0.45776748464211425, 0.07629458077368571, 0.9020654070289614, 0.6736059015245653, 0.2718058900888597, 0.03545294218550344, 0.2501421674710304, 0.6253554186775759, 0.9714092325633084, 0.794223359173783, 0.19855583979344574, 0.5915782454122088, 0.2169120233178099, 0.059157824541220885, 0.029578912270610443, 0.08873673681183132, 0.9579305944503643, 0.9755707558022478, 0.8231402412728427, 0.7339606020989485, 0.035470214971324646, 0.057298039569062896, 0.002728478074717281, 0.016370868448303683, 0.016370868448303683, 0.019099346523020966, 0.030013258821890088, 0.016370868448303683, 0.024556302672455527, 0.010913912298869123, 0.013642390373586404, 0.008185434224151841, 0.010913912298869123, 0.9587914160185468, 0.35353486766173475, 0.45828593956150804, 0.13093883987471658, 0.05237553594988663, 0.7712179437744238, 0.038258853607608494, 0.046313349103947125, 0.020136238740846576, 0.00805449549633863, 0.00805449549633863, 0.036245229733523836, 0.014095367118592602, 0.00805449549633863, 0.01610899099267726, 0.03221798198535452, 0.4634171255651178, 0.060445712030232766, 0.2579017046623265, 0.028207998947441956, 0.03626742721813966, 0.04835656962418621, 0.060445712030232766, 0.016118856541395405, 0.012089142406046553, 0.012089142406046553, 0.8694159569642057, 0.407392863566493, 0.16004719640112225, 0.1454974512737475, 0.27644515742012027, 0.7084077398430854, 0.07556349224992912, 0.03778174612496456, 0.028336309593723415, 0.1416815479686171, 0.361220718307439, 0.1806103591537195, 0.1354577693652896, 0.3160681285190091, 0.8825941465497208, 0.7986755422845647, 0.07986755422845648, 0.542690651630793, 0.16210240243517193, 0.04933551378461755, 0.05638344432527719, 0.028191722162638596, 0.13391068027253333, 0.02114379162197895, 0.9117589971945752, 0.053081859425711565, 0.01717354275537727, 0.00780615579789876, 0.004683693478739256, 0.003122462319159504, 0.001561231159579752, 0.5127877225466972, 0.42732310212224767, 0.2941179804578292, 0.5656115008804408, 0.1357467602113058, 0.28890532922467055, 0.5915680550790873, 0.020636094944619324, 0.030954142416928985, 0.013757396629746215, 0.013757396629746215, 0.010318047472309662, 0.02407544410205588, 0.798106997787551, 0.11367696110920608, 0.02841924027730152, 0.00473654004621692, 0.00710481006932538, 0.01657789016175922, 0.00947308009243384, 0.00947308009243384, 0.00710481006932538, 0.00710481006932538, 0.9495636572474453, 0.73212848504973, 0.15980469470628625, 0.037163882489834016, 0.03344749424085061, 0.014865552995933605, 0.011149164746950204, 0.007432776497966803, 0.9172428674868013, 0.7435584860165755, 0.48372743042128613, 0.3224849536141908, 0.8231528554652034, 0.3684021226943366, 0.42980247647672604, 0.12280070756477886, 0.24082188412825328, 0.7417314031150201, 0.7803078284402276, 0.026010260948007586, 0.01950769571100569, 0.05202052189601517, 0.07803078284402276, 0.013005130474003793, 0.01950769571100569, 0.9141555998842184, 0.5608264707079237, 0.6937594388101747, 0.09539192283639902, 0.03468797194050873, 0.02601597895538155, 0.017343985970254366, 0.13007989477690776, 0.6654696624255038, 0.5164729467555761, 0.1780941195708883, 0.28495059131342126, 0.2944734208930592, 0.049078903482176535, 0.13496698457598547, 0.5153284865628537, 0.9297686275236045, 0.6168767575221278, 0.7558793945847264, 0.22396426506214115, 0.7847946958677852, 0.037371175993704056, 0.1370276453102482, 0.024914117329136037, 0.8083604137313347, 0.5324887968268293, 0.42599103746146344, 0.5176747457168787, 0.08959755214330593, 0.13937397000069812, 0.24888208928696093, 0.9027523641330747, 0.2021273340146839, 0.7276584024528621, 0.6543959194193621, 0.3020288858858594, 0.8468136805013768, 0.03264361126649298, 0.02688297398417069, 0.05760637282322291, 0.007680849709763055, 0.017281911846966872, 0.007680849709763055, 0.9405657224963174, 0.949631670605479, 0.8537381969351444, 0.7435584860165755, 0.4379211289567068, 0.22891331740918763, 0.11943303517001094, 0.11943303517001094, 0.08957477637750821, 0.33318755201441536, 0.5553125866906923, 0.03702083911271282, 0.03702083911271282, 0.02468055940847521, 0.8414753762501487, 0.04892298699128771, 0.02935379219477263, 0.04892298699128771, 0.009784597398257542, 0.009784597398257542, 0.009784597398257542, 0.6667705484392247, 0.034788028614220416, 0.12175810014977147, 0.023192019076146944, 0.08697007153555104, 0.011596009538073472, 0.005798004769036736, 0.011596009538073472, 0.023192019076146944, 0.005798004769036736, 0.4998258978627156, 0.2607787293196777, 0.228181388154718, 0.8553656251627672, 0.0023892894557619197, 0.014335736734571516, 0.05017507857100031, 0.011946447278809598, 0.009557157823047679, 0.019114315646095358, 0.011946447278809598, 0.023892894557619195, 0.5802823535957636, 0.39050447694809254, 0.010948723652750257, 0.018247872754583762, 0.9363600697384299, 0.15677718944674132, 0.07838859472337066, 0.05225906314891377, 0.30048961310625416, 0.06532382893614222, 0.2612953157445689, 0.03919429736168533, 0.026129531574456886, 0.03919429736168533, 0.67858083419929, 0.21889704329009355, 0.08755881731603743, 0.7539599278463913, 0.22847270540799738, 0.825148844350159, 0.18715553000623106, 0.7174295316905525, 0.09357776500311553, 0.21405805836932532, 0.4663407700188873, 0.20641312771327797, 0.04586958393628399, 0.05351451459233133, 0.6041408345723335, 0.34522333404133343, 0.04315291675516668, 0.8488642439705479, 0.8812459516680607, 0.5544294158935921, 0.17393864028034262, 0.13045398021025698, 0.13045398021025698, 0.6092415028300934, 0.06908924258897967, 0.07537008282434145, 0.02512336094144715, 0.02512336094144715, 0.1004934437657886, 0.06280840235361787, 0.031404201176808935, 0.3365116394109405, 0.3996075718004918, 0.23135175209502157, 0.8494875343496681, 0.5613579579724026, 0.18954944035431776, 0.03645181545275342, 0.18225907726376708, 0.02187108927165205, 0.012160496309899724, 0.9850002011018776, 0.6022118621372041, 0.24088474485488165, 0.3444807286554323, 0.48227302011760514, 0.06889614573108645, 0.25176473648573855, 0.18882355236430393, 0.5664706570929118, 0.9319458089629811, 0.02034816176775068, 0.024417794121300816, 0.008139264707100271, 0.008139264707100271, 0.15994651089788323, 0.836833549285317, 0.0008244665510200167, 0.0016489331020400333, 0.8778796008192471, 0.10327995303755848, 0.4151810441316898, 0.55118862755414, 0.02147488159301844, 0.00715829386433948, 0.9335611198384431, 0.7530781689519765, 0.03383713110133113, 0.9305211052866061, 0.025377848325998348, 0.5352736722498757, 0.38233833732133976, 0.2842518076165342, 0.68764268647274, 0.0008409816793388585, 0.0005606544528925724, 0.007008180661157154, 0.005606544528925723, 0.00364425394380172, 0.0005606544528925724, 0.0019622905851240033, 0.003363926717355434, 0.0008409816793388585, 0.0002803272264462862, 0.0014016361322314308, 0.0005606544528925724, 0.0005606544528925724, 0.0005606544528925724, 0.0005606544528925724, 0.5848374014528563, 0.014806010163363453, 0.08883606098018072, 0.10364207114354416, 0.007403005081681726, 0.04441803049009036, 0.08143305589849899, 0.029612020326726905, 0.01110450762252259, 0.007403005081681726, 0.014806010163363453, 0.01110450762252259, 0.967850774143553, 0.023606116442525683, 0.9564998103420792, 0.8033305318424384, 0.6778816964583316, 0.04420967585597815, 0.08105107240262661, 0.02947311723731877, 0.11789246894927508, 0.014736558618659384, 0.02947311723731877, 0.2930274607182592, 0.08790823821547775, 0.04395411910773887, 0.3223302067900851, 0.16116510339504256, 0.0732568651795648, 0.9770137496562176, 0.8290066686373228, 0.06333983535655949, 0.028875513177255063, 0.007451745336065823, 0.0009314681670082279, 0.012109086171106962, 0.0167664270061481, 0.0037258726680329114, 0.02514964050922215, 0.0027944045010246834, 0.0009314681670082279, 0.0037258726680329114, 0.0037258726680329114, 0.17176038111189598, 0.6870415244475839, 0.6256216054032011, 0.9059991576234236, 0.7713406649296605, 0.10587028734328673, 0.03781081690831669, 0.007562163381663338, 0.015124326763326677, 0.007562163381663338, 0.01764504789054779, 0.03529009578109558, 0.8036386276687368, 0.13393977127812282, 0.6916576216833196, 0.11527627028055326, 0.15370169370740433, 0.834748280335499, 0.0868524222314392, 0.014475403705239866, 0.009650269136826577, 0.009650269136826577, 0.006433512757884385, 0.0032167563789421926, 0.004825134568413289, 0.0032167563789421926, 0.01769216008418206, 0.004825134568413289, 0.004825134568413289, 0.22613758727773045, 0.20101118869131596, 0.5527807689011189, 0.2928359568088784, 0.6370057068432176, 0.005833385593802359, 0.016333479662646606, 0.017500156781407077, 0.0011666771187604718, 0.01866683390016755, 0.008166739831323303, 0.0023333542375209436, 0.853925417481426, 0.05822218755555178, 0.027724851216929417, 0.008317455365078826, 0.008317455365078826, 0.024952366095236475, 0.008317455365078826, 0.008317455365078826, 0.7257205366012484, 0.26524556459151005, 0.6631139114787752, 0.04420759409858501, 0.9393149164767821, 0.5575991603406791, 0.037399943681387014, 0.06799989760252184, 0.04759992832176529, 0.26179960576970907, 0.010199984640378275, 0.003399994880126092, 0.010199984640378275, 0.924624062604698, 0.17687157075181179, 0.7698124555578856, 0.011791438050120786, 0.011791438050120786, 0.015160420350155296, 0.011791438050120786, 0.43828983314382874, 0.26678511582667835, 0.26678511582667835, 0.9509446025841649, 0.719069717166296, 0.2739313208252556, 0.4270235766738168, 0.47726164451779524, 0.025119033921989223, 0.06279758480497305, 0.7580859818811935, 0.7581903883034481, 0.09347552732508264, 0.051930848513934806, 0.08308935762229569, 0.4934512875578106, 0.3289675250385404, 0.8231402412728427, 0.18611617857635349, 0.3393883256392328, 0.459816441188638, 0.8805134295429824, 0.07656638517765064, 0.9666685182630186, 0.5235306472293796, 0.18452309697428954, 0.025747408880133424, 0.06865975701368913, 0.05578605257362242, 0.03432987850684457, 0.06007728738697799, 0.021456174066777856, 0.021456174066777856, 0.046117153671355764, 0.9223430734271152, 0.5124281072269308, 0.43359301380740295, 0.4547142845452746, 0.4547142845452746, 0.9203930451624066, 0.14178171202835502, 0.7672892650946271, 0.07506090636795265, 0.01668020141510059, 0.8253915675262239, 0.700808089022777, 0.025028860322242035, 0.08760101112784713, 0.03754329048336305, 0.1376587317723312, 0.8757361846142802, 0.6763337741930526, 0.10405134987585425, 0.10405134987585425, 0.09104493114137246, 0.7269347187591039, 0.08308852141814704, 0.036928231741398684, 0.14771292696559474, 0.7201005189572744, 0.43712597168403866, 0.5342650765027139, 0.44117494658456474, 0.11029373664614119, 0.08088207354050353, 0.08088207354050353, 0.1838228944102353, 0.05147041043486589, 0.05147041043486589, 0.49166820893431323, 0.36875115670073494, 0.8251074294465538, 0.8252642939122886, 0.890812398784118, 0.9403829001274309, 0.39961286593520595, 0.6119127368540097, 0.2557026654003471, 0.011824400712154777, 0.001478050089019347, 0.010346350623135431, 0.020692701246270862, 0.001478050089019347, 0.048775652937638456, 0.001478050089019347, 0.011824400712154777, 0.005912200356077388, 0.011824400712154777, 0.007390250445096736, 0.002956100178038694, 0.22381507211007218, 0.6056172539449012, 0.14482151724769377, 0.2456855644691494, 0.6551615052510651, 0.963717835336372, 0.3842898643987053, 0.5816279028737161, 0.02077242510263272, 0.964183845111852, 0.04292014289764306, 0.2682508931102691, 0.6867222863622889, 0.7269347187591039, 0.30302170147046026, 0.4924102648894979, 0.0568165690257113, 0.1136331380514226, 0.8973942047141393, 0.783773656881368, 0.9594301449070802, 0.03725942310318758, 0.5988063621576216, 0.8172951118479543, 0.13621585197465905, 0.7774874461193021, 0.1794201798736851, 0.22776041948426676, 0.740221363323867, 0.9052636751577189, 0.8077157108260025, 0.8078001961049345, 0.8316710671796335, 0.1512129213053879, 0.23166221611139892, 0.21568551155199212, 0.16775539787377164, 0.03195340911881364, 0.3514875003069501, 0.8916488614928434, 0.9801175098862166, 0.9077507930767458, 0.6912136451091887, 0.04538271407282552, 0.027927824044815704, 0.03840075806162159, 0.02094586803361178, 0.003490978005601963, 0.04538271407282552, 0.006981956011203926, 0.01047293401680589, 0.031418802050417666, 0.013963912022407852, 0.03840075806162159, 0.02094586803361178, 0.006981956011203926, 0.2854482347156323, 0.19761800864928392, 0.5050237998815034, 0.4877080248687174, 0.36578101865153806, 0.4403043815228138, 0.13547827123778886, 0.30482611028502493, 0.10160870342834165, 0.9467281403760429, 0.957966885020492, 0.9186727802081363, 0.6899003045133716, 0.2682945628663112, 0.6677342152968203, 0.1824863693978888, 0.11612768961683832, 0.02073708743157827, 0.012442252458946963, 0.8543233741378922, 0.016429295656497925, 0.12321971742373444, 0.9160457619421469, 0.9468052688279405, 0.9467097225855154, 0.24423458433773257, 0.24423458433773257, 0.48846916867546514, 0.8521806373253091, 0.06464818627985104, 0.011754215687245644, 0.02350843137449129, 0.04701686274898258, 0.5446303414644018, 0.09270303684500455, 0.02317575921125114, 0.09270303684500455, 0.03476363881687671, 0.09270303684500455, 0.03476363881687671, 0.06952727763375342, 0.2842883667618978, 0.1895255778412652, 0.07107209169047445, 0.0473813944603163, 0.30797906399205593, 0.07107209169047445, 0.7481953585453076, 0.14388372279717454, 0.08633023367830472, 0.9236030649430412, 0.00607633595357264, 0.02430534381429056, 0.02126717583750424, 0.00911450393035896, 0.00607633595357264, 0.00607633595357264, 0.8608749595742401, 0.7664027878252275, 0.37049634767691536, 0.2963970781415323, 0.0673629723048937, 0.1145170529183193, 0.12125335014880867, 0.02694518892195748, 0.7435584860165755, 0.9498655324383636, 0.32662672775121926, 0.28579838678231684, 0.04082834096890241, 0.3402361747408534, 0.4759186879386988, 0.36685398861941365, 0.07931978132311647, 0.039659890661558235, 0.034702404328863456, 0.8285679041135819, 0.8399377501065289, 0.04027098801880618, 0.05752998288400883, 0.023011993153603534, 0.023011993153603534, 0.011505996576801767, 0.5647850698824565, 0.0529486003014803, 0.06177336701839368, 0.07059813373530706, 0.20296963448900782, 0.04412383358456692, 0.8947167171036712, 0.30175425414406, 0.60350850828812, 0.8713025197223211, 0.8982757308496222, 0.765166102685953, 0.11020910944036359, 0.012595326793184411, 0.11020910944036359, 0.9080668594037595, 0.7095348193585181, 0.2147911527990072, 0.005631228725067604, 0.003217844985752917, 0.015284763682326355, 0.016893686175202812, 0.004826767478629375, 0.0016089224928764584, 0.013675841189449896, 0.0024133837393146874, 0.003217844985752917, 0.008849073710820521, 0.0849009966835863, 0.48110564787365573, 0.06603410853167824, 0.18866888151908068, 0.09433444075954034, 0.0283003322278621, 0.04716722037977017, 0.7687598185415501, 0.021579222976604914, 0.016184417232453684, 0.002697402872075614, 0.048553251697361056, 0.010789611488302457, 0.010789611488302457, 0.010789611488302457, 0.04046104308113421, 0.010789611488302457, 0.013487014360378071, 0.005394805744151228, 0.0188818201045293, 0.005394805744151228, 0.005394805744151228, 0.49943879999199065, 0.32147784827070663, 0.11481351723953807, 0.04018473103383833, 0.01722202758593071, 0.9591350440295768, 0.02368234676616239, 0.8605634140006904, 0.10326760968008283, 0.978130660379028, 0.7438583048621974, 0.1692038204192981, 0.028732724222144963, 0.0031925249135716623, 0.009577574740714988, 0.0031925249135716623, 0.04150282387643161, 0.8155424553158612, 0.641332557429753, 0.13227483996988657, 0.016033313935743823, 0.048099941807231476, 0.058120763017071364, 0.028058299387551694, 0.010020821209839891, 0.002004164241967978, 0.058120763017071364, 0.0060124927259039345, 0.17931731559757677, 0.4482932889939419, 0.3138053022957593, 0.7741298827637565, 0.05692131490909975, 0.0455370519272798, 0.0455370519272798, 0.05692131490909975, 0.9471966818843874, 0.7045690431857093, 0.1627575905231426, 0.029981661412157846, 0.017132377949804482, 0.014990830706078923, 0.008566188974902241, 0.014990830706078923, 0.023557019680981162, 0.008566188974902241, 0.010707736218627802, 0.0021415472437255603, 0.1356487440781681, 0.8582865988945908, 0.004932681602842476, 0.8854163707866103, 0.6654696624255038, 0.8187416154741541, 0.26537019930729405, 0.6899625181989645, 0.9270368588565091, 0.019605009099684483, 0.0028007155856692115, 0.011202862342676846, 0.03360858702803054, 0.90279142200061, 0.010186645100147926, 0.019099959562777363, 0.005093322550073963, 0.0025466612750369814, 0.012733306375184907, 0.0140066370127034, 0.007639983825110945, 0.0140066370127034, 0.005093322550073963, 0.005093322550073963, 0.0012733306375184907, 0.3518527772550839, 0.12418333314885315, 0.034495370319125876, 0.04829351844677622, 0.034495370319125876, 0.09658703689355244, 0.0551925925106014, 0.12418333314885315, 0.020697222191475524, 0.0551925925106014, 0.04139444438295105, 0.8745207080054112, 0.027097824755097245, 0.041878456439695746, 0.046805333667895244, 0.004926877228199499, 0.6791297109298181, 0.1940370602656623, 0.6196005040669837, 0.02383078861796091, 0.07546416395687622, 0.035746182926941365, 0.06752056775088926, 0.10326675067783062, 0.04766157723592182, 0.02383078861796091, 0.2060886451776601, 0.7556583656514204, 0.8787683645573058, 0.03423772848924568, 0.06847545697849136, 0.011412576163081892, 0.9497142175063455, 0.3927258695961327, 0.5890888043941991, 0.918688296815708, 0.4731760179540007, 0.3943133482950006, 0.6330081177153238, 0.15825202942883096, 0.1978150367860387, 0.4401923423285874, 0.48910260258731936, 0.8219627144849144, 0.9488404756853978, 0.9734104837671459, 0.878682589525067, 0.44530876632429767, 0.526273996565079, 0.9129178247648837, 0.007938415867520728, 0.0555689110726451, 0.015876831735041456, 0.007938415867520728, 0.5988063621576216, 0.2711369933001366, 0.10609708433483604, 0.5422739866002732, 0.07073138955655736, 0.30726216422972463, 0.6486645689294186, 0.27123084727105934, 0.7137653875554193, 0.45086750017308275, 0.495954250190391, 0.889043289531221, 0.8533516998828627, 0.029942164908170622, 0.003742770613521328, 0.037427706135213276, 0.059884329816341245, 0.011228311840563984, 0.6654696624255038, 0.917660656353237, 0.5242665428882515, 0.04194132343106012, 0.15727996286647544, 0.12582397029318035, 0.03145599257329509, 0.01048533085776503, 0.08388264686212024, 0.7059953127597092, 0.9387008436416274, 0.8396771644922766, 0.009244152269639741, 0.046220761348198705, 0.02465107271903931, 0.03389522498867905, 0.009244152269639741, 0.0323545329437391, 0.5079857828976798, 0.04980252773506665, 0.17928909984623995, 0.01992101109402666, 0.2290916275813066, 0.21095227627472865, 0.3970866376936069, 0.03722687228377564, 0.03722687228377564, 0.01861343614188782, 0.03102239356981304, 0.08686270199547651, 0.024817914855850432, 0.11788509556528955, 0.03102239356981304, 0.012408957427925216, 0.3971547329306612, 0.49644341616332655, 0.9616278192676948, 0.5685623153913695, 0.3790415435942463, 0.3400129547549015, 0.5666882579248359, 0.1454010901302874, 0.1454010901302874, 0.1454010901302874, 0.16617267443461417, 0.08308633721730709, 0.10385792152163387, 0.18694425873894097, 0.560322956352659, 0.2801614781763295, 0.8278307852171918, 0.15768205432708415, 0.11176896066749042, 0.1564765449344866, 0.536491011203954, 0.1564765449344866, 0.6182898340179822, 0.20609661133932738, 0.8337786027155458, 0.024086937411782437, 0.03705682678735759, 0.018528413393678795, 0.007411365357471519, 0.005558524018103639, 0.012969889375575158, 0.0037056826787357595, 0.03335114410862183, 0.0037056826787357595, 0.009264206696839397, 0.009264206696839397, 0.8119325018692618, 0.024116806986215697, 0.028136274817251648, 0.012058403493107848, 0.0160778713241438, 0.012058403493107848, 0.048233613972431394, 0.0160778713241438, 0.024116806986215697, 0.7459543696248029, 0.04875518755717666, 0.017064315645011832, 0.017064315645011832, 0.0365663906678825, 0.002437759377858833, 0.004875518755717666, 0.017064315645011832, 0.021939834400729497, 0.021939834400729497, 0.014626556267152998, 0.007313278133576499, 0.014626556267152998, 0.009751037511435332, 0.009751037511435332, 0.007313278133576499, 0.9311197543552336, 0.23051029979180512, 0.45085102753397177, 0.12542472194554102, 0.03389857349879487, 0.06101743229783076, 0.02372900144915641, 0.006779714699758974, 0.06101743229783076, 0.006779714699758974, 0.743536739866757, 0.46771310170707847, 0.5066891935160017, 0.16415045901310382, 0.7660354753944845, 0.8525441767908556, 0.8019214320477935, 0.1336535720079656, 0.18407678847425404, 0.10226488248569669, 0.5726833419199014, 0.040905952994278676, 0.10226488248569669, 0.8288114576104573, 0.041211619439194005, 0.02747441295946267, 0.013737206479731335, 0.08700230770496513, 0.8026027557122793, 0.8841739757727682, 0.09146627335580361, 0.743536739866757, 0.5060517056711834, 0.4723149252931045, 0.7435584860165755, 0.9483643468075552, 0.9842796501986095, 0.854892431599415, 0.15070198166656765, 0.8037439022216941, 0.857577467200773, 0.8980996477154862, 0.3130024952336867, 0.33908603650316055, 0.06520885317368473, 0.11737593571263251, 0.039125311904210834, 0.05216708253894778, 0.06520885317368473, 0.6375700069767298, 0.20818612472709544, 0.05204653118177386, 0.09108142956810425, 0.6274044394335916, 0.09411066591503874, 0.2666468867592764, 0.7409814791873057, 0.1790705241369322, 0.008644783923851899, 0.0037049073959365283, 0.038284043091344123, 0.0037049073959365283, 0.011114722187809584, 0.0037049073959365283, 0.004939876527915371, 0.0012349691319788428, 0.0024699382639576857, 0.9611552417136764, 0.8252642939122886, 0.9711078830630527, 0.018497293010724815, 0.5281382718257506, 0.24646452685201695, 0.04694562416228894, 0.12910046644629458, 0.03520921812171671, 0.9502504722586143, 0.9264689937910923, 0.5781730087216875, 0.24090542030070314, 0.1686337942104922, 0.6159295263803715, 0.2009875296609633, 0.05835121828866677, 0.025933874794963008, 0.006483468698740752, 0.025933874794963008, 0.012966937397481504, 0.051867749589926015, 0.914063317174045, 0.8671143991983399, 0.2999557110305822, 0.12629714148656093, 0.5683371366895242, 0.9330952874986695, 0.9495659517415932, 0.8216680789236539, 0.7209401292982607, 0.84540078291859, 0.05404388566602858, 0.04246305302330817, 0.01930138773786735, 0.01544111019029388, 0.01544111019029388, 0.00772055509514694, 0.8250906442417765, 0.82315348618497, 0.9175834610451943, 0.13827105682935648, 0.355554146132631, 0.13827105682935648, 0.0592590243554385, 0.29629512177719247, 0.9519634450441368, 0.9078747067557512, 0.4637011794784833, 0.30913411965232224, 0.9619267803721058, 0.027962987801514703, 0.5508501510319131, 0.43608970290026455, 0.9918904893736298, 0.9392417249951982, 0.8080554732236376, 0.11543649617480538, 0.49375854574381134, 0.0740637818615717, 0.34563098202066794, 0.04937585457438114, 0.45181362861649754, 0.45181362861649754, 0.7436055839498257, 0.9491037086019204, 0.016506151453946443, 0.008253075726973222, 0.006189806795229916, 0.004126537863486611, 0.004126537863486611, 0.008253075726973222, 0.24307906012805724, 0.13890232007317557, 0.5903348603109961, 0.9154123438091714, 0.5845914560173642, 0.8133039865145738, 0.11618628378779626, 0.5091920168556991, 0.46845665550724325, 0.20013681319823623, 0.320218901117178, 0.4403009890361197, 0.9457531610726835, 0.7626962909871045, 0.2145083318401231, 0.3772148796907241, 0.5985851142510081, 0.0017709618764822728, 0.002361282501976364, 0.004722565003952728, 0.009445130007905455, 0.000590320625494091, 0.002361282501976364, 0.002361282501976364, 0.9426180209138056, 0.0036677744004428234, 0.018338872002214117, 0.025674420803099764, 0.0036677744004428234, 0.0036677744004428234, 0.07098830904132411, 0.09465107872176548, 0.1656393877630896, 0.6625575510523584, 0.8651862176912485, 0.4693426353471883, 0.4693426353471883, 0.05568471944797149, 0.9440200940882996, 0.7764882591186144, 0.48325144411855525, 0.46766268785666637, 0.031177512523777758, 0.6565825319791365, 0.06201057246469623, 0.05836289173147881, 0.05471521099826138, 0.014590722932869702, 0.014590722932869702, 0.0875443375972182, 0.025533765132521976, 0.02188608439930455, 0.35324253490147156, 0.5887375581691193, 0.588907482276719, 0.7676949359745399, 0.09031705129112234, 0.12042273505482978, 0.9145818919847226, 0.6530954324946372, 0.0483774394440472, 0.016125813148015734, 0.24188719722023602, 0.04031453287003934, 0.9712841157296074, 0.9018642161215514, 0.9121154859383337, 0.936366044145733, 0.2604489579089831, 0.6077142351209606, 0.8267966699797044, 0.032636710657093594, 0.01087890355236453, 0.038076162433275856, 0.016318355328546797, 0.016318355328546797, 0.03535643654518472, 0.01087890355236453, 0.008159177664273399, 0.0027197258880911324, 0.6144537858348323, 0.030078157348558225, 0.11171887015178769, 0.04296879621222604, 0.008593759242445207, 0.06445319431833905, 0.055859435075893846, 0.06015631469711645, 0.01289063886366781, 0.004296879621222604, 0.8187780855260902, 0.6653204780743458, 0.8583935426454755, 0.10402141407439516, 0.7489541813356452, 0.1248256968892742, 0.48001796432274085, 0.11826529555777673, 0.006956782091633925, 0.1113085134661428, 0.020870346274901776, 0.09739494928287495, 0.09043816719124104, 0.06956782091633926, 0.6422317935060036, 0.12004332588897264, 0.018006498883345896, 0.12004332588897264, 0.08403032812228084, 0.012004332588897264, 0.43499349255717584, 0.12083152571032663, 0.43499349255717584, 0.7386976038717991, 0.8932211151692384, 0.08373947954711611, 0.6774593026601204, 0.1625902326384289, 0.13549186053202408, 0.617037387523647, 0.7435584860165755, 0.9629232742580539, 0.4233878033148169, 0.06414966716891166, 0.3720680695796876, 0.02565986686756466, 0.0898095340364763, 0.38544254082715956, 0.14454095281018484, 0.43362285843055454, 0.8605967458059418, 0.09644618702997623, 0.02596628112345514, 0.014837874927688652, 0.743536739866757, 0.34528944048863114, 0.6491441481186265, 0.31422129733174553, 0.13965390992522025, 0.13965390992522025, 0.0931026066168135, 0.06400804204905927, 0.05818912913550844, 0.11637825827101687, 0.017456738740652532, 0.017456738740652532, 0.034913477481305064, 0.9613127353130089, 0.03269771208547649, 0.8809161099627092, 0.9866854111813494, 0.8164646348778829, 0.3356063486516132, 0.5593439144193554, 0.05593439144193554, 0.902998972201214, 0.7708645895901934, 0.16061680501667927, 0.7495450900778365, 0.9657822641817643, 0.874479471583447, 0.11659726287779293, 0.2894122792723279, 0.3183535071995607, 0.34729473512679343, 0.9566417157611712, 0.024529274763106953, 0.9176450910432568, 0.3597289268424457, 0.6115391756321576, 0.6297450032660815, 0.10988838983166524, 0.06762362451179399, 0.04226476531987124, 0.016905906127948497, 0.12679429595961372, 0.9783041263226298, 0.5249565341418392, 0.41996522731347136, 0.025418947968973267, 0.01657757476237387, 0.009946544857424322, 0.0022103433016498492, 0.8889097054436988, 0.6910067899613517, 0.0928687940950336, 0.03305499450840179, 0.015740473575429424, 0.017314520932972366, 0.0015740473575429424, 0.022036663005601195, 0.004722142072628827, 0.004722142072628827, 0.10073903088274831, 0.004722142072628827, 0.0031480947150858848, 0.0062961894301717695, 0.9747727897329521, 0.1622431281861245, 0.5272901666049046, 0.20280391023265562, 0.08112156409306225, 0.23686232756228515, 0.34048959587078487, 0.10362726830849975, 0.25166622303492797, 0.04441168641792846, 0.029607790945285643, 0.9805139859943554, 0.961168596559146, 0.015756862238674525, 0.5988063621576216, 0.9172623693251323, 0.1002931272354238, 0.22793892553505407, 0.031911449574907574, 0.018235114042804326, 0.08205801319261948, 0.059264120639114064, 0.07749923468191838, 0.02735267106420649, 0.07749923468191838, 0.2552915965992606, 0.03647022808560865, 0.9304519116530975, 0.7671026868391625, 0.965177633533135, 0.9575942409955335, 0.20486465653346023, 0.7784856948271489, 0.8460311954990638, 0.7435584860165755, 0.19057974517863466, 0.4446860720834809, 0.3176329086310578, 0.44719013557039733, 0.1341570406711192, 0.08943802711407946, 0.10434436496642603, 0.11925070281877262, 0.029812675704693155, 0.014906337852346577, 0.04471901355703973, 0.898265165078513, 0.3169451629071134, 0.16253598097800687, 0.1137751866846048, 0.03250719619560137, 0.1137751866846048, 0.06501439239120274, 0.16253598097800687, 0.016253598097800685, 0.016253598097800685, 0.2136514294921046, 0.09495619088537983, 0.09495619088537983, 0.2611295249347945, 0.07121714316403487, 0.14243428632806973, 0.11869523860672479, 0.5526733567406863, 0.4145050175555147, 0.6653046120578078, 0.8696101715849591, 0.8647442556164182, 0.09977818334035594, 0.9639684815579062, 0.9416527606262055, 0.04046164205815727, 0.014713324384784461, 0.6654696624255038, 0.8352296482478988, 0.8947678491377492, 0.8983483733466637, 0.8283732696875963, 0.9253449248084774, 0.24086410971679084, 0.5620162560058453, 0.18733875200194844, 0.9639736932766927, 0.6011870759941672, 0.3506924609965975, 0.16975197501915787, 0.10185118501149472, 0.712958295080463, 0.46604963779871145, 0.46604963779871145, 0.3037136227795658, 0.3037136227795658, 0.10123787425985527, 0.10123787425985527, 0.10123787425985527, 0.050618937129927634, 0.3933779603565955, 0.26648184411253245, 0.08565487846474257, 0.06662046102813311, 0.082482475558641, 0.03172402906101577, 0.006344805812203154, 0.015862014530507885, 0.012689611624406308, 0.009517208718304731, 0.012689611624406308, 0.009517208718304731, 0.6534356051175716, 0.3267178025587858, 0.38396980856932833, 0.6033811277518016, 0.9658234865535175, 0.022118095111912612, 0.7337057033421502, 0.035217873760423214, 0.005869645626737202, 0.017608936880211607, 0.17608936880211606, 0.017608936880211607, 0.011739291253474404, 0.6296232836587072, 0.25427094147755486, 0.10897326063323778, 0.872017950535862, 0.013625280477122843, 0.004541760159040948, 0.027250560954245686, 0.0068126402385614215, 0.011354400397602368, 0.02952144103376616, 0.004541760159040948, 0.0068126402385614215, 0.013625280477122843, 0.0068126402385614215, 0.004541760159040948, 0.8252642939122886, 0.6070988554105436, 0.24283954216421746, 0.9378464395095323, 0.8526886891088419, 0.8805550959680682, 0.5801911232129457, 0.3754177856083767, 0.9463374013526058, 0.03943072505635858, 0.013553454576303848, 0.982625456782029, 0.837568121021171, 0.025088446021371478, 0.05789641389547264, 0.044387250653195695, 0.005789641389547264, 0.0077195218527296855, 0.009649402315912107, 0.009649402315912107, 0.711643333706644, 0.2668662501399915, 0.7104706068786739, 0.0493382365887968, 0.02960294195327808, 0.11512255204052586, 0.023024510408105173, 0.01973529463551872, 0.016446078862932267, 0.016446078862932267, 0.00986764731775936, 0.00986764731775936, 0.7289700045948118, 0.08099666717720132, 0.13499444529533552, 0.5014020270319652, 0.1253505067579913, 0.2507010135159826, 0.6809837080006951, 0.07738451227280627, 0.10833831718192877, 0.025794837424268753, 0.041271739878830004, 0.041271739878830004, 0.015476902454561252, 0.1714438453139269, 0.7429233296936832, 0.2307463434613374, 0.6263115036807729, 0.13185505340647852, 0.8860324373249393, 0.10126084997999306, 0.23063259900967, 0.13089958322170459, 0.043633194407234864, 0.043633194407234864, 0.09349970230121757, 0.11219964276146108, 0.06856644835422622, 0.06856644835422622, 0.06856644835422622, 0.06856644835422622, 0.049866507893982705, 0.018699940460243515, 0.9071744110396043, 0.5383979695343102, 0.15499335486593777, 0.04078772496472047, 0.21209616981654641, 0.05710281495060865, 0.6472872844486286, 0.491949611649339, 0.43728854368830133, 0.598861629893289, 0.9692204667266984, 0.89452259708369, 0.6723593179314747, 0.22411977264382493, 0.4964059351205933, 0.17183282369558997, 0.3054805754588266, 0.2796933859126079, 0.5593867718252158, 0.919951232609258, 0.004401680538800278, 0.06162352754320389, 0.013205041616400834, 0.698686100175926, 0.12626857232095048, 0.025253714464190096, 0.05892533374977689, 0.08417904821396699, 0.3316247429598856, 0.4712562136798374, 0.03490786767998796, 0.13963147071995183, 0.15680773760004482, 0.41815396693345286, 0.10453849173336321, 0.07840386880002241, 0.18294236053338564, 0.19030562748046714, 0.6343520916015571, 0.12687041832031143, 0.14902192638642434, 0.5960877055456973, 0.17385891411749507, 0.049673975462141445, 0.3500747412040963, 0.286424788257897, 0.07956244118274916, 0.12729990589239867, 0.1432123941289485, 0.29589534141558055, 0.6145518629400519, 0.06828354032667243, 0.416362031394213, 0.5697585692762915, 0.9014081322619155, 0.331203647176689, 0.5520060786278149, 0.569816758112967, 0.6654696624255038, 0.8466786100165512, 0.9524093159542629, 0.4420292452575548, 0.7650146149402305, 0.22500429851183248, 0.7181080031784243, 0.07428703481156114, 0.06053017651312389, 0.030265088256561944, 0.016508229958124697, 0.008254114979062348, 0.016508229958124697, 0.008254114979062348, 0.011005486638749798, 0.027513716596874495, 0.013756858298437247, 0.013756858298437247, 0.9543237833429004, 0.023276189837631717, 0.011638094918815858, 0.7358561579895936, 0.1320767463058245, 0.1132086396907067, 0.09490610702532018, 0.11388732843038421, 0.1518497712405123, 0.5694366421519211, 0.056943664215192105, 0.8838734971314072, 0.39149761132759414, 0.2433633800144504, 0.3491735452381245, 0.7484416291886162, 0.18711040729715406, 0.13697582538830363, 0.75336703963567, 0.8808205396424977, 0.5626530561493468, 0.4018950401066763, 0.16187931722780377, 0.6475172689112151, 0.16187931722780377, 0.5698241169027226, 0.7427797685980673, 0.9621613423932958, 0.6970730582105478, 0.18588614885614607, 0.09294307442807304, 0.9592129106248342, 0.027015798319793033, 0.21612638655834426, 0.24314218487813732, 0.2971737815177234, 0.13507899159896516, 0.054031596639586066, 0.8251228588735462, 0.974657686771073, 0.9724301602521648, 0.21031394124707825, 0.7010464708235942, 0.8940537274132229, 0.1891498276363395, 0.29232246089252467, 0.08597719438015432, 0.25793158314046294, 0.08597719438015432, 0.06878175550412345, 0.8394036299640947, 0.9557965672731453, 0.8231528554652034, 0.8870263258385641, 0.07096210606708513, 0.9732834146707152, 0.010420593304825644, 0.002084118660965129, 0.010420593304825644, 0.8357716187817149, 0.7091827705389068, 0.03693660263223473, 0.022161961579340837, 0.0812605257909164, 0.11819712842315114, 0.014774641052893892, 0.5902314439436583, 0.0574900757087979, 0.1149801514175958, 0.03449404542527874, 0.007665343427839719, 0.061322747422717754, 0.0574900757087979, 0.06898809085055747, 0.8827190656198771, 0.9497824627033771, 0.7853113614434473, 0.9151051041342424, 0.5889031384985914, 0.9469940987597922, 0.32057464731288576, 0.5342910788548096, 0.6727837322905758, 0.27857451415156653, 0.01576836872556037, 0.03153673745112074, 0.14108734601213507, 0.4408979562879221, 0.1058155095091013, 0.008817959125758442, 0.044089795628792214, 0.1058155095091013, 0.05290775475455065, 0.05290775475455065, 0.026453877377275326, 0.017635918251516884, 0.7435584860165755, 0.8555796313938393, 0.9262231294938129, 0.743536739866757, 0.9154843783281035, 0.7179027033515133, 0.7978941532748004, 0.18674118480899582, 0.008488235673136174, 0.6956556438328155, 0.2650116738410726, 0.3542799941868866, 0.6073371328918056, 0.5992380997631528, 0.07295072518855773, 0.03126459650938189, 0.0833722573583517, 0.12505838603752756, 0.0833722573583517, 0.9250218145545599, 0.2263587948244036, 0.1697690961183027, 0.5093072883549081, 0.2186110809832977, 0.3133425494093934, 0.3789258737043827, 0.06558332429498931, 0.021861108098329772, 0.26475578229153474, 0.7220612244314584, 0.7498825311408327, 0.10415035154733789, 0.14581049216627304, 0.7425873480976435, 0.4888980133741881, 0.4888980133741881, 0.6654696624255038, 0.24122656694955658, 0.6030664173738914, 0.08040885564985219, 0.9518718909024919, 0.9119528963345592, 0.002674348669602813, 0.029417835365630945, 0.002674348669602813, 0.005348697339205626, 0.005348697339205626, 0.016046092017616878, 0.018720440687219692, 0.005348697339205626, 0.825173178074777, 0.4403884266070156, 0.5033010589794464, 0.26940860461320615, 0.4041129069198092, 0.13470430230660307, 0.6170373754153042, 0.8655365792623912, 0.24037328071039507, 0.09013998026639815, 0.4281649062653912, 0.23286161568819522, 0.29554208482366723, 0.33248484542662565, 0.11082828180887522, 0.11082828180887522, 0.11082828180887522, 0.9755477561619246, 0.968658212769802, 0.9772934232767444, 0.02008137171116598, 0.8231402412728427, 0.19719382125256552, 0.7230440112594069, 0.8735547217019456, 0.030384512059198107, 0.013673030426639149, 0.007596128014799527, 0.021269158441438677, 0.009115353617759432, 0.028865286456238204, 0.007596128014799527, 0.007596128014799527, 0.8718717177831001, 0.027718465016225727, 0.007559581368061561, 0.005039720912041041, 0.017639023192143645, 0.015119162736123123, 0.030238325472246246, 0.012599302280102602, 0.007559581368061561, 0.19356644574402587, 0.48391611436006465, 0.2903496686160388, 0.9664656447875367, 0.567365625759098, 0.36369591394813977, 0.058191346231702364, 0.249963073549489, 0.499926147098978, 0.21425406304241912, 0.8095918555603742, 0.1666806761447829, 0.925134227795764, 0.978382477519421, 0.46421194579285774, 0.1547373152642859, 0.1547373152642859, 0.09284238915857154, 0.061894926105714364, 0.061894926105714364, 0.8252642939122886, 0.8212051976810255, 0.09124502196455839, 0.01650175929146269, 0.004853458615136085, 0.03300351858292538, 0.000970691723027217, 0.01261899239935382, 0.005824150338163301, 0.0029120751690816507, 0.0029120751690816507, 0.003882766892108868, 0.003882766892108868, 0.8250906442417765, 0.8744008928112781, 0.9400617748278487, 0.3220009364206966, 0.2927281240188151, 0.3512737488225781, 0.3126637627718183, 0.6253275255436366, 0.9553786009414391, 0.6736400898357175, 0.31276147028086887, 0.6180510464396365, 0.012118647969404638, 0.012118647969404638, 0.04847459187761855, 0.266610255326902, 0.036355943908213915, 0.625612573832288, 0.35972722995356554, 0.9734159062866861, 0.34511386347464285, 0.5176707952119642, 0.1293328730334654, 0.7759972382007926, 0.0712573600998677, 0.0712573600998677, 0.17814340024966924, 0.4275441605992062, 0.08907170012483462, 0.053443020074900774, 0.053443020074900774, 0.017814340024966926, 0.8406708411839368, 0.1050838551479921, 0.0021892469822498354, 0.01970322284024852, 0.030649457751497695, 0.8859461108693634, 0.743536739866757, 0.9341672895147287, 0.8972965402873125, 0.895448842602606, 0.5789706177079611, 0.11436456646083183, 0.08577342484562388, 0.18584242049885172, 0.028591141615207958, 0.9480130491111328, 0.443762548422089, 0.2218812742110445, 0.2218812742110445, 0.08320547782914169, 0.8307023466200715, 0.4281304233870934, 0.5598628613523529, 0.7200543418117883, 0.4752493920305314, 0.3168329280203543, 0.2351582160112304, 0.7054746480336912, 0.6121484163247202, 0.1434722850761063, 0.057388914030442516, 0.03825927602029501, 0.1434722850761063, 0.8232047529180181, 0.0718396690024788, 0.23347892425805614, 0.43103801401487285, 0.06285971037716896, 0.053879751751859106, 0.04489979312654926, 0.0718396690024788, 0.0179599172506197, 0.29295452916781667, 0.32957384531379375, 0.08544507100727985, 0.04882575486130277, 0.158683703299234, 0.061032193576628466, 0.9030397119600995, 0.7949935694911512, 0.17666523766470024, 0.8089387585391159, 0.8774312403022483, 0.5211944548030335, 0.3822092668555579, 0.04517018608292957, 0.03127166728818201, 0.01389851879474756, 0.8285394304699942, 0.5889122359186693, 0.8239458510844881, 0.9415392719964245, 0.7639189909608624, 0.07872447616772246, 0.03207293473499804, 0.02624149205590749, 0.023325770716362212, 0.03790437741408859, 0.023325770716362212, 0.008747164018635829, 0.0029157213395452765, 0.2951434363391327, 0.25579097816058166, 0.1574098327142041, 0.2754672072498572, 0.8498927840084314, 0.18987143301227502, 0.6645500155429626, 0.12002918914099915, 0.14003405399783234, 0.1800437837114987, 0.10002432428416595, 0.38009243227983064, 0.040009729713666384, 0.3617672454439245, 0.10049090151220125, 0.5326017780146667, 0.20153365480723653, 0.671778849357455, 0.6642955009681976, 0.12078100017603591, 0.12078100017603591, 0.27384080385428106, 0.6389618756599891, 0.2879444936032556, 0.2879444936032556, 0.04113492765760794, 0.3496468850896675, 0.22532976503146016, 0.15905630472808951, 0.15905630472808951, 0.13254692060674128, 0.09278284442471889, 0.06627346030337064, 0.026509384121348255, 0.1458016126674154, 0.7635884427329507, 0.19309133034626338, 0.01316531797815432, 0.01755375730420576, 0.00877687865210288, 0.3309679772077161, 0.6619359544154322, 0.8263326722768197, 0.05183636689559386, 0.06403315910632182, 0.019819787342432945, 0.012196792210727966, 0.009147594158045974, 0.009147594158045974, 0.0015245990263409958, 0.004573797079022987, 0.1487672242423055, 0.07438361212115276, 0.595068896969222, 0.17356176161602307, 0.9026456277824162, 0.7454214072585162, 0.004778342354221258, 0.2484738024195054, 0.3316790205589181, 0.4201267593746296, 0.13267160822356724, 0.07739177146374757, 0.022111934703927874, 0.8751126015032438, 0.7270105665515729, 0.04132876605719536, 0.06575030963644717, 0.08829327294037191, 0.0056357408259811856, 0.015028642202616495, 0.0056357408259811856, 0.015028642202616495, 0.018785802753270617, 0.013150061927289432, 0.5889031384985914, 0.9102027252083591, 0.3312952534360813, 0.12423572003853048, 0.12423572003853048, 0.41411906679510163, 0.8450952834908441, 0.021546338672571094, 0.03830460208457084, 0.016758263411999742, 0.021546338672571094, 0.02633441393314245, 0.007182112890857031, 0.021546338672571094, 0.19445160056918534, 0.06481720018972845, 0.6319677018498523, 0.09722580028459267, 0.8859892650079131, 0.9207895133544008, 0.1843801473459208, 0.4964080890082483, 0.22692941211805637, 0.08509852954427113, 0.11717383183583702, 0.11717383183583702, 0.11717383183583702, 0.11717383183583702, 0.31246355156223204, 0.07811588789055801, 0.13670280380847652, 0.8832622504333707, 0.007886270093155096, 0.027601945326042834, 0.027601945326042834, 0.03154508037262038, 0.011829405139732643, 0.007886270093155096, 0.8978473135418285, 0.996579149312642, 0.7346560376644656, 0.04081422431469253, 0.13204601984165232, 0.021607530519543106, 0.03841338759029886, 0.012004183621968393, 0.007202510173181035, 0.007202510173181035, 0.004801673448787357, 0.5559261623243565, 0.1523085376231114, 0.02665399408404449, 0.030461707524622277, 0.045692561286933414, 0.10280826289560019, 0.015230853762311139, 0.02665399408404449, 0.007615426881155569, 0.019038567202888924, 0.015230853762311139, 0.18137910355161146, 0.7935335780383002, 0.7369458225360573, 0.24564860751201914, 0.8207618682761976, 0.08051777821696875, 0.010389390737673386, 0.018181433790928427, 0.036362867581856854, 0.018181433790928427, 0.00779204305325504, 0.00779204305325504, 0.3958151860604144, 0.1696350797401776, 0.3958151860604144, 0.8560413555291014, 0.0337277081915781, 0.043364196246314705, 0.011242569397192701, 0.014454732082104902, 0.0048182440273683005, 0.019272976109473202, 0.009636488054736601, 0.0048182440273683005, 0.9531681822489094, 0.9419543915349696, 0.41790672045950283, 0.5572089606126704, 0.5874033161741379, 0.8815691436978433, 0.23234907258619478, 0.006454140905172078, 0.14844524081895777, 0.006454140905172078, 0.03872484543103247, 0.5744185405603149, 0.5657911686727133, 0.3689942404387261, 0.024599616029248407, 0.9755200450078542, 0.009060557693571402, 0.012080743591428534, 0.801232518427635, 0.03230776283982399, 0.025846210271859193, 0.019384657703894394, 0.07753863081557757, 0.03230776283982399, 0.9373233722907739, 0.8251228588735462, 0.5099950992527406, 0.47221768449327833], \"Term\": [\"#\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"10\", \"10\", \"10\", \"10\", \"10\", \"11\", \"12th\", \"15\", \"15\", \"15\", \"15\", \"16\", \"16\", \"17\", \"1st\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"20\", \"20\", \"20\", \"20\", \"20\", \"21\", \"2k\", \"2nd\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"30\", \"30\", \"3840x1080\", \"3rd\", \"3x\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"40\", \"40\", \"5\", \"5\", \"5\", \"5\", \"5\", \"5\", \"5\\u20ac\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6hr\", \"6th\", \"7\", \"7\", \"7\", \"7\", \"7\", \"7\", \"7\", \"8\", \"8\", \"8\", \"8\", \"8\", \"9\", \"9\", \"9\", \"<\", \"<\", \"aaanyway\", \"abil\", \"abil\", \"abil\", \"abil\", \"accid\", \"achiev\", \"achiev\", \"achiev\", \"achiev\", \"achiev\", \"achiev\", \"achiev\", \"achiev\", \"achiev\", \"achiev\", \"acquir\", \"acquir\", \"acquir\", \"act\", \"act\", \"act\", \"act\", \"activ\", \"activ\", \"activ\", \"activ\", \"actual\", \"actual\", \"actual\", \"actual\", \"actual\", \"actual\", \"actual\", \"actual\", \"ad\", \"ad\", \"ad\", \"ad\", \"ad\", \"ad\", \"ad\", \"addict\", \"adjac\", \"advanc\", \"advis\", \"ai\", \"ai\", \"alarm\", \"albeit\", \"albeit\", \"alert\", \"alert\", \"allot\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"although\", \"although\", \"although\", \"amaz\", \"ancient\", \"annoy\", \"annoy\", \"answer\", \"antfarm\", \"anyon\", \"anyon\", \"anywher\", \"anywher\", \"apm\", \"appear\", \"appear\", \"appear\", \"appreci\", \"appreci\", \"appreci\", \"approx\", \"arab\", \"arab\", \"arabesqu\", \"arent\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"artsi\", \"ask\", \"ask\", \"ask\", \"ask\", \"aspect\", \"aspect\", \"aspect\", \"aspect\", \"aspect\", \"ass\", \"ass\", \"asset\", \"assign\", \"attack\", \"attack\", \"attack\", \"attent\", \"attent\", \"attent\", \"attent\", \"averag\", \"aw\", \"aw\", \"awesom\", \"b\", \"b\", \"b\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"backdrop\", \"background\", \"background\", \"background\", \"bad\", \"bad\", \"balanc\", \"balanc\", \"balanc\", \"balanc\", \"balanc\", \"balanc\", \"balanc\", \"balanc\", \"balanc\", \"balanc\", \"balanc\", \"balanc\", \"bang\", \"banner\", \"bar\", \"bar\", \"bar\", \"bare\", \"bartend\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"basi\", \"basi\", \"bear\", \"bear\", \"beat\", \"beat\", \"beat\", \"beat\", \"beat\", \"beat\", \"beat\", \"beaten\", \"beaten\", \"beauti\", \"beauti\", \"beauti\", \"beauti\", \"beauti\", \"beauti\", \"beauti\", \"beauti\", \"beauti\", \"becam\", \"becam\", \"beer\", \"began\", \"believ\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"bethesda\", \"better\", \"better\", \"better\", \"better\", \"bias\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"bigger\", \"bigger\", \"bind\", \"bind\", \"biolog\", \"bit\", \"bit\", \"bit\", \"bit\", \"bit\", \"bit\", \"bit\", \"bit\", \"bit\", \"bit\", \"block\", \"blow\", \"blow\", \"blown\", \"blueprint\", \"bob\", \"bobbl\", \"bodi\", \"bolster\", \"bolt\", \"bone\", \"bonus\", \"boom\", \"boon\", \"border\", \"borderlin\", \"bought\", \"bought\", \"bounc\", \"box\", \"box\", \"brain\", \"brain\", \"branch\", \"branch\", \"breed\", \"brief\", \"brilliantli\", \"brilliantli\", \"brilliantli\", \"bring\", \"bring\", \"buck\", \"buck\", \"buck\", \"budget\", \"buggi\", \"buggo\", \"buggo\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"build\", \"builder\", \"builder\", \"builder\", \"builder\", \"builder\", \"builder\", \"builder\", \"builder\", \"builder\", \"builder\", \"builder\", \"builder\", \"built\", \"built\", \"built\", \"built\", \"built\", \"bum\", \"buri\", \"buri\", \"burn\", \"burn\", \"burn\", \"butter\", \"butter\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"calm\", \"calm\", \"calm\", \"came\", \"came\", \"campaign\", \"campaign\", \"campaign\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"cancel\", \"cannon\", \"cannon\", \"cant\", \"cant\", \"cant\", \"cant\", \"captur\", \"captur\", \"care\", \"care\", \"care\", \"carri\", \"carri\", \"case\", \"case\", \"cat\", \"caught\", \"caught\", \"cent\", \"center\", \"center\", \"center\", \"certain\", \"certain\", \"certain\", \"certain\", \"certain\", \"certainli\", \"certainli\", \"chain\", \"chain\", \"chaotic\", \"charact\", \"charact\", \"charact\", \"charact\", \"charact\", \"charact\", \"charact\", \"charact\", \"charact\", \"charg\", \"charg\", \"charg\", \"charm\", \"charm\", \"charm\", \"chase\", \"chat\", \"cheat\", \"check\", \"check\", \"cheeri\", \"chill\", \"chill\", \"chill\", \"chill\", \"chill\", \"chill\", \"choic\", \"choic\", \"choic\", \"choic\", \"choic\", \"choic\", \"choic\", \"choic\", \"choos\", \"chose\", \"citi\", \"citi\", \"citi\", \"citi\", \"citi\", \"citi\", \"citi\", \"citi\", \"citi\", \"citi\", \"citi\", \"citi\", \"citi\", \"civil\", \"closer\", \"closer\", \"closest\", \"clunki\", \"clunki\", \"clunki\", \"cluster\", \"color\", \"color\", \"colour\", \"com\", \"combat\", \"combat\", \"combat\", \"combat\", \"combin\", \"combin\", \"combin\", \"combin\", \"come\", \"come\", \"come\", \"comfort\", \"comfort\", \"comfort\", \"comfort\", \"comfort\", \"comment\", \"comment\", \"comment\", \"common\", \"common\", \"commun\", \"commun\", \"commun\", \"commun\", \"complain\", \"complain\", \"complaint\", \"complet\", \"complet\", \"complet\", \"complet\", \"complet\", \"complet\", \"complet\", \"complet\", \"complet\", \"completionist\", \"complic\", \"con\", \"con\", \"con\", \"concept\", \"concept\", \"concept\", \"concept\", \"concept\", \"concept\", \"concept\", \"conclus\", \"confirm\", \"confirm\", \"connect\", \"connect\", \"consist\", \"construct\", \"control\", \"control\", \"cool\", \"cool\", \"cool\", \"cool\", \"cool\", \"cool\", \"cool\", \"cool\", \"cool\", \"cooler\", \"corner\", \"correct\", \"cosmet\", \"cosmet\", \"cosmet\", \"cosmet\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"couldnt\", \"count\", \"countless\", \"coupl\", \"coupl\", \"coupl\", \"coupl\", \"cours\", \"cours\", \"cours\", \"cours\", \"creat\", \"creat\", \"creat\", \"creat\", \"creat\", \"creat\", \"creator\", \"credit\", \"credit\", \"credit\", \"critiqu\", \"cruel\", \"cup\", \"curat\", \"currenc\", \"current\", \"cursor\", \"cursor\", \"curv\", \"curv\", \"custom\", \"custom\", \"custom\", \"custom\", \"customis\", \"customis\", \"cutoff\", \"dark\", \"dark\", \"dark\", \"dark\", \"dead\", \"dead\", \"debri\", \"decent\", \"decent\", \"decent\", \"decent\", \"decent\", \"deconstruct\", \"deep\", \"deep\", \"deep\", \"deep\", \"degrad\", \"delight\", \"demo\", \"demolish\", \"depend\", \"depend\", \"depth\", \"depth\", \"depth\", \"depth\", \"descript\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"detail\", \"detail\", \"detail\", \"detail\", \"detail\", \"detail\", \"detail\", \"detail\", \"detail\", \"dev\", \"dev\", \"dev\", \"dev\", \"dev\", \"develop\", \"develop\", \"develop\", \"develop\", \"dick\", \"die\", \"die\", \"die\", \"die\", \"difficult\", \"difficult\", \"difficulti\", \"difficulti\", \"difficulti\", \"difficulti\", \"difficulti\", \"difficulti\", \"difficulti\", \"difficulti\", \"disagre\", \"disclaim\", \"discord\", \"discord\", \"discourag\", \"display\", \"distanc\", \"distanc\", \"divid\", \"divid\", \"dlc\", \"doesn\", \"doesnt\", \"dollar\", \"done\", \"done\", \"done\", \"done\", \"done\", \"done\", \"done\", \"done\", \"doom\", \"downsid\", \"downsid\", \"dr\", \"dr\", \"draw\", \"draw\", \"drift\", \"drink\", \"drive\", \"drive\", \"driven\", \"due\", \"due\", \"due\", \"due\", \"dull\", \"dumb\", \"dumb\", \"ear\", \"earli\", \"earli\", \"earli\", \"earli\", \"earli\", \"earn\", \"earth\", \"easi\", \"easi\", \"easi\", \"easi\", \"easi\", \"easi\", \"easi\", \"easi\", \"easi\", \"easi\", \"easi\", \"easi\", \"easier\", \"easier\", \"easier\", \"easier\", \"easili\", \"easili\", \"easili\", \"easili\", \"easili\", \"easili\", \"easili\", \"eastern\", \"edg\", \"edg\", \"edg\", \"edgi\", \"effect\", \"effect\", \"effect\", \"eg\", \"eg\", \"egg\", \"eight\", \"eight\", \"els\", \"els\", \"els\", \"els\", \"els\", \"emot\", \"empti\", \"emptier\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"endless\", \"enemi\", \"enemi\", \"enemi\", \"enemi\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enough\", \"enough\", \"enough\", \"enough\", \"enough\", \"enough\", \"enough\", \"enough\", \"enough\", \"enough\", \"ensur\", \"entertain\", \"entertain\", \"entertain\", \"entertain\", \"entir\", \"entir\", \"entir\", \"entir\", \"entir\", \"epic\", \"epic\", \"epic\", \"epic\", \"equal\", \"escap\", \"escap\", \"etc\", \"etc\", \"etc\", \"etc\", \"etc\", \"etc\", \"etc\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"event\", \"event\", \"eventu\", \"eventu\", \"eventu\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"everi\", \"everi\", \"everi\", \"everi\", \"everi\", \"everi\", \"everi\", \"everi\", \"everi\", \"everi\", \"everybodi\", \"everyth\", \"everyth\", \"everyth\", \"everyth\", \"everyth\", \"everyth\", \"everyth\", \"evil\", \"evok\", \"evolut\", \"evolut\", \"exclus\", \"expand\", \"expand\", \"expand\", \"expans\", \"expans\", \"expect\", \"expect\", \"expect\", \"expect\", \"expect\", \"expect\", \"expect\", \"explos\", \"fab\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"factorio\", \"fairli\", \"fairli\", \"fairli\", \"fan\", \"fan\", \"fan\", \"fan\", \"fanci\", \"fanstast\", \"fantasi\", \"fantasi\", \"fantast\", \"fantast\", \"fantast\", \"fantast\", \"fare\", \"fascin\", \"fascin\", \"fast\", \"fast\", \"fast\", \"fast\", \"fastest\", \"favourit\", \"favourit\", \"feedback\", \"feedback\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"fell\", \"fenc\", \"fidel\", \"fieri\", \"fight\", \"fight\", \"fight\", \"fight\", \"fight\", \"figur\", \"figur\", \"figur\", \"figur\", \"figur\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"finish\", \"finish\", \"finish\", \"finish\", \"finish\", \"finish\", \"finish\", \"finish\", \"finish\", \"finish\", \"fire\", \"fire\", \"fire\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"fix\", \"fix\", \"fix\", \"fix\", \"flesh\", \"fli\", \"fli\", \"fli\", \"fli\", \"fli\", \"fli\", \"fli\", \"fli\", \"fli\", \"float\", \"float\", \"float\", \"floor\", \"floor\", \"flotsam\", \"focu\", \"focu\", \"focu\", \"food\", \"food\", \"food\", \"food\", \"food\", \"forget\", \"forget\", \"forget\", \"forgiv\", \"forgot\", \"forward\", \"forward\", \"forward\", \"forward\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"four\", \"four\", \"four\", \"freak\", \"free\", \"free\", \"free\", \"free\", \"free\", \"friend\", \"friend\", \"frostpunk\", \"frostpunk\", \"fuel\", \"fuel\", \"fuel\", \"fulfil\", \"fulfil\", \"fulfil\", \"full\", \"full\", \"full\", \"full\", \"full\", \"fun\", \"fun\", \"fun\", \"fun\", \"function\", \"function\", \"funni\", \"funni\", \"funni\", \"funni\", \"furnitur\", \"furthermor\", \"futur\", \"futur\", \"futur\", \"gain\", \"gain\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"gameplay\", \"gameplay\", \"gameplay\", \"gameplay\", \"gameplay\", \"gameplay\", \"gameplay\", \"gameplay\", \"gameplay\", \"gameplay\", \"gameplay\", \"gameplay\", \"gave\", \"gave\", \"gear\", \"gem\", \"gener\", \"gener\", \"gener\", \"gener\", \"gener\", \"gener\", \"gener\", \"genr\", \"genr\", \"genr\", \"genr\", \"genr\", \"genr\", \"genuin\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"gg\", \"gg\", \"ghibli\", \"gimmick\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"glass\", \"glass\", \"global\", \"global\", \"global\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"gone\", \"gone\", \"gone\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"got\", \"got\", \"got\", \"got\", \"got\", \"got\", \"got\", \"got\", \"goti\", \"grab\", \"grab\", \"grab\", \"grand\", \"graphic\", \"graphic\", \"graphic\", \"graphic\", \"graphic\", \"graphic\", \"graphic\", \"graphic\", \"grate\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"green\", \"green\", \"green\", \"grew\", \"grind\", \"grind\", \"ground\", \"ground\", \"ground\", \"ground\", \"gruel\", \"guess\", \"guess\", \"guess\", \"guess\", \"gui\", \"gui\", \"gush\", \"half\", \"half\", \"half\", \"hang\", \"hang\", \"happili\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hardcor\", \"hardcor\", \"harder\", \"harder\", \"hardest\", \"hardest\", \"harsh\", \"head\", \"head\", \"head\", \"head\", \"hefti\", \"hell\", \"hell\", \"hell\", \"hell\", \"hell\", \"hesit\", \"high\", \"high\", \"high\", \"high\", \"highfleet\", \"highli\", \"highli\", \"highli\", \"highli\", \"hint\", \"hint\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hive\", \"hive\", \"hivemind\", \"hmm\", \"hord\", \"hot\", \"hotkey\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hous\", \"hous\", \"hous\", \"hr\", \"hr\", \"http\", \"huge\", \"huge\", \"huge\", \"hunter\", \"im\", \"im\", \"im\", \"impend\", \"impress\", \"impress\", \"impress\", \"impress\", \"incom\", \"incorpor\", \"incred\", \"incred\", \"indefinit\", \"infantri\", \"infantri\", \"infin\", \"infin\", \"inform\", \"inform\", \"ingam\", \"inhabit\", \"innov\", \"insid\", \"insid\", \"instead\", \"instead\", \"instead\", \"instead\", \"instead\", \"instruct\", \"intens\", \"interconnect\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"internet\", \"internet\", \"internet\", \"introduct\", \"introduct\", \"intuit\", \"intuit\", \"intuit\", \"intuit\", \"invest\", \"invis\", \"isn\", \"isol\", \"isol\", \"issu\", \"issu\", \"issu\", \"issu\", \"issu\", \"item\", \"item\", \"item\", \"ive\", \"joke\", \"judg\", \"justifi\", \"justifi\", \"justifi\", \"kind\", \"kind\", \"kind\", \"kind\", \"kind\", \"kinda\", \"kinda\", \"kinda\", \"kinda\", \"kinda\", \"kinda\", \"kinda\", \"kinda\", \"kingdom\", \"kingdom\", \"kingdom\", \"kingdom\", \"kingdom\", \"kingdom\", \"knew\", \"knew\", \"knew\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"knowledg\", \"kudo\", \"lack\", \"lack\", \"lack\", \"lack\", \"lack\", \"lack\", \"landlubb\", \"landscap\", \"larg\", \"larg\", \"larg\", \"larg\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learnt\", \"least\", \"least\", \"least\", \"least\", \"least\", \"least\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"legend\", \"legit\", \"legit\", \"length\", \"librari\", \"life\", \"life\", \"life\", \"life\", \"lightn\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"limit\", \"limit\", \"limit\", \"limit\", \"limit\", \"limit\", \"limit\", \"littl\", \"littl\", \"littl\", \"littl\", \"littl\", \"littl\", \"littl\", \"littl\", \"littl\", \"littl\", \"littl\", \"littl\", \"littl\", \"littl\", \"littl\", \"live\", \"live\", \"live\", \"live\", \"live\", \"load\", \"load\", \"local\", \"local\", \"log\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"longev\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"lore\", \"lore\", \"lore\", \"lose\", \"lose\", \"lose\", \"lose\", \"lose\", \"loss\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"love\", \"love\", \"love\", \"lowest\", \"m1\", \"mac\", \"machin\", \"machin\", \"made\", \"made\", \"made\", \"made\", \"made\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"manag\", \"manag\", \"manag\", \"manag\", \"manag\", \"manag\", \"manag\", \"manag\", \"manag\", \"manag\", \"manag\", \"mani\", \"mani\", \"mani\", \"mani\", \"mani\", \"manufactur\", \"manufactur\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"mate\", \"mate\", \"matter\", \"matter\", \"matter\", \"matter\", \"meant\", \"meat\", \"meat\", \"mediev\", \"mediocr\", \"mediocr\", \"meet\", \"meet\", \"meet\", \"meh\", \"meh\", \"memor\", \"mental\", \"messag\", \"micromanag\", \"mile\", \"mile\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"minus\", \"miss\", \"miss\", \"miss\", \"miss\", \"mistak\", \"mistak\", \"model\", \"model\", \"modern\", \"modern\", \"modifi\", \"money\", \"money\", \"money\", \"money\", \"money\", \"money\", \"moni\", \"moral\", \"mostli\", \"mostli\", \"mostli\", \"mostli\", \"mostli\", \"mostli\", \"mostli\", \"mould\", \"movi\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"multipl\", \"multipl\", \"multipl\", \"multipl\", \"multipl\", \"music\", \"music\", \"music\", \"music\", \"music\", \"music\", \"music\", \"music\", \"music\", \"music\", \"music\", \"mysteri\", \"mysteri\", \"n\", \"nail\", \"nail\", \"narr\", \"narr\", \"natur\", \"natur\", \"natur\", \"natur\", \"natur\", \"natur\", \"natur\", \"navig\", \"navig\", \"nearli\", \"nearli\", \"neat\", \"neat\", \"neat\", \"neat\", \"necessarili\", \"necessarili\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"newer\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nomin\", \"non\", \"non\", \"nonetheless\", \"nonetheless\", \"nope\", \"nostalg\", \"nostalg\", \"not\", \"not\", \"not\", \"not\", \"not\", \"noth\", \"noth\", \"noth\", \"noth\", \"noth\", \"nowher\", \"npc\", \"npc\", \"nuisanc\", \"number\", \"number\", \"nurtur\", \"obviou\", \"obvious\", \"occupi\", \"occur\", \"occur\", \"ocd\", \"oddli\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"oh\", \"oh\", \"oh\", \"oh\", \"ok\", \"ok\", \"ok\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"onto\", \"oop\", \"open\", \"open\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"opportun\", \"oppos\", \"optim\", \"optim\", \"optim\", \"option\", \"option\", \"option\", \"option\", \"option\", \"option\", \"option\", \"option\", \"ordinari\", \"orient\", \"origin\", \"origin\", \"origin\", \"outcom\", \"outfit\", \"output\", \"outstand\", \"overal\", \"overal\", \"overal\", \"overal\", \"overal\", \"overal\", \"overal\", \"overrun\", \"overstay\", \"packag\", \"paint\", \"paint\", \"paint\", \"paint\", \"paint\", \"pair\", \"paper\", \"paradis\", \"paradis\", \"part\", \"part\", \"pass\", \"pass\", \"patch\", \"pattern\", \"paus\", \"paus\", \"peac\", \"peac\", \"peac\", \"peac\", \"peasant\", \"peasant\", \"penal\", \"peopl\", \"peopl\", \"peopl\", \"peopl\", \"peopl\", \"peopl\", \"peopl\", \"per\", \"per\", \"per\", \"percentag\", \"philosophis\", \"pictur\", \"pictur\", \"pile\", \"pile\", \"pirat\", \"pirat\", \"pirat\", \"piss\", \"platform\", \"platform\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"player\", \"player\", \"player\", \"player\", \"player\", \"player\", \"playthrough\", \"playthrough\", \"playthrough\", \"playthrough\", \"playtim\", \"pleas\", \"pleas\", \"pleas\", \"pleasant\", \"pleasantli\", \"plu\", \"plu\", \"plu\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"pointless\", \"pointless\", \"poli\", \"polish\", \"polish\", \"polish\", \"portrait\", \"power\", \"power\", \"power\", \"power\", \"power\", \"practic\", \"predict\", \"premis\", \"preset\", \"pressur\", \"pressur\", \"pretti\", \"pretti\", \"pretti\", \"pretti\", \"pretti\", \"pretti\", \"pretti\", \"pretti\", \"pretti\", \"pretti\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"pricey\", \"prici\", \"priorit\", \"pro\", \"pro\", \"pro\", \"probabl\", \"probabl\", \"probabl\", \"probabl\", \"probabl\", \"probabl\", \"probabl\", \"probabl\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"produc\", \"produc\", \"produc\", \"profil\", \"program\", \"program\", \"properli\", \"properli\", \"properli\", \"propheci\", \"pummel\", \"punish\", \"purchas\", \"purchas\", \"purchas\", \"purchas\", \"purchas\", \"pure\", \"pure\", \"pure\", \"put\", \"put\", \"put\", \"put\", \"quaint\", \"qualiti\", \"qualiti\", \"quest\", \"quest\", \"quest\", \"quest\", \"quest\", \"quest\", \"quest\", \"quest\", \"quest\", \"quest\", \"question\", \"question\", \"questlin\", \"quick\", \"quirk\", \"quirki\", \"quirki\", \"quirki\", \"quiz\", \"quot\", \"race\", \"race\", \"rais\", \"randomli\", \"randomli\", \"rang\", \"rang\", \"rang\", \"rate\", \"rate\", \"raw\", \"reaction\", \"reaction\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"realis\", \"realli\", \"realli\", \"realli\", \"realli\", \"realli\", \"realli\", \"rebel\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"reflect\", \"refresh\", \"refresh\", \"refresh\", \"refresh\", \"refund\", \"refund\", \"refund\", \"refund\", \"refund\", \"refund\", \"regard\", \"regret\", \"regret\", \"regrow\", \"regular\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"relic\", \"reliev\", \"reload\", \"remain\", \"rememb\", \"rememb\", \"rend\", \"renew\", \"replac\", \"replac\", \"replac\", \"replay\", \"replay\", \"replay\", \"replay\", \"replay\", \"replay\", \"replay\", \"replay\", \"request\", \"requir\", \"requir\", \"requir\", \"requir\", \"requir\", \"requir\", \"requir\", \"requir\", \"requir\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"reset\", \"reset\", \"resolut\", \"ressourc\", \"rich\", \"rich\", \"ridicul\", \"right\", \"right\", \"right\", \"rimworld\", \"rins\", \"risk\", \"rng\", \"road\", \"roam\", \"rock\", \"rock\", \"rock\", \"roleplay\", \"round\", \"round\", \"rout\", \"rout\", \"rout\", \"rpg\", \"rpg\", \"rt\", \"rt\", \"rt\", \"rt\", \"rt\", \"rt\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"rush\", \"rush\", \"safe\", \"safe\", \"said\", \"said\", \"sale\", \"sale\", \"sale\", \"sale\", \"sale\", \"sale\", \"sale\", \"sandbox\", \"sandbox\", \"sandbox\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"scarc\", \"scaveng\", \"scaveng\", \"scenario\", \"scope\", \"scour\", \"scratch\", \"scratch\", \"screen\", \"screen\", \"secret\", \"secret\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"seek\", \"seek\", \"seem\", \"seem\", \"seem\", \"seem\", \"seem\", \"seem\", \"seem\", \"seem\", \"seem\", \"seem\", \"send\", \"send\", \"send\", \"sequel\", \"sequel\", \"sequel\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"shake\", \"shake\", \"ship\", \"ship\", \"ship\", \"shit\", \"shit\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"shortcut\", \"side\", \"side\", \"side\", \"side\", \"side\", \"sigh\", \"sign\", \"sign\", \"signifi\", \"signific\", \"significantli\", \"simciti\", \"simciti\", \"similar\", \"similar\", \"similar\", \"simplist\", \"simplist\", \"simul\", \"simul\", \"simul\", \"simul\", \"singl\", \"singl\", \"singl\", \"singl\", \"singl\", \"sit\", \"sit\", \"sit\", \"sit\", \"sky\", \"sky\", \"sky\", \"sky\", \"sky\", \"skylin\", \"skylin\", \"skylin\", \"slightli\", \"slightli\", \"slightli\", \"slightli\", \"slow\", \"slow\", \"slow\", \"slow\", \"slow\", \"slower\", \"slower\", \"slower\", \"slowli\", \"slowli\", \"smoothli\", \"snap\", \"snap\", \"snobbish\", \"sock\", \"softlock\", \"soldier\", \"someday\", \"somehow\", \"somehow\", \"someth\", \"someth\", \"someth\", \"someth\", \"someth\", \"someth\", \"someth\", \"someth\", \"someth\", \"someth\", \"someth\", \"someth\", \"sometim\", \"sometim\", \"sometim\", \"somewhat\", \"somewhat\", \"somewhat\", \"sort\", \"sort\", \"sort\", \"sort\", \"sort\", \"sorta\", \"space\", \"space\", \"space\", \"spam\", \"spam\", \"spare\", \"spare\", \"spike\", \"split\", \"split\", \"spoiler\", \"spoiler\", \"spoiler\", \"spread\", \"ssd\", \"stabl\", \"stack\", \"stack\", \"stack\", \"stage\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"standard\", \"starcracft\", \"starv\", \"state\", \"statement\", \"statement\", \"static\", \"stay\", \"stay\", \"stay\", \"stay\", \"stay\", \"stay\", \"steampow\", \"steep\", \"steril\", \"stick\", \"stick\", \"still\", \"still\", \"still\", \"still\", \"stood\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stori\", \"stori\", \"stori\", \"stori\", \"stori\", \"stori\", \"stori\", \"stori\", \"storm\", \"storylin\", \"storytel\", \"stretch\", \"strictli\", \"strong\", \"stronger\", \"stronger\", \"stuff\", \"stuff\", \"stuff\", \"stuff\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"stylist\", \"subsequ\", \"success\", \"sultanwav\", \"sum\", \"sunset\", \"super\", \"super\", \"super\", \"suppos\", \"suppos\", \"surprisingli\", \"surprisingli\", \"surviv\", \"surviv\", \"surviv\", \"surviv\", \"surviv\", \"surviv\", \"sustain\", \"sweet\", \"sweet\", \"sweet\", \"system\", \"system\", \"system\", \"system\", \"system\", \"t\", \"t\", \"tabl\", \"tabl\", \"tabl\", \"tackl\", \"tactic\", \"tactic\", \"tad\", \"tag\", \"tag\", \"tag\", \"tailor\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"tamat\", \"tast\", \"tast\", \"tax\", \"tax\", \"tax\", \"td\", \"tea\", \"team\", \"team\", \"team\", \"team\", \"tech\", \"tech\", \"tech\", \"tech\", \"tech\", \"tediou\", \"tend\", \"thank\", \"thank\", \"theorycraft\", \"therefor\", \"therefor\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"thoroughli\", \"thoroughli\", \"thoroughli\", \"threat\", \"three\", \"three\", \"three\", \"throughout\", \"throughout\", \"throughout\", \"throw\", \"throw\", \"thrown\", \"till\", \"tilt\", \"tilt\", \"tilt\", \"tilt\", \"tilt\", \"tilt\", \"timberborn\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tin\", \"tip\", \"tire\", \"titl\", \"titl\", \"titl\", \"tl\", \"tl\", \"tldr\", \"ton\", \"ton\", \"took\", \"took\", \"took\", \"took\", \"took\", \"took\", \"total\", \"total\", \"touch\", \"travers\", \"travers\", \"treat\", \"treat\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tri\", \"tri\", \"tri\", \"tri\", \"tri\", \"tricki\", \"tripl\", \"trophi\", \"tune\", \"tweak\", \"type\", \"type\", \"type\", \"type\", \"type\", \"uh\", \"ultim\", \"ultim\", \"ultim\", \"ultim\", \"ultrawid\", \"un\", \"un\", \"unclear\", \"uncov\", \"uncov\", \"underground\", \"underground\", \"understand\", \"understand\", \"understand\", \"understand\", \"understand\", \"unintent\", \"uniqu\", \"uniqu\", \"uniqu\", \"uniqu\", \"uniqu\", \"uniqu\", \"uniqu\", \"uniqu\", \"unlock\", \"unlock\", \"unlock\", \"unlock\", \"unlock\", \"unlock\", \"unreli\", \"unus\", \"unus\", \"unwind\", \"upcom\", \"updat\", \"updat\", \"updat\", \"updat\", \"updat\", \"upset\", \"urgenc\", \"url\", \"usag\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"usual\", \"usual\", \"usual\", \"usual\", \"ux\", \"va\", \"va\", \"valu\", \"valu\", \"valu\", \"valu\", \"valu\", \"valu\", \"version\", \"version\", \"version\", \"via\", \"via\", \"vibe\", \"vibe\", \"vibe\", \"victori\", \"victori\", \"view\", \"view\", \"view\", \"view\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"wait\", \"wait\", \"wait\", \"wait\", \"wait\", \"walk\", \"walk\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"war\", \"war\", \"war\", \"war\", \"warm\", \"watch\", \"watch\", \"watch\", \"water\", \"water\", \"water\", \"water\", \"water\", \"wave\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"waypoint\", \"weather\", \"welcom\", \"welcom\", \"welcom\", \"welcom\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"whether\", \"whether\", \"whether\", \"whether\", \"whim\", \"wholesom\", \"wing\", \"wing\", \"wing\", \"wing\", \"within\", \"within\", \"within\", \"within\", \"within\", \"within\", \"within\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"won\", \"word\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"worri\", \"worri\", \"wors\", \"wors\", \"worth\", \"worth\", \"worth\", \"worth\", \"worth\", \"worth\", \"worth\", \"worth\", \"worthwhil\", \"worthwhil\", \"worthwhil\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"wow\", \"written\", \"x\", \"x\", \"xxx\", \"yah\", \"ye\", \"ye\", \"ye\", \"ye\", \"ye\", \"ye\", \"yeah\", \"yeah\", \"yeah\", \"year\", \"year\", \"year\", \"yet\", \"yet\", \"yet\", \"yet\", \"yet\", \"yet\", \"zen\", \"zerg\", \"\\u2019\", \"\\u2019\"]}, \"R\": 25, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [18, 10, 12, 21, 7, 5, 3, 9, 8, 25, 11, 24, 15, 17, 2, 13, 16, 19, 1, 22, 20, 14, 23, 4, 6]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el257223550595436166552096264\", ldavis_el257223550595436166552096264_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el257223550595436166552096264\", ldavis_el257223550595436166552096264_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el257223550595436166552096264\", ldavis_el257223550595436166552096264_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "17     0.178718  0.318674       1        1  52.876451\n",
       "9      0.418952 -0.280974       2        1   7.425169\n",
       "11    -0.181205  0.316424       3        1   5.490581\n",
       "20    -0.068167  0.376835       4        1   4.655893\n",
       "6      0.298594 -0.000030       5        1   3.140493\n",
       "4      0.429367  0.082145       6        1   2.863348\n",
       "2      0.071060  0.344031       7        1   2.844863\n",
       "8     -0.325595 -0.239676       8        1   2.684957\n",
       "7     -0.350791  0.215185       9        1   2.595288\n",
       "24     0.150390  0.144459      10        1   2.264626\n",
       "10     0.285963  0.186567      11        1   1.846477\n",
       "23     0.003945  0.191181      12        1   1.642897\n",
       "14    -0.364156 -0.053885      13        1   1.224858\n",
       "16    -0.113485  0.121223      14        1   1.017347\n",
       "1     -0.088983 -0.425992      15        1   1.006641\n",
       "12    -0.307085  0.078016      16        1   0.934279\n",
       "15     0.061949 -0.111539      17        1   0.855040\n",
       "18    -0.141465  0.009088      18        1   0.812128\n",
       "0      0.038985  0.036767      19        1   0.774847\n",
       "21     0.210866 -0.147238      20        1   0.717621\n",
       "19    -0.198663 -0.119370      21        1   0.710268\n",
       "13     0.035511 -0.318093      22        1   0.545958\n",
       "22     0.133052 -0.288145      23        1   0.431854\n",
       "3     -0.044102 -0.191524      24        1   0.386849\n",
       "5     -0.133654 -0.244129      25        1   0.251267, topic_info=         Term         Freq        Total Category  logprob  loglift\n",
       "187      game  3567.000000  3567.000000  Default  25.0000  25.0000\n",
       "34       play  1693.000000  1693.000000  Default  24.0000  24.0000\n",
       "462       fun  1212.000000  1212.000000  Default  23.0000  23.0000\n",
       "253      love   810.000000   810.000000  Default  22.0000  22.0000\n",
       "1073     word   379.000000   379.000000  Default  21.0000  21.0000\n",
       "...       ...          ...          ...      ...      ...      ...\n",
       "12    complet     1.297844   323.219989  Topic25  -5.5358   0.4688\n",
       "1536     mind     0.512345   125.969717  Topic25  -6.4652   0.4816\n",
       "308    pretti     1.016152   367.684113  Topic25  -5.7805   0.0952\n",
       "150      even     1.309139   640.520139  Topic25  -5.5271  -0.2065\n",
       "27       make     1.023337   785.341977  Topic25  -5.7734  -0.6566\n",
       "\n",
       "[1166 rows x 6 columns], token_table=      Topic      Freq  Term\n",
       "term                       \n",
       "1010      9  0.881242     #\n",
       "432       1  0.388054     1\n",
       "432       2  0.173239     1\n",
       "432       3  0.103943     1\n",
       "432       5  0.242534     1\n",
       "...     ...       ...   ...\n",
       "431      21  0.032308   yet\n",
       "1519     11  0.937323   zen\n",
       "1631     13  0.825123  zerg\n",
       "643       1  0.509995     \n",
       "643       5  0.472218     \n",
       "\n",
       "[2864 rows x 3 columns], R=25, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[18, 10, 12, 21, 7, 5, 3, 9, 8, 25, 11, 24, 15, 17, 2, 13, 16, 19, 1, 22, 20, 14, 23, 4, 6])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_by_genre['simulation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "820906dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "# Making Topic models for each genre & rating\n",
    "# genre = df.genre.unique().tolist()\n",
    "voted_up = [True, False]\n",
    "lda_by_vote = dict()\n",
    "for v in voted_up:\n",
    "    subset = df[df['voted_up'] == v]\n",
    "    subset_fd = FrequencyDistribution(subset.review_stemmed)\n",
    "    vis = lda_vis(subset_fd)\n",
    "    lda_by_vote[v] = vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e7ab5e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "3     -0.187436  0.511818       1        1  44.394761\n",
       "16    -0.488663  0.198013       2        1  37.098949\n",
       "11    -0.358684  0.356177       3        1   5.901905\n",
       "21     0.340081  0.371000       4        1   1.985452\n",
       "12    -0.243736 -0.418907       5        1   1.141217\n",
       "7      0.482882  0.067892       6        1   1.081455\n",
       "22     0.056733  0.408814       7        1   1.062607\n",
       "23     0.222999  0.313528       8        1   0.990539\n",
       "1     -0.410787 -0.109142       9        1   0.709534\n",
       "13     0.287705 -0.299217      10        1   0.562428\n",
       "4     -0.001196 -0.422381      11        1   0.547415\n",
       "9     -0.291028 -0.215934      12        1   0.488357\n",
       "18     0.353229 -0.141430      13        1   0.425398\n",
       "14    -0.059549  0.228315      14        1   0.405959\n",
       "8      0.142572 -0.338607      15        1   0.397672\n",
       "17    -0.264921  0.027444      16        1   0.396442\n",
       "2      0.022807 -0.256846      17        1   0.365552\n",
       "24     0.288412  0.036232      18        1   0.359384\n",
       "19     0.064223  0.118933      19        1   0.335155\n",
       "20     0.205246 -0.145014      20        1   0.325952\n",
       "0     -0.132810 -0.220167      21        1   0.325125\n",
       "5     -0.107873  0.057473      22        1   0.234494\n",
       "6      0.139317  0.009508      23        1   0.203193\n",
       "15    -0.077946 -0.070995      24        1   0.143890\n",
       "10     0.018421 -0.066506      25        1   0.117162, topic_info=        Term          Freq         Total Category  logprob  loglift\n",
       "57      game  18479.000000  18479.000000  Default  25.0000  25.0000\n",
       "263    earli   1749.000000   1749.000000  Default  24.0000  24.0000\n",
       "1283  friend   1317.000000   1317.000000  Default  23.0000  23.0000\n",
       "648   access   1498.000000   1498.000000  Default  22.0000  22.0000\n",
       "144     play   8334.000000   8334.000000  Default  21.0000  21.0000\n",
       "...      ...           ...           ...      ...      ...      ...\n",
       "7896    unit      0.025025      1.012483  Topic25 -10.4928   3.0491\n",
       "4681  tactic      0.025025      1.012483  Topic25 -10.4928   3.0491\n",
       "2703   style      0.025025      1.012483  Topic25 -10.4928   3.0491\n",
       "2988   accur      0.025025      1.012482  Topic25 -10.4928   3.0491\n",
       "3022     sex      0.025025      1.012482  Topic25 -10.4928   3.0491\n",
       "\n",
       "[778 rows x 6 columns], token_table=      Topic      Freq  Term\n",
       "term                       \n",
       "1         1  0.968562    10\n",
       "1         4  0.031221    10\n",
       "1184     21  0.973516  1080\n",
       "1150     24  0.918796   10x\n",
       "1995     12  0.757313  1300\n",
       "...     ...       ...   ...\n",
       "1266      5  0.914084     \n",
       "1012      4  0.358180     \n",
       "1012     24  0.573088     \n",
       "2387     15  0.959378     \n",
       "1013     24  0.913269     \n",
       "\n",
       "[819 rows x 3 columns], R=25, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[4, 17, 12, 22, 13, 8, 23, 24, 2, 14, 5, 10, 19, 15, 9, 18, 3, 25, 20, 21, 1, 6, 7, 16, 11])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_by_vote[True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1eeb1319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(True, PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "3     -0.187436  0.511818       1        1  44.394761\n",
       "16    -0.488663  0.198013       2        1  37.098949\n",
       "11    -0.358684  0.356177       3        1   5.901905\n",
       "21     0.340081  0.371000       4        1   1.985452\n",
       "12    -0.243736 -0.418907       5        1   1.141217\n",
       "7      0.482882  0.067892       6        1   1.081455\n",
       "22     0.056733  0.408814       7        1   1.062607\n",
       "23     0.222999  0.313528       8        1   0.990539\n",
       "1     -0.410787 -0.109142       9        1   0.709534\n",
       "13     0.287705 -0.299217      10        1   0.562428\n",
       "4     -0.001196 -0.422381      11        1   0.547415\n",
       "9     -0.291028 -0.215934      12        1   0.488357\n",
       "18     0.353229 -0.141430      13        1   0.425398\n",
       "14    -0.059549  0.228315      14        1   0.405959\n",
       "8      0.142572 -0.338607      15        1   0.397672\n",
       "17    -0.264921  0.027444      16        1   0.396442\n",
       "2      0.022807 -0.256846      17        1   0.365552\n",
       "24     0.288412  0.036232      18        1   0.359384\n",
       "19     0.064223  0.118933      19        1   0.335155\n",
       "20     0.205246 -0.145014      20        1   0.325952\n",
       "0     -0.132810 -0.220167      21        1   0.325125\n",
       "5     -0.107873  0.057473      22        1   0.234494\n",
       "6      0.139317  0.009508      23        1   0.203193\n",
       "15    -0.077946 -0.070995      24        1   0.143890\n",
       "10     0.018421 -0.066506      25        1   0.117162, topic_info=        Term          Freq         Total Category  logprob  loglift\n",
       "57      game  18479.000000  18479.000000  Default  25.0000  25.0000\n",
       "263    earli   1749.000000   1749.000000  Default  24.0000  24.0000\n",
       "1283  friend   1317.000000   1317.000000  Default  23.0000  23.0000\n",
       "648   access   1498.000000   1498.000000  Default  22.0000  22.0000\n",
       "144     play   8334.000000   8334.000000  Default  21.0000  21.0000\n",
       "...      ...           ...           ...      ...      ...      ...\n",
       "7896    unit      0.025025      1.012483  Topic25 -10.4928   3.0491\n",
       "4681  tactic      0.025025      1.012483  Topic25 -10.4928   3.0491\n",
       "2703   style      0.025025      1.012483  Topic25 -10.4928   3.0491\n",
       "2988   accur      0.025025      1.012482  Topic25 -10.4928   3.0491\n",
       "3022     sex      0.025025      1.012482  Topic25 -10.4928   3.0491\n",
       "\n",
       "[778 rows x 6 columns], token_table=      Topic      Freq  Term\n",
       "term                       \n",
       "1         1  0.968562    10\n",
       "1         4  0.031221    10\n",
       "1184     21  0.973516  1080\n",
       "1150     24  0.918796   10x\n",
       "1995     12  0.757313  1300\n",
       "...     ...       ...   ...\n",
       "1266      5  0.914084     \n",
       "1012      4  0.358180     \n",
       "1012     24  0.573088     \n",
       "2387     15  0.959378     \n",
       "1013     24  0.913269     \n",
       "\n",
       "[819 rows x 3 columns], R=25, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[4, 17, 12, 22, 13, 8, 23, 24, 2, 14, 5, 10, 19, 15, 9, 18, 3, 25, 20, 21, 1, 6, 7, 16, 11])), (False, PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "21    -0.215872  0.409394       1        1  57.403329\n",
       "9     -0.405155  0.225053       2        1  15.498433\n",
       "22    -0.312391  0.329905       3        1  10.492465\n",
       "16     0.121799  0.398770       4        1   5.759096\n",
       "17    -0.420988 -0.168141       5        1   2.129150\n",
       "20     0.398807  0.204474       6        1   1.189401\n",
       "1     -0.001201  0.447231       7        1   1.018463\n",
       "3      0.282274  0.259892       8        1   0.906708\n",
       "4     -0.326724 -0.023001       9        1   0.670562\n",
       "10     0.288187 -0.297894      10        1   0.654673\n",
       "23    -0.098158 -0.426380      11        1   0.625773\n",
       "12    -0.247804 -0.253852      12        1   0.620516\n",
       "18     0.354102 -0.042245      13        1   0.515058\n",
       "24    -0.051955  0.175310      14        1   0.504179\n",
       "0      0.110708 -0.346603      15        1   0.382263\n",
       "11    -0.169361  0.014748      16        1   0.333630\n",
       "8     -0.015784 -0.260080      17        1   0.289449\n",
       "19     0.214939  0.020705      18        1   0.283557\n",
       "14     0.082931  0.025404      19        1   0.180465\n",
       "13     0.284626 -0.170417      20        1   0.142363\n",
       "7     -0.059857 -0.156613      21        1   0.128801\n",
       "15    -0.003951 -0.059720      22        1   0.106347\n",
       "5      0.096785 -0.105327      23        1   0.078794\n",
       "6      0.045544 -0.121679      24        1   0.054927\n",
       "2      0.048499 -0.078933      25        1   0.031597, topic_info=           Term         Freq        Total Category  logprob  loglift\n",
       "765       crash   674.000000   674.000000  Default  25.0000  25.0000\n",
       "203         say   959.000000   959.000000  Default  24.0000  24.0000\n",
       "290     content   878.000000   878.000000  Default  23.0000  23.0000\n",
       "697       money   988.000000   988.000000  Default  22.0000  22.0000\n",
       "90         game  6433.000000  6433.000000  Default  21.0000  21.0000\n",
       "...         ...          ...          ...      ...      ...      ...\n",
       "2879             0.078196     0.868524  Topic25  -7.3755   5.6523\n",
       "2880          0.078196     0.868524  Topic25  -7.3755   5.6523\n",
       "2881           0.078196     0.868524  Topic25  -7.3755   5.6523\n",
       "2882       0.078196     0.868524  Topic25  -7.3755   5.6523\n",
       "2883              0.078196     0.868524  Topic25  -7.3755   5.6523\n",
       "\n",
       "[1011 rows x 6 columns], token_table=      Topic      Freq   Term\n",
       "term                        \n",
       "845       1  0.504149   1000\n",
       "845       5  0.485477   1000\n",
       "3397     24  0.777733  1080p\n",
       "4704     12  0.221327   1370\n",
       "4704     19  0.663980   1370\n",
       "...     ...       ...    ...\n",
       "1818     20  0.930988      \n",
       "4904     15  0.966718      \n",
       "844       5  0.988468      \n",
       "1065      5  0.980702      \n",
       "1066      5  0.971330      \n",
       "\n",
       "[1731 rows x 3 columns], R=25, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[22, 10, 23, 17, 18, 21, 2, 4, 5, 11, 24, 13, 19, 25, 1, 12, 9, 20, 15, 14, 8, 16, 6, 7, 3]))])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_by_vote.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "facb85ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "voted_up_fd = FrequencyDistribution(df[df['voted_up']==True]['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3f38bf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "7      0.413654  0.344658       1        1  52.749701\n",
       "15     0.566248  0.124739       2        1  37.669031\n",
       "8     -0.267176  0.446590       3        1   3.262325\n",
       "16     0.163232  0.462254       4        1   2.389622\n",
       "13     0.175300 -0.323502       5        1   0.492367\n",
       "22     0.293944 -0.137514       6        1   0.478505\n",
       "12    -0.117801  0.242859       7        1   0.383492\n",
       "2     -0.167698 -0.310481       8        1   0.299558\n",
       "6     -0.334947  0.014321       9        1   0.295243\n",
       "11     0.107348  0.157520      10        1   0.266995\n",
       "10     0.168312 -0.028396      11        1   0.224624\n",
       "1     -0.027705  0.082711      12        1   0.174891\n",
       "21    -0.229656 -0.100208      13        1   0.164544\n",
       "17    -0.150426  0.084398      14        1   0.143109\n",
       "9     -0.021572 -0.238839      15        1   0.136411\n",
       "23    -0.200524  0.009081      16        1   0.127336\n",
       "19     0.075213 -0.099097      17        1   0.118671\n",
       "18    -0.163741 -0.152980      18        1   0.117165\n",
       "20     0.032085 -0.165914      19        1   0.105160\n",
       "3      0.015154 -0.009684      20        1   0.100558\n",
       "0     -0.091031 -0.146619      21        1   0.079448\n",
       "4     -0.029346 -0.115841      22        1   0.072200\n",
       "24    -0.069525 -0.018053      23        1   0.065193\n",
       "14    -0.093082 -0.061328      24        1   0.065181\n",
       "5     -0.046262 -0.060674      25        1   0.018668, topic_info=          Term          Freq         Total Category  logprob  loglift\n",
       "10           .  18192.000000  18192.000000  Default  25.0000  25.0000\n",
       "127       game  16697.000000  16697.000000  Default  24.0000  24.0000\n",
       "7            ,  14759.000000  14759.000000  Default  23.0000  23.0000\n",
       "183        the  15642.000000  15642.000000  Default  22.0000  22.0000\n",
       "98         and  15016.000000  15016.000000  Default  21.0000  21.0000\n",
       "...        ...           ...           ...      ...      ...      ...\n",
       "4630   BECAUSE      0.005388      0.749204  Topic25 -10.6965   3.6512\n",
       "4635       YOU      0.005388      0.749204  Topic25 -10.6965   3.6512\n",
       "4766  actively      0.005388      0.749204  Topic25 -10.6965   3.6512\n",
       "4789     works      0.005388      0.749205  Topic25 -10.6965   3.6512\n",
       "4799       ONE      0.005388      0.749204  Topic25 -10.6965   3.6512\n",
       "\n",
       "[847 rows x 6 columns], token_table=      Topic      Freq Term\n",
       "term                      \n",
       "0         2  0.977547    !\n",
       "0         4  0.022393    !\n",
       "637       1  0.999658   ''\n",
       "2886     14  0.954778   'S\n",
       "245       1  0.994211  're\n",
       "...     ...       ...  ...\n",
       "1865      4  0.115758    \n",
       "1865      8  0.879759    \n",
       "1866      8  0.981653    \n",
       "1867      8  0.994811    \n",
       "3736     10  0.948225    \n",
       "\n",
       "[841 rows x 3 columns], R=25, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[8, 16, 9, 17, 14, 23, 13, 3, 7, 12, 11, 2, 22, 18, 10, 24, 20, 19, 21, 4, 1, 5, 25, 15, 6])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_vis(voted_up_fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb1bcba",
   "metadata": {},
   "source": [
    "## Converting Author column to an actual dataframe/dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1148310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert author column to a dictionary\n",
    "# https://stackoverflow.com/questions/39169718/convert-string-to-dict-then-access-keyvalues-how-to-access-data-in-a-class\n",
    "import ast\n",
    "\n",
    "# df = df[\"Coordinates\"].astype('str')\n",
    "# df = df.apply(lambda x: ast.literal_eval(x))\n",
    "# df = df.apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b82f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df['author'].astype('str')\n",
    "df2 = df2.apply(lambda x: ast.literal_eval(x))\n",
    "df2 = df2.apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8820a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_ids = df2[df2.steamid.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28eaab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79f1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['steamid'] == '76561198098399242']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aac594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think I double-counted/ collected duplicate reviews. Have to figure this out.\n",
    "# Downloading more recent reviews should help\n",
    "df.recommendationid.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd979430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flask\n",
    "import pickle\n",
    "\n",
    "# with open('NLP_classification_model', 'wb') as f:\n",
    "#     f.\n",
    "\n",
    "s = pickle.dumps(clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8440b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = pickle.loads(s)\n",
    "clf2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd03a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving a classifier\n",
    "with open('my_dumped_classifier.pkl', 'wb') as fid:\n",
    "    pickle.dump(clf, fid)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
